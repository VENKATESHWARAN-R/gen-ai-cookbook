{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook on Document splitting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries to interfact with the docuements\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from langchain_community.document_loaders import (\n",
    "    PyMuPDFLoader,\n",
    "    UnstructuredMarkdownLoader,\n",
    ")\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "\n",
    "import requests\n",
    "import logging\n",
    "from typing import List, Dict, Any, Optional, Literal\n",
    "from langchain_core.documents import Document\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "import json # Added for parsing LLM response if needed\n",
    "\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_paths = (\"./data/quantum_computing.md\", \"./data/quantum_physics.md\",)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "CONTEXTUAL_PROMPT_TEMPLATE = \"\"\"Given the following document:\n",
    "<document>\n",
    "{full_doc_content}\n",
    "</document>\n",
    "\n",
    "And this specific chunk from the document:\n",
    "<chunk>\n",
    "{chunk_content}\n",
    "</chunk>\n",
    "\n",
    "Provide a short, succinct context (1-2 sentences) that helps understand where this chunk fits within the larger document. This context will be used to improve search retrieval for the chunk. Focus on the chunk's relationship to surrounding information or its main topic within the document's scope. Respond *only* with the context itself, nothing else.\n",
    "\"\"\"\n",
    "\n",
    "# Custom Exception\n",
    "class LLMError(Exception):\n",
    "    \"\"\"Custom exception for LLM API errors.\"\"\"\n",
    "    pass\n",
    "\n",
    "class SemanticSplitter:\n",
    "    \"\"\"\n",
    "    Splits documents into semantic chunks using an LLM, with a fallback mechanism.\n",
    "    \"\"\"\n",
    "    DEFAULT_SYSTEM_PROMPT = (\n",
    "        \"You are an expert text analyzer. Your task is to segment the provided text \"\n",
    "        \"into semantically coherent chunks. Each chunk should represent a distinct idea or topic. \"\n",
    "        \"Aim for chunks around {target_size} {size_unit} long, but prioritize semantic boundaries \"\n",
    "        \"over strict size adherence. Output *only* the chunks separated by the delimiter '{separator}'. \"\n",
    "        \"Try to stick with the document's original structure and meaning. \"\n",
    "        \"Sometimes the document may contain tabular data but the structure might be lost in the text. \"\n",
    "        \"In such cases, please try to keep the table data as much as possible. and add context about the table and table data in single chunk. \"\n",
    "        #\"Do not chunk the table data. instead, keep it in a single chunk with the context.\"\n",
    "        \"The document may contain technical jargon or specialized terms. \"\n",
    "        \"If you encounter any such terms, please try to keep them in the same chunk. \"\n",
    "    )\n",
    "\n",
    "    DEFAULT_SUMMARY_PROMPT = CONTEXTUAL_PROMPT_TEMPLATE\n",
    "\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        llm_endpoint_url: str,\n",
    "        llm_headers: Dict[str, str],\n",
    "        target_chunk_size: int = 125, # Default size in tokens\n",
    "        size_unit: Literal['tokens', 'characters'] = 'tokens', # Note: Token count is approximate unless tokenizer is matched\n",
    "        llm_chunk_separator: str = \"\\n---CHUNK_SEPARATOR---\\n\",\n",
    "        llm_request_timeout: int = 60, # seconds\n",
    "        fallback_chunk_size: int = 650,\n",
    "        fallback_chunk_overlap: int = 50,\n",
    "        add_context: bool = False, # Flag to enable summarization\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initializes the SemanticSplitter.\n",
    "\n",
    "        Args:\n",
    "            llm_endpoint_url: The URL for the LLM API endpoint.\n",
    "            llm_headers: Headers for the LLM API request.\n",
    "            target_chunk_size: Desired approximate size for LLM-generated chunks.\n",
    "            size_unit: Unit for target_chunk_size ('tokens' or 'characters').\n",
    "            llm_chunk_separator: A unique string used to separate chunks in the LLM output.\n",
    "            llm_request_timeout: Timeout for LLM API calls in seconds.\n",
    "            fallback_chunk_size: Chunk size for the RecursiveCharacterTextSplitter fallback.\n",
    "            fallback_chunk_overlap: Chunk overlap for the fallback splitter.\n",
    "            add_context: Whether to add a summary metadata field (requires extra LLM calls).\n",
    "        \"\"\"\n",
    "        self.llm_endpoint_url = llm_endpoint_url\n",
    "        self.llm_headers = llm_headers.copy() # Make a copy\n",
    "\n",
    "        self.target_chunk_size = target_chunk_size\n",
    "        self.size_unit = size_unit\n",
    "        self.llm_chunk_separator = llm_chunk_separator\n",
    "        self.llm_request_timeout = llm_request_timeout\n",
    "        self.add_context = add_context\n",
    "\n",
    "        self.fallback_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=fallback_chunk_size,\n",
    "            chunk_overlap=fallback_chunk_overlap,\n",
    "            separators=[\"\\n\\n\", \"\\n\", \" \", \".\", \"\"], # Added more separators for flexibility\n",
    "        )\n",
    "        logger.info(f\"SemanticSplitter initialized. LLM Endpoint: {self.llm_endpoint_url}, Target Size: {target_chunk_size} {size_unit}, Fallback Size: {fallback_chunk_size}\")\n",
    "\n",
    "\n",
    "    def _call_llm_api(self, payload: Dict[str, Any]) -> str:\n",
    "        \"\"\"Helper function to call the LLM API endpoint.\"\"\"\n",
    "        try:\n",
    "            response = requests.post(\n",
    "                self.llm_endpoint_url,\n",
    "                headers=self.llm_headers,\n",
    "                json=payload,\n",
    "                timeout=self.llm_request_timeout\n",
    "            )\n",
    "            response.raise_for_status() # Raises HTTPError for bad responses (4xx or 5xx)\n",
    "\n",
    "            # Assuming the response JSON has a key like 'response' or 'generated_text'\n",
    "            # Adjust based on your actual API response structure\n",
    "            response_data = response.json()\n",
    "            if \"response\" in response_data:\n",
    "                 generated_text = response_data[\"response\"]\n",
    "            else:\n",
    "                raise LLMError(f\"Could not extract generated text from LLM response: {response_data}\")\n",
    "\n",
    "\n",
    "            if not isinstance(generated_text, str):\n",
    "                raise LLMError(f\"LLM response content is not a string: {type(generated_text)}\")\n",
    "\n",
    "            return generated_text.strip()\n",
    "\n",
    "        except requests.exceptions.Timeout:\n",
    "            logger.error(f\"LLM API call timed out after {self.llm_request_timeout} seconds.\")\n",
    "            raise LLMError(\"LLM API call timed out.\")\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            logger.error(f\"LLM API request failed: {e}\")\n",
    "            raise LLMError(f\"LLM API request failed: {e}\")\n",
    "        except (json.JSONDecodeError, KeyError, TypeError) as e:\n",
    "             logger.error(f\"Failed to parse LLM JSON response or access expected key: {e}\")\n",
    "             raise LLMError(f\"Failed to parse LLM JSON response: {e}\")\n",
    "\n",
    "\n",
    "    def _split_with_llm(self, doc: Document) -> List[Document]:\n",
    "        \"\"\"Attempts to split a single document using the LLM.\"\"\"\n",
    "        logger.info(f\"Attempting LLM splitting for source: {doc.metadata.get('source', 'N/A')}, page: {doc.metadata.get('page', 'N/A')}\")\n",
    "        page_content = doc.page_content\n",
    "        if not page_content or page_content.isspace():\n",
    "             logger.warning(f\"Skipping empty page content for source: {doc.metadata.get('source', 'N/A')}, page: {doc.metadata.get('page', 'N/A')}\")\n",
    "             return []\n",
    "\n",
    "        system_prompt = self.DEFAULT_SYSTEM_PROMPT.format(\n",
    "            target_size=self.target_chunk_size,\n",
    "            size_unit=self.size_unit,\n",
    "            separator=self.llm_chunk_separator\n",
    "        )\n",
    "\n",
    "        payload = {\n",
    "            \"prompt\": page_content,\n",
    "            \"system_prompt\": system_prompt,\n",
    "            \"kwargs\": {} # Add any specific LLM params here if needed\n",
    "        }\n",
    "\n",
    "        try:\n",
    "            generated_text = self._call_llm_api(payload)\n",
    "\n",
    "            if not generated_text or self.llm_chunk_separator not in generated_text:\n",
    "                # Basic validation: LLM didn't follow instructions or produced empty result\n",
    "                # Log the generated text for debugging\n",
    "                logger.warning(f\"LLm generated text: {generated_text}\")\n",
    "                logger.warning(f\"LLM output validation failed for source: {doc.metadata.get('source', 'N/A')}, page: {doc.metadata.get('page', 'N/A')}. Output didn't contain separator or was empty.\")\n",
    "                raise ValueError(\"Invalid LLM response format (separator missing or empty).\")\n",
    "\n",
    "            raw_chunks = generated_text.split(self.llm_chunk_separator)\n",
    "            # Filter out potential empty strings resulting from split\n",
    "            raw_chunks = [chunk.strip() for chunk in raw_chunks if chunk.strip()]\n",
    "\n",
    "            if not raw_chunks:\n",
    "                 logger.warning(f\"LLM output resulted in zero valid chunks after splitting for source: {doc.metadata.get('source', 'N/A')}, page: {doc.metadata.get('page', 'N/A')}.\")\n",
    "                 raise ValueError(\"LLM output resulted in zero valid chunks.\")\n",
    "\n",
    "\n",
    "            logger.info(f\"LLM successfully generated {len(raw_chunks)} raw chunks.\")\n",
    "            return self._post_process_chunks(raw_chunks, doc, split_method=\"llm\")\n",
    "\n",
    "        except (LLMError, ValueError) as e:\n",
    "            logger.warning(f\"LLM splitting failed for source: {doc.metadata.get('source', 'N/A')}, page: {doc.metadata.get('page', 'N/A')}. Reason: {e}\")\n",
    "            # Re-raise the specific error to be caught by the calling method for fallback\n",
    "            raise e\n",
    "\n",
    "\n",
    "    def _split_with_fallback(self, doc: Document) -> List[Document]:\n",
    "        \"\"\"Splits a single document using the fallback RecursiveCharacterTextSplitter.\"\"\"\n",
    "        logger.warning(f\"Using fallback RecursiveCharacterTextSplitter for source: {doc.metadata.get('source', 'N/A')}, page: {doc.metadata.get('page', 'N/A')}\")\n",
    "\n",
    "        fallback_chunks_text = self.fallback_splitter.split_text(doc.page_content)\n",
    "\n",
    "        processed_chunks = []\n",
    "        source_metadata = doc.metadata # Preserve original metadata\n",
    "\n",
    "        for i, chunk_text in enumerate(fallback_chunks_text):\n",
    "            chunk_metadata = source_metadata.copy() # Start with original metadata\n",
    "            chunk_metadata.update({\n",
    "                \"chunk_number\": i + 1,\n",
    "                \"split_method\": \"recursive\",\n",
    "                # Potentially add estimated size if needed\n",
    "                # \"estimated_char_count\": len(chunk_text)\n",
    "            })\n",
    "            processed_chunks.append(Document(page_content=chunk_text, metadata=chunk_metadata))\n",
    "\n",
    "        logger.info(f\"Fallback splitter created {len(processed_chunks)} chunks.\")\n",
    "        return processed_chunks\n",
    "\n",
    "    def _summarize_chunk(self, chunk_text: str, doc_content: str) -> Optional[str]:\n",
    "        \"\"\"(Optional) Generates a summary for a given text chunk using the LLM.\"\"\"\n",
    "        if not chunk_text or chunk_text.isspace():\n",
    "            return None\n",
    "\n",
    "        logger.debug(f\"Requesting summary for chunk: {chunk_text[:100]}...\") # Log first 100 chars\n",
    "\n",
    "        summary_prompt = self.DEFAULT_SUMMARY_PROMPT.format(full_doc_content=doc_content, chunk_content=chunk_text)\n",
    "        payload = {\n",
    "            \"prompt\": summary_prompt,\n",
    "            \"kwargs\": {}\n",
    "        }\n",
    "\n",
    "        try:\n",
    "            summary = self._call_llm_api(payload)\n",
    "            logger.debug(f\"Received summary: {summary}\")\n",
    "            return summary\n",
    "        except LLMError as e:\n",
    "            logger.warning(f\"Failed to generate summary for chunk: {e}\")\n",
    "            return None # Return None on summary failure\n",
    "\n",
    "\n",
    "    def _post_process_chunks(self, raw_chunks: List[str], original_doc: Document, split_method: str) -> List[Document]:\n",
    "        \"\"\"Formats raw text chunks into Document objects with metadata.\"\"\"\n",
    "        processed_chunks = []\n",
    "        source_metadata = original_doc.metadata\n",
    "\n",
    "        for i, chunk_text in enumerate(raw_chunks):\n",
    "            if not chunk_text or chunk_text.isspace():\n",
    "                continue # Skip empty chunks just in case\n",
    "\n",
    "            chunk_metadata = source_metadata.copy() # Start with original metadata\n",
    "            chunk_metadata.update({\n",
    "                \"chunk_number\": i + 1,\n",
    "                \"split_method\": split_method,\n",
    "                # \"estimated_char_count\": len(chunk_text)\n",
    "            })\n",
    "\n",
    "            # --- Optional Summarization ---\n",
    "            if self.add_context:\n",
    "                summary = self._summarize_chunk(chunk_text, original_doc.page_content)\n",
    "                if summary:\n",
    "                    chunk_metadata[\"summary\"] = summary\n",
    "                    chunk_text += f\"\\n\\nContext of above chunk based on whole document: {summary}\" # Append summary to chunk text\n",
    "            # --- End Optional Summarization ---\n",
    "\n",
    "            processed_chunks.append(Document(page_content=chunk_text, metadata=chunk_metadata))\n",
    "\n",
    "        return processed_chunks\n",
    "\n",
    "\n",
    "    def split_documents(self, documents: List[Document]) -> List[Document]:\n",
    "        \"\"\"\n",
    "        Splits a list of documents using the LLM semantic splitter with fallback.\n",
    "\n",
    "        Args:\n",
    "            documents: A list of LangChain Document objects (e.g., from loader.load()).\n",
    "\n",
    "        Returns:\n",
    "            A list of new Document objects, each representing a chunk.\n",
    "        \"\"\"\n",
    "        all_chunks = []\n",
    "        total_docs = len(documents)\n",
    "        for i, doc in enumerate(documents):\n",
    "            logger.info(f\"Processing document {i+1}/{total_docs}: source={doc.metadata.get('source', 'N/A')}, page={doc.metadata.get('page', 'N/A')}\")\n",
    "            try:\n",
    "                # Decide based on content length\n",
    "                # if len(doc.page_content) < self.target_chunk_size / 2: # Example heuristic\n",
    "                #    logger.info(\"Content too short, using fallback splitter directly.\")\n",
    "                #    doc_chunks = self._split_with_fallback(doc)\n",
    "                # else:\n",
    "                #    doc_chunks = self._split_single_document(doc) # Use the method with try/except LLM->Fallback\n",
    "\n",
    "                # Simpler: Always try LLM first for any non-empty doc\n",
    "                if not doc.page_content or doc.page_content.isspace():\n",
    "                    logger.info(f\"Skipping document {i+1}/{total_docs} due to empty content.\")\n",
    "                    continue\n",
    "\n",
    "                doc_chunks = self._split_single_document(doc) # Use the method with try/except LLM->Fallback\n",
    "                all_chunks.extend(doc_chunks)\n",
    "                logger.info(f\"Finished processing document {i+1}/{total_docs}. Generated {len(doc_chunks)} chunks.\")\n",
    "\n",
    "            except Exception as e:\n",
    "                # Catch unexpected errors during processing of a single document\n",
    "                logger.error(f\"Unexpected error processing document {i+1}/{total_docs} (source: {doc.metadata.get('source', 'N/A')}). Skipping this document. Error: {e}\", exc_info=True)\n",
    "                # Decide if you want to halt completely or just skip the problematic doc\n",
    "                # continue\n",
    "\n",
    "        logger.info(f\"Completed splitting. Total chunks generated: {len(all_chunks)}\")\n",
    "        return all_chunks\n",
    "\n",
    "    def _split_single_document(self, doc: Document) -> List[Document]:\n",
    "        \"\"\"Internal helper to try LLM split and fallback on error for one document.\"\"\"\n",
    "        try:\n",
    "            # Attempt LLM splitting\n",
    "            llm_chunks = self._split_with_llm(doc)\n",
    "            # Optional: Add a size check here? If LLM produced chunks that are way too big?\n",
    "            # For now, trust the LLM or accept its output.\n",
    "            return llm_chunks\n",
    "        except (LLMError, ValueError, requests.exceptions.RequestException) as e:\n",
    "            # Logged inside _split_with_llm or _call_llm_api, log fallback activation here\n",
    "            logger.warning(f\"LLM splitting failed or produced invalid output for doc (source: {doc.metadata.get('source', 'N/A')}, page: {doc.metadata.get('page', 'N/A')}). Activating fallback splitter. Reason: {e}\")\n",
    "            # Use fallback splitter\n",
    "            fallback_chunks = self._split_with_fallback(doc)\n",
    "            return fallback_chunks\n",
    "        except Exception as e:\n",
    "             # Catch any other unexpected error during LLM attempt\n",
    "             logger.error(f\"Unexpected error during LLM splitting attempt for doc (source: {doc.metadata.get('source', 'N/A')}). Error: {e}\", exc_info=True)\n",
    "             logger.warning(\"Activating fallback splitter due to unexpected error.\")\n",
    "             fallback_chunks = self._split_with_fallback(doc)\n",
    "             return fallback_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "LLM_ENDPOINT = 'http://localhost:8000/api/llm/generate_response'\n",
    "LLM_HEADERS = {\n",
    "    'accept': 'application/json',\n",
    "    'Content-Type': 'application/json'\n",
    "}\n",
    "TIMEOUT = 3000 # seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-31 01:49:39,320 - INFO - SemanticSplitter initialized. LLM Endpoint: http://localhost:8000/api/llm/generate_response, Target Size: 125 tokens, Fallback Size: 650\n"
     ]
    }
   ],
   "source": [
    "# --- Initialize the splitter ---\n",
    "splitter = SemanticSplitter(\n",
    "    llm_endpoint_url=LLM_ENDPOINT,\n",
    "    llm_headers=LLM_HEADERS,\n",
    "    llm_request_timeout=TIMEOUT,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading docs using the langchain document loaders\n",
    "#docs = PyMuPDFLoader(file_paths[0]).load()\n",
    "# function for loading documents\n",
    "def load_documents(file_path: str) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Loads documents using the appropriate loader based on file extension.\n",
    "    \"\"\"\n",
    "    _, ext = os.path.splitext(file_path)\n",
    "    ext = ext.lower()\n",
    "\n",
    "    if ext == '.pdf':\n",
    "        logger.info(f\"Loading PDF file: {file_path}\")\n",
    "        loader = PyMuPDFLoader(file_path)\n",
    "    elif ext == '.md':\n",
    "        logger.info(f\"Loading Markdown file: {file_path}\")\n",
    "        loader = UnstructuredMarkdownLoader(file_path)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported file extension: {ext}\")\n",
    "\n",
    "    documents = loader.load()\n",
    "    logger.info(f\"Loaded {len(documents)} documents from {file_path}.\")\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-31 01:49:39,691 - INFO - Loading Markdown file: ./data/quantum_computing.md\n",
      "2025-03-31 01:49:39,711 - INFO - Loaded 1 documents from ./data/quantum_computing.md.\n"
     ]
    }
   ],
   "source": [
    "docs = load_documents(file_paths[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantum Computing: Harnessing the Power of the Quantum Realm\n",
      "\n",
      "Quantum computing is an emerging field that leverages the principles of quantum mechanics to solve complex problems that are intractable for classical computers. By exploiting phenomena like superposition and entanglement, quantum computers have the potential to revolutionize various industries, from medicine and materials science to finance and artificial intelligence.\n",
      "\n",
      "The Limitations of Classical Computing\n",
      "\n",
      "Classical computers, the workhorses of our digital age, store and process information as bits, which can exist in one of two states: 0 or 1. These bits are the fundamental units of information. While classical computers have made tremendous progress, they face fundamental limitations when dealing with certain types of problems, particularly those involving:\n",
      "\n",
      "Exponential Complexity: Many real-world problems, such as simulating large molecules, factoring large numbers, or optimizing complex systems, exhibit exponential complexity. This means that the computational resources (time and memory) required to solve them grow exponentially with the size of the problem. Classical computers struggle with such problems, often taking prohibitively long to find a solution.\n",
      "\n",
      "Exploring Vast Solution Spaces: Problems with a vast number of possible solutions, like drug discovery or materials design, require exploring a massive search space. Classical algorithms typically explore these spaces sequentially, which can be inefficient and time-consuming.\n",
      "\n",
      "Quantum Mechanics: The Foundation of Quantum Computing\n",
      "\n",
      "Quantum computers overcome these limitations by harnessing the unique principles of quantum mechanics:\n",
      "\n",
      "Superposition: Unlike classical bits, which can be either 0 or 1, a quantum bit, or qubit, can exist in a superposition of both states simultaneously. Mathematically, a qubit's state can be represented as a linear combination of the basis states $|0\\rangle$ and $|1\\rangle$: $$|\\psi\\rangle = \\alpha|0\\rangle + \\beta|1\\rangle$$ where $\\alpha$ and $\\beta$ are complex numbers such that $|\\alpha|^2 + |\\beta|^2 = 1$. $|\\alpha|^2$ represents the probability of measuring the qubit in the $|0\\rangle$ state, and $|\\beta|^2$ represents the probability of measuring it in the $|1\\rangle$ state. This ability to be in multiple states at once allows quantum computers to explore many possibilities simultaneously.\n",
      "\n",
      "Entanglement: When two or more qubits become entangled, their quantum states are linked in such a way that they share the same fate, regardless of the distance between them. Measuring the state of one entangled qubit instantaneously determines the state of the other(s). Entanglement allows quantum computers to perform correlated operations on multiple qubits, leading to exponential increases in computational power.\n",
      "\n",
      "Quantum Interference: Quantum computations manipulate the probabilities of different outcomes through interference. By carefully designing sequences of quantum gates, the probabilities of desired outcomes can be amplified, while the probabilities of undesired outcomes are suppressed. This is analogous to wave interference, where waves can constructively or destructively interfere with each other.\n",
      "\n",
      "How Quantum Computers Work\n",
      "\n",
      "A quantum computer typically consists of the following key components:\n",
      "\n",
      "Qubits: The fundamental building blocks of quantum computers, which store and process information in quantum states. Different physical systems can be used to implement qubits, including:\n",
      "\n",
      "Superconducting circuits: Tiny electrical circuits cooled to near absolute zero, where quantum effects become dominant.\n",
      "\n",
      "Trapped ions: Individual ions held in place by electromagnetic fields, whose quantum states are manipulated using lasers.\n",
      "\n",
      "Photonic systems: Using photons (particles of light) as qubits, which can be transmitted through optical fibers.\n",
      "\n",
      "Quantum dots: Semiconductor nanostructures that confine electrons, whose spin states can be used as qubits.\n",
      "\n",
      "Neutral atoms in optical lattices: Atoms trapped in a grid of light beams.\n",
      "\n",
      "Quantum Gates: Analogous to logic gates in classical computers, quantum gates are operations that manipulate the states of qubits. However, unlike classical gates that operate on definite 0 or 1 states, quantum gates operate on superpositions and can create entanglement. Examples of fundamental quantum gates include the Pauli-X gate (bit flip), the Hadamard gate (creates superposition), and the CNOT gate (controlled-NOT, entangling gate).\n",
      "\n",
      "Quantum Algorithms: These are specific sequences of quantum gates designed to solve particular computational problems. Quantum algorithms leverage superposition, entanglement, and interference to achieve speedups over their classical counterparts for certain tasks. Some prominent quantum algorithms include:\n",
      "\n",
      "Shor's Algorithm: Efficiently factors large integers, which has significant implications for breaking modern public-key cryptography.\n",
      "\n",
      "Grover's Algorithm: Provides a quadratic speedup for searching unsorted databases.\n",
      "\n",
      "Quantum Fourier Transform (QFT): A quantum analogue of the classical Discrete Fourier Transform, which is a key component of many other quantum algorithms, including Shor's algorithm and algorithms for quantum simulation.\n",
      "\n",
      "Variational Quantum Eigensolver (VQE): A hybrid quantum-classical algorithm used for finding the ground state energy of molecules and materials.\n",
      "\n",
      "Quantum Approximate Optimization Algorithm (QAOA): Another hybrid algorithm aimed at finding approximate solutions to combinatorial optimization problems.\n",
      "\n",
      "Measurement: The final step in a quantum computation involves measuring the states of the qubits. This process causes the superposition to collapse into one of the basis states (0 or 1), with a probability determined by the amplitudes $\\alpha$ and $\\beta$. Repeated measurements are often required to obtain a statistically significant result.\n",
      "\n",
      "Challenges and the NISQ Era\n",
      "\n",
      "Building and operating quantum computers is a formidable technological challenge. Some of the major hurdles include:\n",
      "\n",
      "Decoherence: Quantum states are fragile and can easily be disrupted by interactions with the environment, leading to the loss of quantum information. Maintaining the coherence of qubits for sufficiently long periods is crucial for performing complex computations.\n",
      "\n",
      "Scalability: Building quantum computers with a large number of high-quality, interconnected qubits is a significant engineering challenge. Current quantum computers have a relatively small number of qubits, and increasing this number while maintaining fidelity is a key area of research.\n",
      "\n",
      "Fidelity: Quantum gates and measurements are not perfect and introduce errors into the computation. Reducing error rates and implementing quantum error correction techniques are essential for building fault-tolerant quantum computers.\n",
      "\n",
      "Currently, we are in the Noisy Intermediate-Scale Quantum (NISQ) era, characterized by quantum computers with a limited number of noisy qubits. While these computers may not be capable of solving all the problems envisioned for fault-tolerant quantum computers, they are being used to explore potential quantum advantages for specific applications and to develop and test quantum algorithms.\n",
      "\n",
      "Potential Applications of Quantum Computing\n",
      "\n",
      "Quantum computing holds immense promise for various fields:\n",
      "\n",
      "Drug Discovery and Development: Simulating molecular interactions and designing new drugs and therapies with greater accuracy and efficiency.\n",
      "\n",
      "Materials Science: Discovering and designing novel materials with desired properties, such as new superconductors or catalysts.\n",
      "\n",
      "Finance: Developing more sophisticated financial models for risk analysis, portfolio optimization, and fraud detection.\n",
      "\n",
      "Artificial Intelligence and Machine Learning: Accelerating machine learning algorithms and developing new quantum machine learning techniques.\n",
      "\n",
      "Cryptography: Breaking current public-key encryption algorithms (like RSA) and developing new, quantum-resistant cryptographic methods.\n",
      "\n",
      "Optimization Problems: Finding optimal solutions to complex logistical and scheduling problems.\n",
      "\n",
      "Fundamental Science: Advancing our understanding of quantum physics, cosmology, and other fundamental scientific questions through simulations.\n",
      "\n",
      "The Future of Quantum Computing\n",
      "\n",
      "The field of quantum computing is rapidly evolving, with significant investments from both academia and industry. While fault-tolerant, universal quantum computers are still likely some years away, the progress being made in qubit technology, algorithm development, and error correction is encouraging. The NISQ era is providing valuable insights and paving the way for the eventual realization of the full potential of quantum computing. As the technology matures, it is expected to have a transformative impact on science, technology, and society as a whole.\n"
     ]
    }
   ],
   "source": [
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitted 1 documents into 18 chunks.\n",
      "First chunk: Quantum Computing: Harnessing the Power of the Quantum Realm\n",
      "\n",
      "Quantum computing is an emerging field that leverages the principles of quantum mechanics to solve complex problems that are intractable for classical computers. By exploiting phenomena like superposition and entanglement, quantum computers have the potential to revolutionize various industries, from medicine and materials science to finance and artificial intelligence.\n",
      "\n",
      "The Limitations of Classical Computing...\n"
     ]
    }
   ],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=650,\n",
    "    chunk_overlap=50,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \".\", \"\"], # Added more separators for flexibility\n",
    ")\n",
    "\n",
    "splitted_docs = text_splitter.split_documents(docs)\n",
    "\n",
    "print(f\"Splitted {len(docs)} documents into {len(splitted_docs)} chunks.\")\n",
    "print(f\"First chunk: {splitted_docs[0].page_content[:1000]}...\") # Print first 100 chars of the first chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-31 01:53:35,634 - INFO - Processing document 1/1: source=./data/quantum_computing.md, page=N/A\n",
      "2025-03-31 01:53:35,636 - INFO - Attempting LLM splitting for source: ./data/quantum_computing.md, page: N/A\n",
      "2025-03-31 01:56:59,361 - INFO - LLM successfully generated 9 raw chunks.\n",
      "2025-03-31 01:56:59,365 - INFO - Finished processing document 1/1. Generated 9 chunks.\n",
      "2025-03-31 01:56:59,365 - INFO - Completed splitting. Total chunks generated: 9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Successfully generated 9 chunks.\n",
      "\n",
      "Example chunk 1:\n",
      "Content: Quantum computing represents a paradigm shift in computation, utilizing quantum mechanics to tackle problems beyond the reach of classical computers. It leverages phenomena like superposition and entanglement to explore vastly larger solution spaces and achieve exponential speedups for specific tasks....\n",
      "Metadata: {'source': './data/quantum_computing.md', 'chunk_number': 1, 'split_method': 'llm'}\n",
      "\n",
      "No fallback chunks were generated in this run.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    semantic_chunks = splitter.split_documents(docs)\n",
    "\n",
    "    # --- Inspect the results ---\n",
    "    if semantic_chunks:\n",
    "        print(f\"\\nSuccessfully generated {len(semantic_chunks)} chunks.\")\n",
    "        print(\"\\nExample chunk 1:\")\n",
    "        print(\"Content:\", semantic_chunks[0].page_content[:500] + \"...\") # Print first 500 chars\n",
    "        print(\"Metadata:\", semantic_chunks[0].metadata)\n",
    "\n",
    "        # Find a chunk created by fallback (if any)\n",
    "        fallback_chunk = next((c for c in semantic_chunks if c.metadata.get(\"split_method\") == \"recursive\"), None)\n",
    "        if fallback_chunk:\n",
    "             print(\"\\nExample fallback chunk:\")\n",
    "             print(\"Content:\", fallback_chunk.page_content[:500] + \"...\")\n",
    "             print(\"Metadata:\", fallback_chunk.metadata)\n",
    "        else:\n",
    "             print(\"\\nNo fallback chunks were generated in this run.\")\n",
    "\n",
    "    else:\n",
    "        print(\"\\nNo chunks were generated.\")\n",
    "\n",
    "except Exception as e:\n",
    "    logger.error(f\"An error occurred during the main splitting process: {e}\", exc_info=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(semantic_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1:\n",
      "('Content: Quantum computing represents a paradigm shift in computation, '\n",
      " 'utilizing quantum mechanics to tackle problems beyond the reach of classical '\n",
      " 'computers. It leverages phenomena like superposition and entanglement to '\n",
      " 'explore vastly larger solution spaces and achieve exponential speedups for '\n",
      " 'specific tasks.')\n",
      "----------------------------------------\n",
      "Chunk 2:\n",
      "('Content: Classical computers store information as bits representing either 0 '\n",
      " 'or 1, while quantum computers utilize qubits, which can exist in a '\n",
      " 'superposition of both states simultaneously. This allows quantum computers '\n",
      " 'to perform multiple calculations concurrently, dramatically increasing '\n",
      " 'processing power for certain types of problems.')\n",
      "----------------------------------------\n",
      "Chunk 3:\n",
      "('Content: A key limitation of classical computing arises from exponential '\n",
      " 'complexity – problems that require exponentially increasing computational '\n",
      " 'resources as their size grows. For instance, simulating large molecules or '\n",
      " 'factoring large numbers becomes intractable for classical machines. Quantum '\n",
      " 'computers circumvent this limitation by exploiting quantum principles to '\n",
      " 'explore numerous possibilities in parallel.')\n",
      "----------------------------------------\n",
      "Chunk 4:\n",
      "('Content: The foundation of quantum computing rests on principles from '\n",
      " 'quantum mechanics. Superposition enables qubits to exist in multiple states '\n",
      " 'at once, dramatically increasing computational possibilities. Entanglement '\n",
      " 'links qubits together, allowing correlated operations across vast distances, '\n",
      " 'leading to exponential increases in computational power. Quantum '\n",
      " 'interference further refines calculations by manipulating probabilities, '\n",
      " 'amplifying desired outcomes and suppressing unwanted ones.')\n",
      "----------------------------------------\n",
      "Chunk 5:\n",
      "('Content: Quantum computers typically consist of qubits, the fundamental '\n",
      " 'building blocks, implemented using various physical systems such as '\n",
      " 'superconducting circuits, trapped ions, photonic systems, and quantum dots. '\n",
      " 'Quantum gates, analogous to logic gates in classical computers, manipulate '\n",
      " 'the states of qubits through superposition and entanglement.  Prominent '\n",
      " 'quantum algorithms include Shor’s algorithm for factoring, Grover’s '\n",
      " 'algorithm for searching databases, and VQE/QAOA for optimization.')\n",
      "----------------------------------------\n",
      "Chunk 6:\n",
      "('Content: Measurement is the final step in a quantum computation, collapsing '\n",
      " 'the superposition of qubits into a definite state (0 or 1). This process '\n",
      " 'provides the result, though repeated measurements are often necessary to '\n",
      " 'obtain statistically significant outcomes. However, the technology faces '\n",
      " 'significant challenges, including decoherence – the loss of quantum '\n",
      " 'information due to environmental interference – and scalability – the '\n",
      " 'difficulty of building systems with a large number of high-fidelity qubits.')\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for i, x in enumerate(semantic_chunks):\n",
    "    print(f\"Chunk {i+1}:\")\n",
    "    pprint(f\"Content: {x.page_content}\")\n",
    "    #print(f\"Metadata: {x.metadata}\")\n",
    "    print(\"-\" * 40)\n",
    "    if i >= 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk from LLM:  Quantum computing represents a paradigm shift in computation, utilizing quantum mechanics to tackle problems beyond the reach of classical computers. It leverages phenomena like superposition and entanglement to explore vastly larger solution spaces and achieve exponential speedups for specific tasks.\n",
      "****************************************\n",
      "Chunk from Normal splitter:  Quantum Computing: Harnessing the Power of the Quantum Realm\n",
      "\n",
      "Quantum computing is an emerging field that leverages the principles of quantum mechanics to solve complex problems that are intractable for classical computers. By exploiting phenomena like superposition and entanglement, quantum computers have the potential to revolutionize various industries, from medicine and materials science to finance and artificial intelligence.\n",
      "\n",
      "The Limitations of Classical Computing\n",
      "----------------------------------------\n",
      "Chunk from LLM:  Classical computers store information as bits representing either 0 or 1, while quantum computers utilize qubits, which can exist in a superposition of both states simultaneously. This allows quantum computers to perform multiple calculations concurrently, dramatically increasing processing power for certain types of problems.\n",
      "****************************************\n",
      "Chunk from Normal splitter:  The Limitations of Classical Computing\n",
      "\n",
      "Classical computers, the workhorses of our digital age, store and process information as bits, which can exist in one of two states: 0 or 1. These bits are the fundamental units of information. While classical computers have made tremendous progress, they face fundamental limitations when dealing with certain types of problems, particularly those involving:\n",
      "----------------------------------------\n",
      "Chunk from LLM:  A key limitation of classical computing arises from exponential complexity – problems that require exponentially increasing computational resources as their size grows. For instance, simulating large molecules or factoring large numbers becomes intractable for classical machines. Quantum computers circumvent this limitation by exploiting quantum principles to explore numerous possibilities in parallel.\n",
      "****************************************\n",
      "Chunk from Normal splitter:  Exponential Complexity: Many real-world problems, such as simulating large molecules, factoring large numbers, or optimizing complex systems, exhibit exponential complexity. This means that the computational resources (time and memory) required to solve them grow exponentially with the size of the problem. Classical computers struggle with such problems, often taking prohibitively long to find a solution.\n",
      "----------------------------------------\n",
      "Chunk from LLM:  The foundation of quantum computing rests on principles from quantum mechanics. Superposition enables qubits to exist in multiple states at once, dramatically increasing computational possibilities. Entanglement links qubits together, allowing correlated operations across vast distances, leading to exponential increases in computational power. Quantum interference further refines calculations by manipulating probabilities, amplifying desired outcomes and suppressing unwanted ones.\n",
      "****************************************\n",
      "Chunk from Normal splitter:  Exploring Vast Solution Spaces: Problems with a vast number of possible solutions, like drug discovery or materials design, require exploring a massive search space. Classical algorithms typically explore these spaces sequentially, which can be inefficient and time-consuming.\n",
      "\n",
      "Quantum Mechanics: The Foundation of Quantum Computing\n",
      "\n",
      "Quantum computers overcome these limitations by harnessing the unique principles of quantum mechanics:\n",
      "----------------------------------------\n",
      "Chunk from LLM:  Quantum computers typically consist of qubits, the fundamental building blocks, implemented using various physical systems such as superconducting circuits, trapped ions, photonic systems, and quantum dots. Quantum gates, analogous to logic gates in classical computers, manipulate the states of qubits through superposition and entanglement.  Prominent quantum algorithms include Shor’s algorithm for factoring, Grover’s algorithm for searching databases, and VQE/QAOA for optimization.\n",
      "****************************************\n",
      "Chunk from Normal splitter:  Superposition: Unlike classical bits, which can be either 0 or 1, a quantum bit, or qubit, can exist in a superposition of both states simultaneously. Mathematically, a qubit's state can be represented as a linear combination of the basis states $|0\\rangle$ and $|1\\rangle$: $$|\\psi\\rangle = \\alpha|0\\rangle + \\beta|1\\rangle$$ where $\\alpha$ and $\\beta$ are complex numbers such that $|\\alpha|^2 + |\\beta|^2 = 1$. $|\\alpha|^2$ represents the probability of measuring the qubit in the $|0\\rangle$ state, and $|\\beta|^2$ represents the probability of measuring it in the $|1\\rangle$ state. This ability to be in multiple states at once allows quantum\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for x, y in zip(semantic_chunks[:5], splitted_docs[:5]):\n",
    "    print(\"Chunk from LLM: \", x.page_content)\n",
    "    print(\"*\" * 40)\n",
    "    print(\"Chunk from Normal splitter: \", y.page_content)\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
