{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qU \"torch\" \"python-dotenv\" \"chromadb\" \"sentence-transformers\" \"transformers\" \"psycopg2-binary\" \"rich\" \"pydantic\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/venkat/projects/gen-ai-cookbook/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import abc\n",
    "from enum import Enum\n",
    "import logging\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "from typing import List, Dict, Any, Literal\n",
    "from pprint import pprint\n",
    "import time\n",
    "\n",
    "import chromadb\n",
    "import psutil\n",
    "import torch\n",
    "from chromadb import Documents, EmbeddingFunction, Embeddings\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "from jinja2 import Environment, meta\n",
    "from psycopg2 import connect\n",
    "from rich import print as rprint\n",
    "from rich.console import Console\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers.utils import get_json_schema\n",
    "from pydantic import BaseModel, Field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load environment variables\n",
    "load_dotenv(find_dotenv())\n",
    "console = Console()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining pydantic model for chat history\n",
    "\n",
    "class RoleEnum(str, Enum):\n",
    "    user = \"user\"\n",
    "    assistant = \"assistant\"\n",
    "    system = \"system\"\n",
    "\n",
    "class ChatHistory(BaseModel):\n",
    "    \"\"\"\n",
    "    Pydantic model for chat history.\n",
    "    \"\"\"\n",
    "    role: RoleEnum = Field(..., description=\"The role of the speaker (user or assistant).\")\n",
    "    content: str = Field(..., description=\"The content of the message.\")\n",
    "\n",
    "    class Config:\n",
    "        json_schema_extra = {\n",
    "            \"example\": {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"Hello, how are you?\",\n",
    "            }\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a base class for the models, since we will be experimenting with different models which have different requirements\n",
    "class BaseLLM(abc.ABC):\n",
    "    \"\"\"\n",
    "    Abstract base class for LLM models, defining common functionality.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, model: str, max_history: int = 10, system_prompt: str = \"\", **kwargs\n",
    "    ):\n",
    "        self.logger = logging.getLogger(self.__class__.__name__)\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.system_prompt = system_prompt\n",
    "        self.max_history = max_history\n",
    "        self.history: List[tuple] = []\n",
    "\n",
    "        # Load model and tokenizer\n",
    "        self.tokenizer = None\n",
    "        self.model = None\n",
    "        self.rag_prompt: str = None\n",
    "        self.rag_prompt_template: str = None\n",
    "        self.tools_prompt_template: str = None\n",
    "        self.default_prompt_template: str = None\n",
    "        self.system_template: str = None\n",
    "        self.user_turn_template: str = None\n",
    "        self.assistant_turn_template: str = None\n",
    "        self.assistant_template: str = None\n",
    "        self.non_sys_prompt_template: str = None\n",
    "        self.load_model_and_tokenizer(model, **kwargs)\n",
    "        self.load_rag_prompt()  # This is just defined as a seperate function for keeping the code clean\n",
    "        self.load_prompt_templates()\n",
    "\n",
    "    def load_model_and_tokenizer(self, model: str, **kwargs) -> None:\n",
    "        \"\"\"\n",
    "        Loads the tokenizer and model.\n",
    "        \"\"\"\n",
    "        self.logger.info(\"Initializing tokenizer and model...\")\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "            model, torch_dtype=torch.bfloat16, **kwargs\n",
    "        )\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            model, torch_dtype=torch.bfloat16, **kwargs\n",
    "        )\n",
    "        self.model.to(self.device)\n",
    "\n",
    "        self.logger.info(\"Loaded model: %s\", model)\n",
    "        self.logger.info(\"Model type: %s\", type(self.model).__name__)\n",
    "        self.logger.info(\"Number of parameters: %s\", self.model.num_parameters())\n",
    "        self.logger.info(\"Device: %s\", self.device.type)\n",
    "\n",
    "    def get_token_count(self, text: str) -> int:\n",
    "        \"\"\"\n",
    "        Gets the token count of the given text.\n",
    "        \"\"\"\n",
    "        return len(self.tokenizer(text)[\"input_ids\"])\n",
    "\n",
    "    def trim_conversation(self, conversation_history, token_limit) -> List:\n",
    "        \"\"\"\n",
    "        Trims the conversation history to fit within the given token limit.\n",
    "        \"\"\"\n",
    "        total_tokens = 0\n",
    "        tokenized_history = []\n",
    "\n",
    "        if not conversation_history:\n",
    "            return []\n",
    "\n",
    "        for user, assistant in conversation_history:\n",
    "            user_tokens = self.get_token_count(user)\n",
    "            assistant_tokens = self.get_token_count(assistant)\n",
    "            total_tokens += user_tokens + assistant_tokens\n",
    "            tokenized_history.append((user, assistant, user_tokens + assistant_tokens))\n",
    "\n",
    "        while total_tokens > token_limit and tokenized_history:\n",
    "            removed_entry = tokenized_history.pop(0)\n",
    "            total_tokens -= removed_entry[2]\n",
    "\n",
    "        return [(entry[0], entry[1]) for entry in tokenized_history]\n",
    "\n",
    "    def clear_history(self) -> None:\n",
    "        \"\"\"Clears the stored conversation history.\"\"\"\n",
    "        self.history = []\n",
    "\n",
    "    def add_to_history(self, user_input, model_response) -> None:\n",
    "        \"\"\"Adds an interaction to history and maintains max history size.\"\"\"\n",
    "        _user = {\"role\": \"user\", \"content\": user_input}\n",
    "        _assistant = {\"role\": \"assistant\", \"content\": model_response}\n",
    "        self.history.extend([_user, _assistant])\n",
    "        if len(self.history) > self.max_history:\n",
    "            self.history.pop(0)\n",
    "\n",
    "    # Method for getting the templates\n",
    "    def get_templates(self) -> Dict[str, str]:\n",
    "        \"\"\"\n",
    "        Get the templates from the model.\n",
    "        \"\"\"\n",
    "        self.logger.debug(\"User turn template: \", self.user_turn_template)\n",
    "        self.logger.debug(\"Assistant turn template: \", self.assistant_turn_template)\n",
    "        self.logger.debug(\"Assistant template: \", self.assistant_template)\n",
    "        self.logger.debug(\"RAG prompt: \", self.rag_prompt)\n",
    "        self.logger.debug(\"RAG prompt template: \", self.rag_prompt_template)\n",
    "        self.logger.debug(\"Tools prompt template: \", self.tools_prompt_template)\n",
    "        self.logger.debug(\"Default prompt template: \", self.default_prompt_template)\n",
    "        self.logger.debug(\"Non system prompt template: \", self.non_sys_prompt_template)\n",
    "        self.logger.debug(\"System prompt template: \", self.system_template)\n",
    "        self.logger.debug(\"Tool calling prompt: \", self.tool_calling_prompt)\n",
    "\n",
    "        return {\n",
    "            \"user_turn_template\": self.user_turn_template,\n",
    "            \"assistant_turn_template\": self.assistant_turn_template,\n",
    "            \"assistant_template\": self.assistant_template,\n",
    "            \"rag_prompt\": self.rag_prompt,\n",
    "            \"rag_prompt_template\": self.rag_prompt_template,\n",
    "            \"tools_prompt_template\": self.tools_prompt_template,\n",
    "            \"default_prompt_template\": self.default_prompt_template,\n",
    "            \"non_sys_prompt_template\": self.non_sys_prompt_template,\n",
    "            \"system_prompt_template\": self.system_template,\n",
    "            \"tool_calling_prompt\": self.tool_calling_prompt,\n",
    "        }\n",
    "\n",
    "    def generate_text(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        max_new_tokens: int = 120,\n",
    "        skip_special_tokens: bool = False,\n",
    "        **kwargs,\n",
    "    ) -> str:\n",
    "        \"\"\"\n",
    "        Generates text based on the given prompt.\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        prompt : str\n",
    "            The prompt text to generate text from.\n",
    "        max_new_tokens : int, optional\n",
    "            The maximum length of the generated text (default is 120).\n",
    "        skip_special_tokens : bool, optional\n",
    "            Flag to indicate if special tokens should be skipped (default is False).\n",
    "\n",
    "        Returns:\n",
    "        -------\n",
    "        str\n",
    "            The generated text.\n",
    "        \"\"\"\n",
    "\n",
    "        self.logger.info(\"Generating response for prompt: %s\", prompt)\n",
    "        try:\n",
    "            with torch.inference_mode():\n",
    "                self.logger.debug(\"Tokenizing prompt...\", prompt)\n",
    "                print(\"Tokenizing prompt...\", prompt)\n",
    "                inputs = self.tokenizer(prompt, return_tensors=\"pt\")\n",
    "                inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "\n",
    "                _start_time = time.time()\n",
    "                outputs = self.model.generate(\n",
    "                    **inputs,\n",
    "                    max_new_tokens=max_new_tokens,\n",
    "                    pad_token_id=self.tokenizer.eos_token_id,\n",
    "                    **kwargs,\n",
    "                )\n",
    "                _end_time = time.time()\n",
    "                self.logger.debug(\"Time taken: %.2f seconds\", _end_time - _start_time)\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(\"Error generating response: %s\", e)\n",
    "            return \"Error generating response\"\n",
    "\n",
    "        decoded_output = self.tokenizer.decode(\n",
    "            outputs[0], skip_special_tokens=skip_special_tokens\n",
    "        )\n",
    "        self.logger.debug(\"Generated response: %s\", decoded_output)\n",
    "        print(\"Generated response: \", decoded_output)\n",
    "\n",
    "        return decoded_output\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def chat(\n",
    "        self, prompt: str, clear_session: bool = False, **kwargs\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Abstract method for chatting with the model.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def format_prompt(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        system_prompt: str = None,\n",
    "        tools_schema: str = None,\n",
    "        documents: List[Dict] = None,\n",
    "        create_chat_session: bool = False,\n",
    "        chat_history: List[Dict] = None,\n",
    "    ) -> str:\n",
    "        \"\"\"\n",
    "        Formats the prompt using the prompt template.\n",
    "        \"\"\"\n",
    "\n",
    "        system_prompt = system_prompt or self.system_prompt\n",
    "        final_prompt = prompt\n",
    "\n",
    "        if chat_history:\n",
    "            print(\"Formatting prompt with chat history\")\n",
    "            final_prompt = self.bos_token\n",
    "            self.logger.debug(\"Formatting prompt with chat history\")\n",
    "            # Look for system prompt in chat history\n",
    "            system_prompt = next(\n",
    "                (msg.get('content') for msg in chat_history if msg.get('role') == \"system\"), None\n",
    "            )\n",
    "            if system_prompt:\n",
    "                final_prompt += (\n",
    "                    f\"\\n{self.system_template.format(system_prompt=system_prompt)}\"\n",
    "                )\n",
    "            # Build the formatted prompt by looping over the chat history\n",
    "            for msg in chat_history:\n",
    "                if msg.get('role') == \"user\":\n",
    "                    final_prompt += (\n",
    "                        f\"\\n{self.user_turn_template.format(user_prompt=msg.get('content'))}\"\n",
    "                    )\n",
    "                elif msg.get('role') == \"assistant\":\n",
    "                    final_prompt += f\"\\n{self.assistant_turn_template.format(assistant_response=msg.get('content'))}\"\n",
    "            final_prompt += f\"\\n{self.user_turn_template.format(user_prompt=prompt)}\"  \n",
    "            final_prompt += f\"\\n{self.assistant_template}\"  # Add the assistant template at the end so the model knows it's the assistant's turn\n",
    "            return final_prompt\n",
    "        \n",
    "        if create_chat_session:\n",
    "            print(\"Formatting prompt with chat history - create chat session\")\n",
    "            final_prompt = self.bos_token\n",
    "            self.logger.debug(\"Formatting prompt with chat history\")\n",
    "            if system_prompt:\n",
    "                final_prompt += (\n",
    "                    f\"\\n{self.system_template.format(system_prompt=system_prompt)}\"\n",
    "                )\n",
    "            final_prompt += f\"\\n{self.user_turn_template.format(user_prompt=prompt)}\"\n",
    "            final_prompt += f\"\\n{self.assistant_template}\"\n",
    "            return final_prompt\n",
    "\n",
    "        if tools_schema:\n",
    "            print(\"Formatting prompt with tool schema\", tools_schema)\n",
    "            self.logger.debug(\"Formatting prompt with tool schema\")\n",
    "            formatted_prompt = self.tool_calling_prompt.format(functions_definition=tools_schema)\n",
    "            system_prompt = formatted_prompt\n",
    "            final_prompt = self.tools_prompt_template.format(\n",
    "                system_prompt=system_prompt, user_prompt=prompt\n",
    "            )\n",
    "\n",
    "            return final_prompt\n",
    "\n",
    "        if documents:\n",
    "            print(\"Formatting prompt with documents\")\n",
    "            required_keys = {\"reference\", \"content\"}\n",
    "            assert all(\n",
    "                required_keys.issubset(doc.keys()) for doc in documents\n",
    "            ), \"Documents must contain 'reference' and 'content' keys.\"\n",
    "\n",
    "            self.logger.debug(\"Formatting prompt with documents\")\n",
    "            _documents = \"\\n\".join(\n",
    "                [\n",
    "                    f\"**Document {doc['reference']}**: {doc['content']}\"\n",
    "                    for doc in documents\n",
    "                ]\n",
    "            )\n",
    "            formatted_prompt = self.rag_prompt.format(\n",
    "                documents=_documents, question=prompt\n",
    "            )\n",
    "            system_prompt = formatted_prompt\n",
    "\n",
    "            final_prompt = self.rag_prompt_template.format(\n",
    "                system_prompt=system_prompt, user_prompt=prompt\n",
    "            )\n",
    "\n",
    "            return final_prompt\n",
    "\n",
    "        if system_prompt:\n",
    "            self.logger.debug(\"Formatting prompt with system prompt\")\n",
    "            final_prompt = self.default_prompt_template.format(\n",
    "                system_prompt=system_prompt, user_prompt=prompt\n",
    "            )\n",
    "        else:\n",
    "            final_prompt = self.non_sys_prompt_template.format(user_prompt=prompt)\n",
    "\n",
    "        return final_prompt\n",
    "\n",
    "    def __call__(self, prompt: str, **kwargs) -> str:\n",
    "        \"\"\"\n",
    "        Enables direct inference by calling the model instance.\n",
    "        \"\"\"\n",
    "        return self.generate_response(prompt, **kwargs)\n",
    "\n",
    "    def __repr__(self):\n",
    "        \"\"\"\n",
    "        Official string representation for debugging.\n",
    "        \"\"\"\n",
    "        return f\"{self.__class__.__name__}(model={self.model.name_or_path!r}, device={self.device})\"\n",
    "\n",
    "    def __str__(self):\n",
    "        \"\"\"\n",
    "        User-friendly string representation.\n",
    "        \"\"\"\n",
    "        return f\"{self.__class__.__name__} running on {self.device.type}, max history: {self.max_history}\"\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Returns the number of stored conversation history entries.\n",
    "        \"\"\"\n",
    "        return len(self.history)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Retrieves conversation history entries like an array.\n",
    "        \"\"\"\n",
    "        return self.history[index]\n",
    "\n",
    "    def load_rag_prompt(self):\n",
    "        \"\"\"\n",
    "        Loads the RAG prompt from the model.\n",
    "        \"\"\"\n",
    "        # Check for env variable\n",
    "        if \"RAG_PROMPT\" in os.environ:\n",
    "            self.rag_prompt = os.environ[\"RAG_PROMPT\"]\n",
    "            self.logger.info(\"Loaded RAG prompt from environment variable.\")\n",
    "        else:\n",
    "            self.rag_prompt = (\n",
    "                self.rag_prompt\n",
    "            ) = \"\"\"You are an advanced AI assistant with expertise in retrieving and synthesizing information from provided references. Your role is to analyze the given documents and accurately answer the question based on their content.\n",
    "\n",
    "## Context:\n",
    "You will be provided with multiple documents, each containing relevant information. Each document is referenced with a unique identifier. Your response should be derived strictly from the given documents while maintaining clarity and conciseness. If the documents do not contain sufficient information, indicate that explicitly.\n",
    "\n",
    "## Instructions:\n",
    "1. **Extract information** only from the provided documents.\n",
    "2. **Cite references** where applicable by mentioning the document identifier.\n",
    "3. **Maintain coherence** while summarizing details from multiple sources.\n",
    "4. **Avoid speculation** or adding external knowledge.\n",
    "5. **If unclear**, state that the answer is not available in the provided documents.\n",
    "\n",
    "## Expected Output:\n",
    "- A **concise and accurate** response based on the referenced documents.\n",
    "- **Citations** to the corresponding documents where relevant.\n",
    "- A disclaimer if the answer cannot be found within the given context.\n",
    "\n",
    "## Documents:\n",
    "{documents}\n",
    "\n",
    "\n",
    "## User's Question:\n",
    "{question}\n",
    "\"\"\"\n",
    "            self.logger.info(\"Loaded default RAG prompt.\")\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def generate_response(self, prompt: str, **kwargs) -> str:\n",
    "        \"\"\"\n",
    "        Generates a response to the given prompt.\n",
    "        \"\"\"\n",
    "        return self.generate_text(prompt, **kwargs)\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def load_prompt_templates(self):\n",
    "        \"\"\"\n",
    "        Loads the prompt templates from the model.\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Llama model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LlamaLLM(BaseLLM):\n",
    "    \"\"\"\n",
    "    A class to represent a DOTLLM model for text generation.\n",
    "\n",
    "    Attributes:\n",
    "    ----------\n",
    "    model : str\n",
    "        The model name or path.\n",
    "    max_history : int, optional\n",
    "        The maximum number of history entries to keep (default is 5).\n",
    "    local_files_only : bool, optional\n",
    "        Flag to indicate if the model is local or remote (default is False).\n",
    "    tokenizer : AutoTokenizer\n",
    "        The tokenizer for the model.\n",
    "    model : AutoModelForCausalLM\n",
    "        The model for causal language modeling.\n",
    "    history : list\n",
    "        The history of text inputs.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: str = \"\",\n",
    "        max_history: int = 10,\n",
    "        system_prompt: str = None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Constructs all the necessary attributes for the DOTLLM object.\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        model : str\n",
    "            The model name or path.\n",
    "        max_history : int, optional\n",
    "            The maximum number of history entries to keep (default is 100).\n",
    "        system_prompt : str, optional\n",
    "            The system prompt text (default is \"You are a helpful AI assistant\").\n",
    "            Note: This is only used if prompt_template is provided.\n",
    "        kwargs : dict,\n",
    "            Additional keyword arguments for the model and tokenizer.\n",
    "        \"\"\"\n",
    "        if not model:\n",
    "            _model = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "        else:\n",
    "            _model = model\n",
    "        self.bos_token = \"<|begin_of_text|>\"\n",
    "        self.tool_calling_prompt = \"\"\"You are an expert in composing functions. You are given a question and a set of possible functions. \n",
    "Based on the question, you will need to make one or more function/tool calls to achieve the purpose. \n",
    "If none of the function can be used, point it out. If the given question lacks the parameters required by the function,\n",
    "also point it out. You should only return the function call in tools call sections.\n",
    "\n",
    "If you decide to invoke any of the function(s), you MUST put it in the format of [func_name1(params_name1=params_value1, params_name2=params_value2...), func_name2(params)]\\n\n",
    "You SHOULD NOT include any other text in the response.\n",
    "\n",
    "Here is a list of functions in JSON format that you can invoke.\\n\\n{functions_definition}\\n\"\"\"\n",
    "        super().__init__(_model, max_history, system_prompt, **kwargs)\n",
    "        self.logger.debug(\"Default role of the AI assistant: %s\", system_prompt)\n",
    "\n",
    "    def generate_response(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        system_prompt: str = None,\n",
    "        tools_schema: str = None,\n",
    "        documents: List[Dict] = None,\n",
    "        create_chat_session: bool = False,\n",
    "        chat_history: List[Dict] = None,\n",
    "        max_new_tokens: int = 120,\n",
    "        skip_special_tokens: bool = False,\n",
    "        **kwargs,\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Generates text based on the given prompt.\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        prompt : str\n",
    "            The prompt text to generate text from.\n",
    "        system_prompt : str, optional\n",
    "            The system prompt text (default is None).\n",
    "        tools_schema : str, optional\n",
    "            The schema for the tools prompt (default is None).\n",
    "        documents : list, optional\n",
    "            The list of documents for the RAG prompt (default is None).\n",
    "        create_chat_session : bool, optional\n",
    "            Flag to indicate if a chat session should be created (default is False).\n",
    "        chat_history : list, optional\n",
    "            The chat history for the prompt (default is None).\n",
    "        max_new_tokens : int, optional\n",
    "            The maximum length of the generated text (default is 120).\n",
    "        skip_special_tokens : bool, optional\n",
    "            Flag to indicate if special tokens should be skipped (default is False).\n",
    "\n",
    "        Returns:\n",
    "        -------\n",
    "        str\n",
    "            The generated text.\n",
    "        \"\"\"\n",
    "        _chat_history = []\n",
    "        special_tokens = [\n",
    "            \"<|begin_of_text|>\",\n",
    "            \"<|start_header_id|>\",\n",
    "            \"<|end_header_id|>\",\n",
    "            \"<|eot_id|>\",\n",
    "        ]\n",
    "        # Check if the chat history aligns with the pydantic model\n",
    "        if chat_history:\n",
    "            try:\n",
    "                _ = [ChatHistory(**msg) for msg in chat_history]\n",
    "                _chat_history.extend(chat_history)\n",
    "            except Exception as e:\n",
    "                self.logger.error(\"Error validating chat history: %s\", e)\n",
    "        input_prompt = self.format_prompt(\n",
    "            prompt,\n",
    "            system_prompt=system_prompt,\n",
    "            tools_schema=tools_schema,\n",
    "            documents=documents,\n",
    "            create_chat_session=create_chat_session,\n",
    "            chat_history=chat_history,\n",
    "        )\n",
    "\n",
    "        model_response = self.generate_text(\n",
    "            input_prompt,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            skip_special_tokens=skip_special_tokens,\n",
    "            **kwargs,\n",
    "        )\n",
    "        # removing the prompt and special tokens from the model response\n",
    "        model_response = model_response.replace(input_prompt, \"\")\n",
    "        for token in special_tokens:\n",
    "            model_response = model_response.replace(token, \"\")\n",
    "        model_response = model_response.strip()\n",
    "        # Add the user input and model response to the chat history\n",
    "        _chat_history.append({\"role\": \"user\", \"content\": prompt})\n",
    "        _chat_history.append({\"role\": \"assistant\", \"content\": model_response})\n",
    "\n",
    "        return {\"response\": model_response, \"chat_history\": _chat_history}\n",
    "\n",
    "    def chat(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        chat_history: List[Dict] = None,\n",
    "        clear_session: bool = False,\n",
    "        **kwargs,\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Chat with the model.\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        prompt : str\n",
    "            The user prompt.\n",
    "        clear_session : bool, optional\n",
    "            Flag to indicate if the session history should be cleared (default is False).\n",
    "\n",
    "        Returns:\n",
    "        -------\n",
    "        dict\n",
    "            The response and chat history.\n",
    "        \"\"\"\n",
    "        _history_checker: bool = (\n",
    "            True  # flag to see if the chat history is passed, so we can return the chat history in the response without affecting original\n",
    "        )\n",
    "        if clear_session:\n",
    "            self.clear_history()\n",
    "\n",
    "        # Initialize chat history if not provided\n",
    "        if chat_history is None:\n",
    "            chat_history = []\n",
    "            _history_checker = False\n",
    "\n",
    "        # Determine if we need to create a new chat session\n",
    "        create_chat_session = not self.history and not chat_history\n",
    "\n",
    "        # If self.history exists, use it as chat_history\n",
    "        if self.history and not chat_history:\n",
    "            chat_history = self.history\n",
    "        # Adding the chat prompt to chat history\n",
    "        generated_response = self.generate_response(\n",
    "            prompt,\n",
    "            create_chat_session=create_chat_session,\n",
    "            chat_history=chat_history,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "        extracted_response = generated_response.get(\n",
    "            \"response\", \"Error generating response\"\n",
    "        )\n",
    "\n",
    "        # If no chat history is passed, add the user input and model response to the history\n",
    "        if not _history_checker:\n",
    "            self.add_to_history(prompt, extracted_response)\n",
    "            generated_response[\"chat_history\"] = self.history\n",
    "        else:  # if chat history is passed, return the chat history as is\n",
    "            generated_response[\"chat_history\"] = chat_history\n",
    "            generated_response[\"chat_history\"].extend(\n",
    "                [\n",
    "                    {\"role\": \"user\", \"content\": prompt},\n",
    "                    {\"role\": \"assistant\", \"content\": extracted_response},\n",
    "                ]\n",
    "            )\n",
    "\n",
    "        return generated_response\n",
    "\n",
    "    def load_prompt_templates(self):\n",
    "        \"\"\"\n",
    "        Loads the prompt templates for the Llama.\n",
    "        \"\"\"\n",
    "        self.system_template = (\n",
    "            \"<|start_header_id|>system<|end_header_id|> {system_prompt} <|eot_id|>\"\n",
    "        )\n",
    "        self.user_turn_template = (\n",
    "            \"<|start_header_id|>user<|end_header_id|> {user_prompt} <|eot_id|>\"\n",
    "        )\n",
    "        self.assistant_turn_template = \"<|start_header_id|>assistant<|end_header_id|> {assistant_response} <|eot_id|>\"\n",
    "        self.assistant_template = \"<|start_header_id|>assistant<|end_header_id|>\"\n",
    "        self.rag_prompt_template = f\"{self.bos_token }\\n{self.system_template}\\n{self.user_turn_template}\\n{self.assistant_template}\"\n",
    "        self.tools_prompt_template = f\"{self.bos_token }\\n{self.system_template}\\n{self.user_turn_template}\\n{self.assistant_template}\"\n",
    "        self.default_prompt_template = f\"{self.bos_token }\\n{self.system_template}\\n{self.user_turn_template}\\n{self.assistant_template}\"\n",
    "        self.non_sys_prompt_template = (\n",
    "            f\"{self.bos_token }\\n{self.user_turn_template}\\n{self.assistant_template}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phi model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PhiLLM(BaseLLM):\n",
    "    \"\"\"\n",
    "    A class to represent a DOTLLM model for text generation.\n",
    "\n",
    "    Attributes:\n",
    "    ----------\n",
    "    model : str\n",
    "        The model name or path.\n",
    "    max_history : int, optional\n",
    "        The maximum number of history entries to keep (default is 5).\n",
    "    local_files_only : bool, optional\n",
    "        Flag to indicate if the model is local or remote (default is False).\n",
    "    tokenizer : AutoTokenizer\n",
    "        The tokenizer for the model.\n",
    "    model : AutoModelForCausalLM\n",
    "        The model for causal language modeling.\n",
    "    history : list\n",
    "        The history of text inputs.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: str = \"\",\n",
    "        max_history: int = 10,\n",
    "        system_prompt: str = None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Constructs all the necessary attributes for the DOTLLM object.\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        model : str\n",
    "            The model name or path.\n",
    "        max_history : int, optional\n",
    "            The maximum number of history entries to keep (default is 100).\n",
    "        system_prompt : str, optional\n",
    "            The system prompt text (default is \"You are a helpful AI assistant\").\n",
    "            Note: This is only used if prompt_template is provided.\n",
    "        kwargs : dict,\n",
    "            Additional keyword arguments for the model and tokenizer.\n",
    "        \"\"\"\n",
    "        if not model:\n",
    "            _model = \"microsoft/Phi-4-mini-instruct\"\n",
    "        else:\n",
    "            _model = model\n",
    "        self.bos_token = \"\"\n",
    "        self.tool_calling_prompt = \"\"\"You are an expert in composing functions. You are given a question and a set of possible functions. \n",
    "Based on the question, you will need to make one or more function/tool calls to achieve the purpose. \n",
    "If none of the function can be used, point it out. If the given question lacks the parameters required by the function,\n",
    "also point it out. You should only return the function call in tools call sections.\n",
    "\n",
    "If you decide to invoke any of the function(s), you MUST put it in the format of [func_name1(params_name1=params_value1, params_name2=params_value2...), func_name2(params)]\\n\n",
    "You SHOULD NOT include any other text in the response.\n",
    "\n",
    "Here is a list of functions in JSON format that you can invoke.\\n\\n{functions_definition}\\n\"\"\"\n",
    "        super().__init__(_model, max_history, system_prompt, **kwargs)\n",
    "        self.logger.debug(\"Default role of the AI assistant: %s\", system_prompt)\n",
    "\n",
    "    def generate_response(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        system_prompt: str = None,\n",
    "        tools_schema: str = None,\n",
    "        documents: List[Dict] = None,\n",
    "        create_chat_session: bool = False,\n",
    "        chat_history: List[Dict] = None,\n",
    "        max_new_tokens: int = 120,\n",
    "        skip_special_tokens: bool = False,\n",
    "        **kwargs,\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Generates text based on the given prompt.\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        prompt : str\n",
    "            The prompt text to generate text from.\n",
    "        system_prompt : str, optional\n",
    "            The system prompt text (default is None).\n",
    "        tools_schema : str, optional\n",
    "            The schema for the tools prompt (default is None).\n",
    "        documents : list, optional\n",
    "            The list of documents for the RAG prompt (default is None).\n",
    "        create_chat_session : bool, optional\n",
    "            Flag to indicate if a chat session should be created (default is False).\n",
    "        chat_history : list, optional\n",
    "            The chat history for the prompt (default is None).\n",
    "        max_new_tokens : int, optional\n",
    "            The maximum length of the generated text (default is 120).\n",
    "        skip_special_tokens : bool, optional\n",
    "            Flag to indicate if special tokens should be skipped (default is False).\n",
    "\n",
    "        Returns:\n",
    "        -------\n",
    "        str\n",
    "            The generated text.\n",
    "        \"\"\"\n",
    "        _chat_history = []\n",
    "        special_tokens = [\n",
    "            \"<|system|>\",\n",
    "            \"<|user|>\",\n",
    "            \"<|assistant|> \",\n",
    "            \"<|end|>\",\n",
    "        ]\n",
    "        # Check if the chat history aligns with the pydantic model\n",
    "        if chat_history:\n",
    "            try:\n",
    "                _ = [ChatHistory(**msg) for msg in chat_history]\n",
    "                _chat_history.extend(chat_history)\n",
    "            except Exception as e:\n",
    "                self.logger.error(\"Error validating chat history: %s\", e)\n",
    "        input_prompt = self.format_prompt(\n",
    "            prompt,\n",
    "            system_prompt=system_prompt,\n",
    "            tools_schema=tools_schema,\n",
    "            documents=documents,\n",
    "            create_chat_session=create_chat_session,\n",
    "            chat_history=chat_history,\n",
    "        )\n",
    "\n",
    "        model_response = self.generate_text(\n",
    "            input_prompt,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            skip_special_tokens=skip_special_tokens,\n",
    "            **kwargs,\n",
    "        )\n",
    "        # removing the prompt and special tokens from the model response\n",
    "        model_response = model_response.replace(input_prompt, \"\")\n",
    "        for token in special_tokens:\n",
    "            model_response = model_response.replace(token, \"\")\n",
    "        model_response = model_response.strip()\n",
    "        # Add the user input and model response to the chat history\n",
    "        _chat_history.append({\"role\": \"user\", \"content\": prompt})\n",
    "        _chat_history.append({\"role\": \"assistant\", \"content\": model_response})\n",
    "\n",
    "        return {\"response\": model_response, \"chat_history\": _chat_history}\n",
    "\n",
    "    def chat(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        chat_history: List[Dict] = None,\n",
    "        clear_session: bool = False,\n",
    "        **kwargs,\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Chat with the model.\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        prompt : str\n",
    "            The user prompt.\n",
    "        clear_session : bool, optional\n",
    "            Flag to indicate if the session history should be cleared (default is False).\n",
    "\n",
    "        Returns:\n",
    "        -------\n",
    "        dict\n",
    "            The response and chat history.\n",
    "        \"\"\"\n",
    "        _history_checker: bool = (\n",
    "            True  # flag to see if the chat history is passed, so we can return the chat history in the response without affecting original\n",
    "        )\n",
    "        if clear_session:\n",
    "            self.clear_history()\n",
    "\n",
    "        # Initialize chat history if not provided\n",
    "        if chat_history is None:\n",
    "            chat_history = []\n",
    "            _history_checker = False\n",
    "\n",
    "        # Determine if we need to create a new chat session\n",
    "        create_chat_session = not self.history and not chat_history\n",
    "\n",
    "        # If self.history exists, use it as chat_history\n",
    "        if self.history and not chat_history:\n",
    "            chat_history = self.history\n",
    "        # Adding the chat prompt to chat history\n",
    "        generated_response = self.generate_response(\n",
    "            prompt,\n",
    "            create_chat_session=create_chat_session,\n",
    "            chat_history=chat_history,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "        extracted_response = generated_response.get(\n",
    "            \"response\", \"Error generating response\"\n",
    "        )\n",
    "\n",
    "        # If no chat history is passed, add the user input and model response to the history\n",
    "        if not _history_checker:\n",
    "            self.add_to_history(prompt, extracted_response)\n",
    "            generated_response[\"chat_history\"] = self.history\n",
    "        else:  # if chat history is passed, return the chat history as is\n",
    "            generated_response[\"chat_history\"] = chat_history\n",
    "            generated_response[\"chat_history\"].extend(\n",
    "                [\n",
    "                    {\"role\": \"user\", \"content\": prompt},\n",
    "                    {\"role\": \"assistant\", \"content\": extracted_response},\n",
    "                ]\n",
    "            )\n",
    "\n",
    "        return generated_response\n",
    "\n",
    "    def load_prompt_templates(self):\n",
    "        \"\"\"\n",
    "        Loads the prompt templates for the Llama.\n",
    "        \"\"\"\n",
    "        self.system_template = (\n",
    "            \"<|system|> {system_prompt} <|end|>\"\n",
    "        )\n",
    "        self.user_turn_template = (\n",
    "            \"<|user|> {user_prompt} <|end|>\"\n",
    "        )\n",
    "        self.assistant_turn_template = \"<|assistant|>  {assistant_response} <|end|>\"\n",
    "        self.assistant_template = \"<|assistant|>\"\n",
    "        self.rag_prompt_template = f\"{self.bos_token }\\n{self.system_template}\\n{self.user_turn_template}\\n{self.assistant_template}\"\n",
    "        self.tools_prompt_template = f\"{self.bos_token }\\n{self.system_template}\\n{self.user_turn_template}\\n{self.assistant_template}\"\n",
    "        self.default_prompt_template = f\"{self.bos_token }\\n{self.system_template}\\n{self.user_turn_template}\\n{self.assistant_template}\"\n",
    "        self.non_sys_prompt_template = (\n",
    "            f\"{self.bos_token }\\n{self.user_turn_template}\\n{self.assistant_template}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "function_definitions = \"\"\"[\n",
    "    {\n",
    "        \"name\": \"get_user_info\",\n",
    "        \"description\": \"Retrieve details for a specific user by their unique identifier. Note that the provided function is in Python 3 syntax.\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"dict\",\n",
    "            \"required\": [\n",
    "                \"user_id\"\n",
    "            ],\n",
    "            \"properties\": {\n",
    "                \"user_id\": {\n",
    "                \"type\": \"integer\",\n",
    "                \"description\": \"The unique identifier of the user. It is used to fetch the specific user details from the database.\"\n",
    "            },\n",
    "            \"special\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"Any special information or parameters that need to be considered while fetching user details.\",\n",
    "                \"default\": \"none\"\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\n",
    "    {\n",
    "        \"reference\": \"Doc1\",\n",
    "        \"content\": \"Quantum computing leverages quantum mechanics to perform computations at speeds unattainable by classical computers. It relies on principles like superposition, where quantum bits (qubits) exist in multiple states simultaneously, and entanglement, which enables qubits to be linked regardless of distance. These properties allow quantum computers to solve complex problems efficiently. Current research is focused on improving qubit stability and error correction.\"\n",
    "    },\n",
    "    {\n",
    "        \"reference\": \"Doc2\",\n",
    "        \"content\": \"The theory of relativity, proposed by Albert Einstein, revolutionized our understanding of space and time. It consists of special relativity, which deals with objects moving at high velocities, and general relativity, which explains gravity as the curvature of spacetime. This theory has been experimentally confirmed through observations like gravitational lensing and time dilation. Modern GPS systems rely on relativity corrections for accurate positioning.\"\n",
    "    },\n",
    "    {\n",
    "        \"reference\": \"Doc3\",\n",
    "        \"content\": \"Machine learning is a subset of artificial intelligence that enables computers to learn from data without explicit programming. It includes supervised, unsupervised, and reinforcement learning techniques. These models are used in applications like image recognition, fraud detection, and recommendation systems. The effectiveness of a machine learning model depends on the quality and quantity of training data.\"\n",
    "    },\n",
    "    {\n",
    "        \"reference\": \"Doc4\",\n",
    "        \"content\": \"Blockchain technology provides a decentralized and secure way to record transactions. It uses cryptographic hashing and distributed consensus to ensure data integrity. Originally developed for Bitcoin, blockchain is now used in supply chain management, digital identity, and smart contracts. The technology faces challenges like scalability and energy consumption.\"\n",
    "    },\n",
    "    {\n",
    "        \"reference\": \"Doc5\",\n",
    "        \"content\": \"The human brain consists of billions of neurons that communicate through electrical and chemical signals. Neural networks in artificial intelligence are inspired by this biological structure. The brain's plasticity allows it to adapt and learn new information throughout life. Research in neuroscience is uncovering new treatments for cognitive disorders.\"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models comparision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "# load Phi model\n",
    "phi_model = PhiLLM()\n",
    "phi_model.get_templates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
