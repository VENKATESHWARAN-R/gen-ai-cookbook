{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Local Models playbook\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qU \"torch\" \"python-dotenv\" \"chromadb\" \"sentence-transformers\" \"transformers\" \"psycopg2-binary\" \"rich\" \"pydantic\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/venkat/projects/gen-ai-cookbook/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import abc\n",
    "from enum import Enum\n",
    "import logging\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "from typing import List, Dict, Any, Literal\n",
    "from pprint import pprint\n",
    "import time\n",
    "\n",
    "import chromadb\n",
    "import psutil\n",
    "import torch\n",
    "from chromadb import Documents, EmbeddingFunction, Embeddings\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "from jinja2 import Environment, meta\n",
    "from psycopg2 import connect\n",
    "from rich import print as rprint\n",
    "from rich.console import Console\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers.utils import get_json_schema\n",
    "from pydantic import BaseModel, Field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load environment variables\n",
    "load_dotenv(find_dotenv())\n",
    "console = Console()\n",
    "\n",
    "# Local models\n",
    "local_models = {\n",
    "    \"llama-mini\": \"meta-llama/Llama-3.2-1B-Instruct\",\n",
    "    \"llama\": \"meta-llama/Llama-3.2-3B-Instruct\",\n",
    "    \"qwen-mini\": \"Qwen/Qwen2.5-3B-Instruct\",\n",
    "    \"qwen\": \"Qwen/Qwen2.5-7B-Instruct\",\n",
    "    \"gemma-mini\": \"google/gemma-2-2b-it\",\n",
    "    \"gemma\": \"google/gemma-2-9b-it\",\n",
    "    \"phi-mini\": \"microsoft/Phi-4-mini-instruct\",\n",
    "    \"phi\": \"microsoft/Phi-4-multimodal-instruct\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Model: llama</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;32mModel: llama\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{{- bos_token }}\n",
      "{%- if custom_tools is defined %}\n",
      "    {%- set tools = custom_tools %}\n",
      "{%- endif %}\n",
      "{%- if not tools_in_user_message is defined %}\n",
      "    {%- set tools_in_user_message = true %}\n",
      "{%- endif %}\n",
      "{%- if not date_string is defined %}\n",
      "    {%- if strftime_now is defined %}\n",
      "        {%- set date_string = strftime_now(\"%d %b %Y\") %}\n",
      "    {%- else %}\n",
      "        {%- set date_string = \"26 Jul 2024\" %}\n",
      "    {%- endif %}\n",
      "{%- endif %}\n",
      "{%- if not tools is defined %}\n",
      "    {%- set tools = none %}\n",
      "{%- endif %}\n",
      "\n",
      "{#- This block extracts the system message, so we can slot it into the right place. #}\n",
      "{%- if messages[0]['role'] == 'system' %}\n",
      "    {%- set system_message = messages[0]['content']|trim %}\n",
      "    {%- set messages = messages[1:] %}\n",
      "{%- else %}\n",
      "    {%- set system_message = \"\" %}\n",
      "{%- endif %}\n",
      "\n",
      "{#- System message #}\n",
      "{{- \"<|start_header_id|>system<|end_header_id|>\\n\\n\" }}\n",
      "{%- if tools is not none %}\n",
      "    {{- \"Environment: ipython\\n\" }}\n",
      "{%- endif %}\n",
      "{{- \"Cutting Knowledge Date: December 2023\\n\" }}\n",
      "{{- \"Today Date: \" + date_string + \"\\n\\n\" }}\n",
      "{%- if tools is not none and not tools_in_user_message %}\n",
      "    {{- \"You have access to the following functions. To call a function, please respond with JSON for a function call.\" }}\n",
      "    {{- 'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.' }}\n",
      "    {{- \"Do not use variables.\\n\\n\" }}\n",
      "    {%- for t in tools %}\n",
      "        {{- t | tojson(indent=4) }}\n",
      "        {{- \"\\n\\n\" }}\n",
      "    {%- endfor %}\n",
      "{%- endif %}\n",
      "{{- system_message }}\n",
      "{{- \"<|eot_id|>\" }}\n",
      "\n",
      "{#- Custom tools are passed in a user message with some extra guidance #}\n",
      "{%- if tools_in_user_message and not tools is none %}\n",
      "    {#- Extract the first user message so we can plug it in here #}\n",
      "    {%- if messages | length != 0 %}\n",
      "        {%- set first_user_message = messages[0]['content']|trim %}\n",
      "        {%- set messages = messages[1:] %}\n",
      "    {%- else %}\n",
      "        {{- raise_exception(\"Cannot put tools in the first user message when there's no first user message!\") }}\n",
      "{%- endif %}\n",
      "    {{- '<|start_header_id|>user<|end_header_id|>\\n\\n' -}}\n",
      "    {{- \"Given the following functions, please respond with a JSON for a function call \" }}\n",
      "    {{- \"with its proper arguments that best answers the given prompt.\\n\\n\" }}\n",
      "    {{- 'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.' }}\n",
      "    {{- \"Do not use variables.\\n\\n\" }}\n",
      "    {%- for t in tools %}\n",
      "        {{- t | tojson(indent=4) }}\n",
      "        {{- \"\\n\\n\" }}\n",
      "    {%- endfor %}\n",
      "    {{- first_user_message + \"<|eot_id|>\"}}\n",
      "{%- endif %}\n",
      "\n",
      "{%- for message in messages %}\n",
      "    {%- if not (message.role == 'ipython' or message.role == 'tool' or 'tool_calls' in message) %}\n",
      "        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' }}\n",
      "    {%- elif 'tool_calls' in message %}\n",
      "        {%- if not message.tool_calls|length == 1 %}\n",
      "            {{- raise_exception(\"This model only supports single tool-calls at once!\") }}\n",
      "        {%- endif %}\n",
      "        {%- set tool_call = message.tool_calls[0].function %}\n",
      "        {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' -}}\n",
      "        {{- '{\"name\": \"' + tool_call.name + '\", ' }}\n",
      "        {{- '\"parameters\": ' }}\n",
      "        {{- tool_call.arguments | tojson }}\n",
      "        {{- \"}\" }}\n",
      "        {{- \"<|eot_id|>\" }}\n",
      "    {%- elif message.role == \"tool\" or message.role == \"ipython\" %}\n",
      "        {{- \"<|start_header_id|>ipython<|end_header_id|>\\n\\n\" }}\n",
      "        {%- if message.content is mapping or message.content is iterable %}\n",
      "            {{- message.content | tojson }}\n",
      "        {%- else %}\n",
      "            {{- message.content }}\n",
      "        {%- endif %}\n",
      "        {{- \"<|eot_id|>\" }}\n",
      "    {%- endif %}\n",
      "{%- endfor %}\n",
      "{%- if add_generation_prompt %}\n",
      "    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n",
      "{%- endif %}\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">**************************************************</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;30m**************************************************\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Model: qwen</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;32mModel: qwen\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{%- if tools %}\n",
      "    {{- '<|im_start|>system\\n' }}\n",
      "    {%- if messages[0]['role'] == 'system' %}\n",
      "        {{- messages[0]['content'] }}\n",
      "    {%- else %}\n",
      "        {{- 'You are Qwen, created by Alibaba Cloud. You are a helpful assistant.' }}\n",
      "    {%- endif %}\n",
      "    {{- \"\\n\\n# Tools\\n\\nYou may call one or more functions to assist with the user query.\\n\\nYou are provided with function signatures within <tools></tools> XML tags:\\n<tools>\" }}\n",
      "    {%- for tool in tools %}\n",
      "        {{- \"\\n\" }}\n",
      "        {{- tool | tojson }}\n",
      "    {%- endfor %}\n",
      "    {{- \"\\n</tools>\\n\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\n<tool_call>\\n{\\\"name\\\": <function-name>, \\\"arguments\\\": <args-json-object>}\\n</tool_call><|im_end|>\\n\" }}\n",
      "{%- else %}\n",
      "    {%- if messages[0]['role'] == 'system' %}\n",
      "        {{- '<|im_start|>system\\n' + messages[0]['content'] + '<|im_end|>\\n' }}\n",
      "    {%- else %}\n",
      "        {{- '<|im_start|>system\\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\\n' }}\n",
      "    {%- endif %}\n",
      "{%- endif %}\n",
      "{%- for message in messages %}\n",
      "    {%- if (message.role == \"user\") or (message.role == \"system\" and not loop.first) or (message.role == \"assistant\" and not message.tool_calls) %}\n",
      "        {{- '<|im_start|>' + message.role + '\\n' + message.content + '<|im_end|>' + '\\n' }}\n",
      "    {%- elif message.role == \"assistant\" %}\n",
      "        {{- '<|im_start|>' + message.role }}\n",
      "        {%- if message.content %}\n",
      "            {{- '\\n' + message.content }}\n",
      "        {%- endif %}\n",
      "        {%- for tool_call in message.tool_calls %}\n",
      "            {%- if tool_call.function is defined %}\n",
      "                {%- set tool_call = tool_call.function %}\n",
      "            {%- endif %}\n",
      "            {{- '\\n<tool_call>\\n{\"name\": \"' }}\n",
      "            {{- tool_call.name }}\n",
      "            {{- '\", \"arguments\": ' }}\n",
      "            {{- tool_call.arguments | tojson }}\n",
      "            {{- '}\\n</tool_call>' }}\n",
      "        {%- endfor %}\n",
      "        {{- '<|im_end|>\\n' }}\n",
      "    {%- elif message.role == \"tool\" %}\n",
      "        {%- if (loop.index0 == 0) or (messages[loop.index0 - 1].role != \"tool\") %}\n",
      "            {{- '<|im_start|>user' }}\n",
      "        {%- endif %}\n",
      "        {{- '\\n<tool_response>\\n' }}\n",
      "        {{- message.content }}\n",
      "        {{- '\\n</tool_response>' }}\n",
      "        {%- if loop.last or (messages[loop.index0 + 1].role != \"tool\") %}\n",
      "            {{- '<|im_end|>\\n' }}\n",
      "        {%- endif %}\n",
      "    {%- endif %}\n",
      "{%- endfor %}\n",
      "{%- if add_generation_prompt %}\n",
      "    {{- '<|im_start|>assistant\\n' }}\n",
      "{%- endif %}\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">**************************************************</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;30m**************************************************\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Model: gemma</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;32mModel: gemma\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\n",
      "' + message['content'] | trim + '<end_of_turn>\n",
      "' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\n",
      "'}}{% endif %}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">**************************************************</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;30m**************************************************\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Model: phi</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;32mModel: phi\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{% for message in messages %}{% if message['role'] == 'system' and 'tools' in message and message['tools'] is not none %}{{ '<|' + message['role'] + '|>' + message['content'] + '<|tool|>' + message['tools'] + '<|/tool|>' + '<|end|>' }}{% else %}{{ '<|' + message['role'] + '|>' + message['content'] + '<|end|>' }}{% endif %}{% endfor %}{% if add_generation_prompt %}{{ '<|assistant|>' }}{% else %}{{ eos_token }}{% endif %}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">**************************************************</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;30m**************************************************\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Let's loop through the models and see their chat templates\n",
    "for model_name, model_id in local_models.items():\n",
    "    if \"mini\" in model_name:\n",
    "        continue\n",
    "    console.print(f\"Model: {model_name}\", style=\"bold green\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    print(tokenizer.get_chat_template())\n",
    "    console.print(\"*\" * 50, style=\"bold black\")\n",
    "    # console.print(tokenizer.get_chat_template(), style=\"Red on White\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating dummy functions for testing\n",
    "\n",
    "\n",
    "def current_time():\n",
    "    \"\"\"Get the current local time as a string.\"\"\"\n",
    "    return str(datetime.now())\n",
    "\n",
    "\n",
    "def multiply(a: float, b: float):\n",
    "    \"\"\"\n",
    "    A function that multiplies two numbers\n",
    "\n",
    "    Args:\n",
    "        a: The first number to multiply\n",
    "        b: The second number to multiply\n",
    "    \"\"\"\n",
    "    return a * b\n",
    "\n",
    "\n",
    "tools = [current_time, multiply]\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"Your name is Iida, You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Tell me something about large language models.\"},\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": \"Large language models are powerful models that can generate human-like text.\",\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Can you show me an example of a large language model?\",\n",
    "    },\n",
    "]\n",
    "\n",
    "fallback_messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Hello, how are you?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"I'm doing great. How can I help you today?\"},\n",
    "    {\"role\": \"user\", \"content\": \"I'd like to show off how chat templating works!\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage: 63.6%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000; background-color: #c0c0c0\">Model: llama</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31;47mModel: llama\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; background-color: #808000; font-weight: bold; font-style: italic\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; background-color: #808000; font-weight: bold; font-style: italic\">|begin_of_text|</span><span style=\"color: #000000; text-decoration-color: #000000; background-color: #808000; font-style: italic\">&gt;&lt;|start_header_id|&gt;system&lt;|end_header_id|&gt;</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; background-color: #808000; font-style: italic\">Environment: ipython</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; background-color: #808000; font-style: italic\">Cutting Knowledge Date: December </span><span style=\"color: #008080; text-decoration-color: #008080; background-color: #808000; font-weight: bold\">2023</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; background-color: #808000; font-style: italic\">Today Date: </span><span style=\"color: #008080; text-decoration-color: #008080; background-color: #808000; font-weight: bold\">12</span><span style=\"color: #000000; text-decoration-color: #000000; background-color: #808000; font-style: italic\"> Mar </span><span style=\"color: #008080; text-decoration-color: #008080; background-color: #808000; font-weight: bold\">2025</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; background-color: #808000; font-style: italic\">Your name is Iida, You are a helpful assistant.&lt;|eot_id|&gt;&lt;|start_header_id|&gt;user&lt;|end_header_id|&gt;</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; background-color: #808000; font-style: italic\">Given the following functions, please respond with a JSON for a function call with its proper arguments that best </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; background-color: #808000; font-style: italic\">answers the given prompt.</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; background-color: #808000; font-style: italic\">Respond in the format </span><span style=\"color: #000000; text-decoration-color: #000000; background-color: #808000; font-weight: bold; font-style: italic\">{</span><span style=\"color: #008000; text-decoration-color: #008000; background-color: #808000\">\"name\"</span><span style=\"color: #000000; text-decoration-color: #000000; background-color: #808000; font-style: italic\">: function name, </span><span style=\"color: #008000; text-decoration-color: #008000; background-color: #808000\">\"parameters\"</span><span style=\"color: #000000; text-decoration-color: #000000; background-color: #808000; font-style: italic\">: dictionary of argument name and its value</span><span style=\"color: #000000; text-decoration-color: #000000; background-color: #808000; font-weight: bold; font-style: italic\">}</span><span style=\"color: #000000; text-decoration-color: #000000; background-color: #808000; font-style: italic\">.Do not use </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; background-color: #808000; font-style: italic\">variables.</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; background-color: #808000; font-weight: bold; font-style: italic\">{</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; background-color: #808000; font-style: italic\">    </span><span style=\"color: #008000; text-decoration-color: #008000; background-color: #808000\">\"type\"</span><span style=\"color: #000000; text-decoration-color: #000000; background-color: #808000; font-style: italic\">: </span><span style=\"color: #008000; text-decoration-color: #008000; background-color: #808000\">\"function\"</span><span style=\"color: #000000; text-decoration-color: #000000; background-color: #808000; font-style: italic\">,</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; background-color: #808000; font-style: italic\">    </span><span style=\"color: #008000; text-decoration-color: #008000; background-color: #808000\">\"function\"</span><span style=\"color: #000000; text-decoration-color: #000000; background-color: #808000; font-style: italic\">: </span><span style=\"color: #000000; text-decoration-color: #000000; background-color: #808000; font-weight: bold; font-style: italic\">{</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; background-color: #808000; font-style: italic\">        </span><span style=\"color: #008000; text-decoration-color: #008000; background-color: #808000\">\"name\"</span><span style=\"color: #000000; text-decoration-color: #000000; background-color: #808000; font-style: italic\">: </span><span style=\"color: #008000; text-decoration-color: #008000; background-color: #808000\">\"current_time\"</span><span style=\"color: #000000; text-decoration-color: #000000; background-color: #808000; font-style: italic\">,</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; background-color: #808000; font-style: italic\">        </span><span style=\"color: #008000; text-decoration-color: #008000; background-color: #808000\">\"description\"</span><span style=\"color: #000000; text-decoration-color: #000000; background-color: #808000; font-style: italic\">: </span><span style=\"color: #008000; text-decoration-color: #008000; background-color: #808000\">\"Get the current local time as a string.\"</span><span style=\"color: #000000; text-decoration-color: #000000; background-color: #808000; font-style: italic\">,</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; background-color: #808000; font-style: italic\">        </span><span style=\"color: #008000; text-decoration-color: #008000; background-color: #808000\">\"parameters\"</span><span style=\"color: #000000; text-decoration-color: #000000; background-color: #808000; font-style: italic\">: </span><span style=\"color: #000000; text-decoration-color: #000000; background-color: #808000; font-weight: bold; font-style: italic\">{</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; background-color: #808000; font-style: italic\">            </span><span style=\"color: #008000; text-decoration-color: #008000; background-color: #808000\">\"type\"</span><span style=\"color: #000000; text-decoration-color: #000000; background-color: #808000; font-style: italic\">: </span><span style=\"color: #008000; text-decoration-color: #008000; background-color: #808000\">\"object\"</span><span style=\"color: #000000; text-decoration-color: #000000; background-color: #808000; font-style: italic\">,</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; background-color: #808000; font-style: italic\">            </span><span style=\"color: #008000; text-decoration-color: #008000; background-color: #808000\">\"properties\"</span><span style=\"color: #000000; text-decoration-color: #000000; background-color: #808000; font-style: italic\">: </span><span style=\"color: #000000; text-decoration-color: #000000; background-color: #808000; font-weight: bold; font-style: italic\">{}</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; background-color: #808000; font-style: italic\">        </span><span style=\"color: #000000; text-decoration-color: #000000; background-color: #808000; font-weight: bold; font-style: italic\">}</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; background-color: #808000; font-style: italic\">    </span><span style=\"color: #000000; text-decoration-color: #000000; background-color: #808000; font-weight: bold; font-style: italic\">}</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; background-color: #808000; font-weight: bold; font-style: italic\">}</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; background-color: #808000; font-weight: bold; font-style: italic\">{</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; background-color: #808000; font-style: italic\">    </span><span style=\"color: #008000; text-decoration-color: #008000; background-color: #808000\">\"type\"</span><span style=\"color: #000000; text-decoration-color: #000000; background-color: #808000; font-style: italic\">: </span><span style=\"color: #008000; text-decoration-color: #008000; background-color: #808000\">\"function\"</span><span style=\"color: #000000; text-decoration-color: #000000; background-color: #808000; font-style: italic\">,</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; background-color: #808000; font-style: italic\">    </span><span style=\"color: #008000; text-decoration-color: #008000; background-color: #808000\">\"function\"</span><span style=\"color: #000000; text-decoration-color: #000000; background-color: #808000; font-style: italic\">: </span><span style=\"color: #000000; text-decoration-color: #000000; background-color: #808000; font-weight: bold; font-style: italic\">{</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; background-color: #808000; font-style: italic\">        </span><span style=\"color: #008000; text-decoration-color: #008000; background-color: #808000\">\"name\"</span><span style=\"color: #000000; text-decoration-color: #000000; background-color: #808000; font-style: italic\">: </span><span style=\"color: #008000; text-decoration-color: #008000; background-color: #808000\">\"multiply\"</span><span style=\"color: #000000; text-decoration-color: #000000; background-color: #808000; font-style: italic\">,</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; background-color: #808000; font-style: italic\">        </span><span style=\"color: #008000; text-decoration-color: #008000; background-color: #808000\">\"description\"</span><span style=\"color: #000000; text-decoration-color: #000000; background-color: #808000; font-style: italic\">: </span><span style=\"color: #008000; text-decoration-color: #008000; background-color: #808000\">\"A function that multiplies two numbers\"</span><span style=\"color: #000000; text-decoration-color: #000000; background-color: #808000; font-style: italic\">,</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; background-color: #808000; font-style: italic\">        </span><span style=\"color: #008000; text-decoration-color: #008000; background-color: #808000\">\"parameters\"</span><span style=\"color: #000000; text-decoration-color: #000000; background-color: #808000; font-style: italic\">: </span><span style=\"color: #000000; text-decoration-color: #000000; background-color: #808000; font-weight: bold; font-style: italic\">{</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; background-color: #808000; font-style: italic\">            </span><span style=\"color: #008000; text-decoration-color: #008000; background-color: #808000\">\"type\"</span><span style=\"color: #000000; text-decoration-color: #000000; background-color: #808000; font-style: italic\">: </span><span style=\"color: #008000; text-decoration-color: #008000; background-color: #808000\">\"object\"</span><span style=\"color: #000000; text-decoration-color: #000000; background-color: #808000; font-style: italic\">,</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; background-color: #808000; font-style: italic\">            </span><span style=\"color: #008000; text-decoration-color: #008000; background-color: #808000\">\"properties\"</span><span style=\"color: #000000; text-decoration-color: #000000; background-color: #808000; font-style: italic\">: </span><span style=\"color: #000000; text-decoration-color: #000000; background-color: #808000; font-weight: bold; font-style: italic\">{</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; background-color: #808000; font-style: italic\">                </span><span style=\"color: #008000; text-decoration-color: #008000; background-color: #808000\">\"a\"</span><span style=\"color: #000000; text-decoration-color: #000000; background-color: #808000; font-style: italic\">: </span><span style=\"color: #000000; text-decoration-color: #000000; background-color: #808000; font-weight: bold; font-style: italic\">{</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; background-color: #808000; font-style: italic\">                    </span><span style=\"color: #008000; text-decoration-color: #008000; background-color: #808000\">\"type\"</span><span style=\"color: #000000; text-decoration-color: #000000; background-color: #808000; font-style: italic\">: </span><span style=\"color: #008000; text-decoration-color: #008000; background-color: #808000\">\"number\"</span><span style=\"color: #000000; text-decoration-color: #000000; background-color: #808000; font-style: italic\">,</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; background-color: #808000; font-style: italic\">                    </span><span style=\"color: #008000; text-decoration-color: #008000; background-color: #808000\">\"description\"</span><span style=\"color: #000000; text-decoration-color: #000000; background-color: #808000; font-style: italic\">: </span><span style=\"color: #008000; text-decoration-color: #008000; background-color: #808000\">\"The first number to multiply\"</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; background-color: #808000; font-style: italic\">                </span><span style=\"color: #000000; text-decoration-color: #000000; background-color: #808000; font-weight: bold; font-style: italic\">}</span><span style=\"color: #000000; text-decoration-color: #000000; background-color: #808000; font-style: italic\">,</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; background-color: #808000; font-style: italic\">                </span><span style=\"color: #008000; text-decoration-color: #008000; background-color: #808000\">\"b\"</span><span style=\"color: #000000; text-decoration-color: #000000; background-color: #808000; font-style: italic\">: </span><span style=\"color: #000000; text-decoration-color: #000000; background-color: #808000; font-weight: bold; font-style: italic\">{</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; background-color: #808000; font-style: italic\">                    </span><span style=\"color: #008000; text-decoration-color: #008000; background-color: #808000\">\"type\"</span><span style=\"color: #000000; text-decoration-color: #000000; background-color: #808000; font-style: italic\">: </span><span style=\"color: #008000; text-decoration-color: #008000; background-color: #808000\">\"number\"</span><span style=\"color: #000000; text-decoration-color: #000000; background-color: #808000; font-style: italic\">,</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; background-color: #808000; font-style: italic\">                    </span><span style=\"color: #008000; text-decoration-color: #008000; background-color: #808000\">\"description\"</span><span style=\"color: #000000; text-decoration-color: #000000; background-color: #808000; font-style: italic\">: </span><span style=\"color: #008000; text-decoration-color: #008000; background-color: #808000\">\"The second number to multiply\"</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; background-color: #808000; font-style: italic\">                </span><span style=\"color: #000000; text-decoration-color: #000000; background-color: #808000; font-weight: bold; font-style: italic\">}</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; background-color: #808000; font-style: italic\">            </span><span style=\"color: #000000; text-decoration-color: #000000; background-color: #808000; font-weight: bold; font-style: italic\">}</span><span style=\"color: #000000; text-decoration-color: #000000; background-color: #808000; font-style: italic\">,</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; background-color: #808000; font-style: italic\">            </span><span style=\"color: #008000; text-decoration-color: #008000; background-color: #808000\">\"required\"</span><span style=\"color: #000000; text-decoration-color: #000000; background-color: #808000; font-style: italic\">: </span><span style=\"color: #000000; text-decoration-color: #000000; background-color: #808000; font-weight: bold; font-style: italic\">[</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; background-color: #808000; font-style: italic\">                </span><span style=\"color: #008000; text-decoration-color: #008000; background-color: #808000\">\"a\"</span><span style=\"color: #000000; text-decoration-color: #000000; background-color: #808000; font-style: italic\">,</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; background-color: #808000; font-style: italic\">                </span><span style=\"color: #008000; text-decoration-color: #008000; background-color: #808000\">\"b\"</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; background-color: #808000; font-style: italic\">            </span><span style=\"color: #000000; text-decoration-color: #000000; background-color: #808000; font-weight: bold; font-style: italic\">]</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; background-color: #808000; font-style: italic\">        </span><span style=\"color: #000000; text-decoration-color: #000000; background-color: #808000; font-weight: bold; font-style: italic\">}</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; background-color: #808000; font-style: italic\">    </span><span style=\"color: #000000; text-decoration-color: #000000; background-color: #808000; font-weight: bold; font-style: italic\">}</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; background-color: #808000; font-weight: bold; font-style: italic\">}</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; background-color: #808000; font-style: italic\">Tell me something about large language models.&lt;|eot_id|&gt;&lt;|start_header_id|&gt;assistant&lt;|end_header_id|&gt;</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; background-color: #808000; font-style: italic\">Large language models are powerful models that can generate human-like </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; background-color: #808000; font-style: italic\">text.&lt;|eot_id|&gt;&lt;|start_header_id|&gt;user&lt;|end_header_id|&gt;</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; background-color: #808000; font-style: italic\">Can you show me an example of a large language model?&lt;|eot_id|&gt;&lt;|start_header_id|&gt;assistant&lt;|end_header_id|</span><span style=\"color: #800080; text-decoration-color: #800080; background-color: #808000; font-weight: bold; font-style: italic\">&gt;</span>\n",
       "\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;3;35;43m<\u001b[0m\u001b[1;3;95;43m|begin_of_text|\u001b[0m\u001b[3;39;43m><|start_header_id|>system<|end_header_id|>\u001b[0m\n",
       "\n",
       "\u001b[3;39;43mEnvironment: ipython\u001b[0m\n",
       "\u001b[3;39;43mCutting Knowledge Date: December \u001b[0m\u001b[1;36;43m2023\u001b[0m\n",
       "\u001b[3;39;43mToday Date: \u001b[0m\u001b[1;36;43m12\u001b[0m\u001b[3;39;43m Mar \u001b[0m\u001b[1;36;43m2025\u001b[0m\n",
       "\n",
       "\u001b[3;39;43mYour name is Iida, You are a helpful assistant.<|eot_id|><|start_header_id|>user<|end_header_id|>\u001b[0m\n",
       "\n",
       "\u001b[3;39;43mGiven the following functions, please respond with a JSON for a function call with its proper arguments that best \u001b[0m\n",
       "\u001b[3;39;43manswers the given prompt.\u001b[0m\n",
       "\n",
       "\u001b[3;39;43mRespond in the format \u001b[0m\u001b[1;3;39;43m{\u001b[0m\u001b[32;43m\"name\"\u001b[0m\u001b[3;39;43m: function name, \u001b[0m\u001b[32;43m\"parameters\"\u001b[0m\u001b[3;39;43m: dictionary of argument name and its value\u001b[0m\u001b[1;3;39;43m}\u001b[0m\u001b[3;39;43m.Do not use \u001b[0m\n",
       "\u001b[3;39;43mvariables.\u001b[0m\n",
       "\n",
       "\u001b[1;3;39;43m{\u001b[0m\n",
       "\u001b[3;39;43m    \u001b[0m\u001b[32;43m\"type\"\u001b[0m\u001b[3;39;43m: \u001b[0m\u001b[32;43m\"function\"\u001b[0m\u001b[3;39;43m,\u001b[0m\n",
       "\u001b[3;39;43m    \u001b[0m\u001b[32;43m\"function\"\u001b[0m\u001b[3;39;43m: \u001b[0m\u001b[1;3;39;43m{\u001b[0m\n",
       "\u001b[3;39;43m        \u001b[0m\u001b[32;43m\"name\"\u001b[0m\u001b[3;39;43m: \u001b[0m\u001b[32;43m\"current_time\"\u001b[0m\u001b[3;39;43m,\u001b[0m\n",
       "\u001b[3;39;43m        \u001b[0m\u001b[32;43m\"description\"\u001b[0m\u001b[3;39;43m: \u001b[0m\u001b[32;43m\"Get the current local time as a string.\"\u001b[0m\u001b[3;39;43m,\u001b[0m\n",
       "\u001b[3;39;43m        \u001b[0m\u001b[32;43m\"parameters\"\u001b[0m\u001b[3;39;43m: \u001b[0m\u001b[1;3;39;43m{\u001b[0m\n",
       "\u001b[3;39;43m            \u001b[0m\u001b[32;43m\"type\"\u001b[0m\u001b[3;39;43m: \u001b[0m\u001b[32;43m\"object\"\u001b[0m\u001b[3;39;43m,\u001b[0m\n",
       "\u001b[3;39;43m            \u001b[0m\u001b[32;43m\"properties\"\u001b[0m\u001b[3;39;43m: \u001b[0m\u001b[1;3;39;43m{\u001b[0m\u001b[1;3;39;43m}\u001b[0m\n",
       "\u001b[3;39;43m        \u001b[0m\u001b[1;3;39;43m}\u001b[0m\n",
       "\u001b[3;39;43m    \u001b[0m\u001b[1;3;39;43m}\u001b[0m\n",
       "\u001b[1;3;39;43m}\u001b[0m\n",
       "\n",
       "\u001b[1;3;39;43m{\u001b[0m\n",
       "\u001b[3;39;43m    \u001b[0m\u001b[32;43m\"type\"\u001b[0m\u001b[3;39;43m: \u001b[0m\u001b[32;43m\"function\"\u001b[0m\u001b[3;39;43m,\u001b[0m\n",
       "\u001b[3;39;43m    \u001b[0m\u001b[32;43m\"function\"\u001b[0m\u001b[3;39;43m: \u001b[0m\u001b[1;3;39;43m{\u001b[0m\n",
       "\u001b[3;39;43m        \u001b[0m\u001b[32;43m\"name\"\u001b[0m\u001b[3;39;43m: \u001b[0m\u001b[32;43m\"multiply\"\u001b[0m\u001b[3;39;43m,\u001b[0m\n",
       "\u001b[3;39;43m        \u001b[0m\u001b[32;43m\"description\"\u001b[0m\u001b[3;39;43m: \u001b[0m\u001b[32;43m\"A function that multiplies two numbers\"\u001b[0m\u001b[3;39;43m,\u001b[0m\n",
       "\u001b[3;39;43m        \u001b[0m\u001b[32;43m\"parameters\"\u001b[0m\u001b[3;39;43m: \u001b[0m\u001b[1;3;39;43m{\u001b[0m\n",
       "\u001b[3;39;43m            \u001b[0m\u001b[32;43m\"type\"\u001b[0m\u001b[3;39;43m: \u001b[0m\u001b[32;43m\"object\"\u001b[0m\u001b[3;39;43m,\u001b[0m\n",
       "\u001b[3;39;43m            \u001b[0m\u001b[32;43m\"properties\"\u001b[0m\u001b[3;39;43m: \u001b[0m\u001b[1;3;39;43m{\u001b[0m\n",
       "\u001b[3;39;43m                \u001b[0m\u001b[32;43m\"a\"\u001b[0m\u001b[3;39;43m: \u001b[0m\u001b[1;3;39;43m{\u001b[0m\n",
       "\u001b[3;39;43m                    \u001b[0m\u001b[32;43m\"type\"\u001b[0m\u001b[3;39;43m: \u001b[0m\u001b[32;43m\"number\"\u001b[0m\u001b[3;39;43m,\u001b[0m\n",
       "\u001b[3;39;43m                    \u001b[0m\u001b[32;43m\"description\"\u001b[0m\u001b[3;39;43m: \u001b[0m\u001b[32;43m\"The first number to multiply\"\u001b[0m\n",
       "\u001b[3;39;43m                \u001b[0m\u001b[1;3;39;43m}\u001b[0m\u001b[3;39;43m,\u001b[0m\n",
       "\u001b[3;39;43m                \u001b[0m\u001b[32;43m\"b\"\u001b[0m\u001b[3;39;43m: \u001b[0m\u001b[1;3;39;43m{\u001b[0m\n",
       "\u001b[3;39;43m                    \u001b[0m\u001b[32;43m\"type\"\u001b[0m\u001b[3;39;43m: \u001b[0m\u001b[32;43m\"number\"\u001b[0m\u001b[3;39;43m,\u001b[0m\n",
       "\u001b[3;39;43m                    \u001b[0m\u001b[32;43m\"description\"\u001b[0m\u001b[3;39;43m: \u001b[0m\u001b[32;43m\"The second number to multiply\"\u001b[0m\n",
       "\u001b[3;39;43m                \u001b[0m\u001b[1;3;39;43m}\u001b[0m\n",
       "\u001b[3;39;43m            \u001b[0m\u001b[1;3;39;43m}\u001b[0m\u001b[3;39;43m,\u001b[0m\n",
       "\u001b[3;39;43m            \u001b[0m\u001b[32;43m\"required\"\u001b[0m\u001b[3;39;43m: \u001b[0m\u001b[1;3;39;43m[\u001b[0m\n",
       "\u001b[3;39;43m                \u001b[0m\u001b[32;43m\"a\"\u001b[0m\u001b[3;39;43m,\u001b[0m\n",
       "\u001b[3;39;43m                \u001b[0m\u001b[32;43m\"b\"\u001b[0m\n",
       "\u001b[3;39;43m            \u001b[0m\u001b[1;3;39;43m]\u001b[0m\n",
       "\u001b[3;39;43m        \u001b[0m\u001b[1;3;39;43m}\u001b[0m\n",
       "\u001b[3;39;43m    \u001b[0m\u001b[1;3;39;43m}\u001b[0m\n",
       "\u001b[1;3;39;43m}\u001b[0m\n",
       "\n",
       "\u001b[3;39;43mTell me something about large language models.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\u001b[0m\n",
       "\n",
       "\u001b[3;39;43mLarge language models are powerful models that can generate human-like \u001b[0m\n",
       "\u001b[3;39;43mtext.<|eot_id|><|start_header_id|>user<|end_header_id|>\u001b[0m\n",
       "\n",
       "\u001b[3;39;43mCan you show me an example of a large language model?<|eot_id|><|start_header_id|>assistant<|end_header_id|\u001b[0m\u001b[1;3;35;43m>\u001b[0m\n",
       "\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Memory usage: 63.6%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000; background-color: #c0c0c0\">Model: qwen</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31;47mModel: qwen\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; background-color: #808000; font-weight: bold; font-style: italic\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; background-color: #808000; font-weight: bold; font-style: italic\">|im_start|</span><span style=\"color: #000000; text-decoration-color: #000000; background-color: #808000; font-style: italic\">&gt;system</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; background-color: #808000; font-style: italic\">Your name is Iida, You are a helpful assistant.</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; background-color: #808000; font-style: italic\"># Tools</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; background-color: #808000; font-style: italic\">You may call one or more functions to assist with the user query.</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; background-color: #808000; font-style: italic\">You are provided with function signatures within &lt;tools&gt;&lt;</span><span style=\"color: #800080; text-decoration-color: #800080; background-color: #808000; font-style: italic\">/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; background-color: #808000; font-style: italic\">tools</span><span style=\"color: #000000; text-decoration-color: #000000; background-color: #808000; font-style: italic\">&gt; XML tags:</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; background-color: #808000; font-style: italic\">&lt;tools&gt;</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; background-color: #808000; font-weight: bold; font-style: italic\">{</span><span style=\"color: #008000; text-decoration-color: #008000; background-color: #808000\">\"type\"</span><span style=\"color: #000000; text-decoration-color: #000000; background-color: #808000; font-style: italic\">: </span><span style=\"color: #008000; text-decoration-color: #008000; background-color: #808000\">\"function\"</span><span style=\"color: #000000; text-decoration-color: #000000; background-color: #808000; font-style: italic\">, </span><span style=\"color: #008000; text-decoration-color: #008000; background-color: #808000\">\"function\"</span><span style=\"color: #000000; text-decoration-color: #000000; background-color: #808000; font-style: italic\">: </span><span style=\"color: #000000; text-decoration-color: #000000; background-color: #808000; font-weight: bold; font-style: italic\">{</span><span style=\"color: #008000; text-decoration-color: #008000; background-color: #808000\">\"name\"</span><span style=\"color: #000000; text-decoration-color: #000000; background-color: #808000; font-style: italic\">: </span><span style=\"color: #008000; text-decoration-color: #008000; background-color: #808000\">\"current_time\"</span><span style=\"color: #000000; text-decoration-color: #000000; background-color: #808000; font-style: italic\">, </span><span style=\"color: #008000; text-decoration-color: #008000; background-color: #808000\">\"description\"</span><span style=\"color: #000000; text-decoration-color: #000000; background-color: #808000; font-style: italic\">: </span><span style=\"color: #008000; text-decoration-color: #008000; background-color: #808000\">\"Get the current local time as a string.\"</span><span style=\"color: #000000; text-decoration-color: #000000; background-color: #808000; font-style: italic\">,</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000; background-color: #808000\">\"parameters\"</span><span style=\"color: #000000; text-decoration-color: #000000; background-color: #808000; font-style: italic\">: </span><span style=\"color: #000000; text-decoration-color: #000000; background-color: #808000; font-weight: bold; font-style: italic\">{</span><span style=\"color: #008000; text-decoration-color: #008000; background-color: #808000\">\"type\"</span><span style=\"color: #000000; text-decoration-color: #000000; background-color: #808000; font-style: italic\">: </span><span style=\"color: #008000; text-decoration-color: #008000; background-color: #808000\">\"object\"</span><span style=\"color: #000000; text-decoration-color: #000000; background-color: #808000; font-style: italic\">, </span><span style=\"color: #008000; text-decoration-color: #008000; background-color: #808000\">\"properties\"</span><span style=\"color: #000000; text-decoration-color: #000000; background-color: #808000; font-style: italic\">: </span><span style=\"color: #000000; text-decoration-color: #000000; background-color: #808000; font-weight: bold; font-style: italic\">{}}}}</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; background-color: #808000; font-weight: bold; font-style: italic\">{</span><span style=\"color: #008000; text-decoration-color: #008000; background-color: #808000\">\"type\"</span><span style=\"color: #000000; text-decoration-color: #000000; background-color: #808000; font-style: italic\">: </span><span style=\"color: #008000; text-decoration-color: #008000; background-color: #808000\">\"function\"</span><span style=\"color: #000000; text-decoration-color: #000000; background-color: #808000; font-style: italic\">, </span><span style=\"color: #008000; text-decoration-color: #008000; background-color: #808000\">\"function\"</span><span style=\"color: #000000; text-decoration-color: #000000; background-color: #808000; font-style: italic\">: </span><span style=\"color: #000000; text-decoration-color: #000000; background-color: #808000; font-weight: bold; font-style: italic\">{</span><span style=\"color: #008000; text-decoration-color: #008000; background-color: #808000\">\"name\"</span><span style=\"color: #000000; text-decoration-color: #000000; background-color: #808000; font-style: italic\">: </span><span style=\"color: #008000; text-decoration-color: #008000; background-color: #808000\">\"multiply\"</span><span style=\"color: #000000; text-decoration-color: #000000; background-color: #808000; font-style: italic\">, </span><span style=\"color: #008000; text-decoration-color: #008000; background-color: #808000\">\"description\"</span><span style=\"color: #000000; text-decoration-color: #000000; background-color: #808000; font-style: italic\">: </span><span style=\"color: #008000; text-decoration-color: #008000; background-color: #808000\">\"A function that multiplies two numbers\"</span><span style=\"color: #000000; text-decoration-color: #000000; background-color: #808000; font-style: italic\">, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000; background-color: #808000\">\"parameters\"</span><span style=\"color: #000000; text-decoration-color: #000000; background-color: #808000; font-style: italic\">: </span><span style=\"color: #000000; text-decoration-color: #000000; background-color: #808000; font-weight: bold; font-style: italic\">{</span><span style=\"color: #008000; text-decoration-color: #008000; background-color: #808000\">\"type\"</span><span style=\"color: #000000; text-decoration-color: #000000; background-color: #808000; font-style: italic\">: </span><span style=\"color: #008000; text-decoration-color: #008000; background-color: #808000\">\"object\"</span><span style=\"color: #000000; text-decoration-color: #000000; background-color: #808000; font-style: italic\">, </span><span style=\"color: #008000; text-decoration-color: #008000; background-color: #808000\">\"properties\"</span><span style=\"color: #000000; text-decoration-color: #000000; background-color: #808000; font-style: italic\">: </span><span style=\"color: #000000; text-decoration-color: #000000; background-color: #808000; font-weight: bold; font-style: italic\">{</span><span style=\"color: #008000; text-decoration-color: #008000; background-color: #808000\">\"a\"</span><span style=\"color: #000000; text-decoration-color: #000000; background-color: #808000; font-style: italic\">: </span><span style=\"color: #000000; text-decoration-color: #000000; background-color: #808000; font-weight: bold; font-style: italic\">{</span><span style=\"color: #008000; text-decoration-color: #008000; background-color: #808000\">\"type\"</span><span style=\"color: #000000; text-decoration-color: #000000; background-color: #808000; font-style: italic\">: </span><span style=\"color: #008000; text-decoration-color: #008000; background-color: #808000\">\"number\"</span><span style=\"color: #000000; text-decoration-color: #000000; background-color: #808000; font-style: italic\">, </span><span style=\"color: #008000; text-decoration-color: #008000; background-color: #808000\">\"description\"</span><span style=\"color: #000000; text-decoration-color: #000000; background-color: #808000; font-style: italic\">: </span><span style=\"color: #008000; text-decoration-color: #008000; background-color: #808000\">\"The first number to </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000; background-color: #808000\">multiply\"</span><span style=\"color: #000000; text-decoration-color: #000000; background-color: #808000; font-weight: bold; font-style: italic\">}</span><span style=\"color: #000000; text-decoration-color: #000000; background-color: #808000; font-style: italic\">, </span><span style=\"color: #008000; text-decoration-color: #008000; background-color: #808000\">\"b\"</span><span style=\"color: #000000; text-decoration-color: #000000; background-color: #808000; font-style: italic\">: </span><span style=\"color: #000000; text-decoration-color: #000000; background-color: #808000; font-weight: bold; font-style: italic\">{</span><span style=\"color: #008000; text-decoration-color: #008000; background-color: #808000\">\"type\"</span><span style=\"color: #000000; text-decoration-color: #000000; background-color: #808000; font-style: italic\">: </span><span style=\"color: #008000; text-decoration-color: #008000; background-color: #808000\">\"number\"</span><span style=\"color: #000000; text-decoration-color: #000000; background-color: #808000; font-style: italic\">, </span><span style=\"color: #008000; text-decoration-color: #008000; background-color: #808000\">\"description\"</span><span style=\"color: #000000; text-decoration-color: #000000; background-color: #808000; font-style: italic\">: </span><span style=\"color: #008000; text-decoration-color: #008000; background-color: #808000\">\"The second number to multiply\"</span><span style=\"color: #000000; text-decoration-color: #000000; background-color: #808000; font-weight: bold; font-style: italic\">}}</span><span style=\"color: #000000; text-decoration-color: #000000; background-color: #808000; font-style: italic\">, </span><span style=\"color: #008000; text-decoration-color: #008000; background-color: #808000\">\"required\"</span><span style=\"color: #000000; text-decoration-color: #000000; background-color: #808000; font-style: italic\">: </span><span style=\"color: #000000; text-decoration-color: #000000; background-color: #808000; font-weight: bold; font-style: italic\">[</span><span style=\"color: #008000; text-decoration-color: #008000; background-color: #808000\">\"a\"</span><span style=\"color: #000000; text-decoration-color: #000000; background-color: #808000; font-style: italic\">, </span><span style=\"color: #008000; text-decoration-color: #008000; background-color: #808000\">\"b\"</span><span style=\"color: #000000; text-decoration-color: #000000; background-color: #808000; font-weight: bold; font-style: italic\">]}}}</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; background-color: #808000; font-style: italic\">&lt;</span><span style=\"color: #800080; text-decoration-color: #800080; background-color: #808000; font-style: italic\">/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; background-color: #808000; font-style: italic\">tools</span><span style=\"color: #000000; text-decoration-color: #000000; background-color: #808000; font-style: italic\">&gt;</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; background-color: #808000; font-style: italic\">For each function call, return a json object with function name and arguments within &lt;tool_call&gt;&lt;</span><span style=\"color: #800080; text-decoration-color: #800080; background-color: #808000; font-style: italic\">/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; background-color: #808000; font-style: italic\">tool_call</span><span style=\"color: #000000; text-decoration-color: #000000; background-color: #808000; font-style: italic\">&gt; XML </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; background-color: #808000; font-style: italic\">tags:</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; background-color: #808000; font-style: italic\">&lt;tool_call&gt;</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; background-color: #808000; font-weight: bold; font-style: italic\">{</span><span style=\"color: #008000; text-decoration-color: #008000; background-color: #808000\">\"name\"</span><span style=\"color: #000000; text-decoration-color: #000000; background-color: #808000; font-style: italic\">: &lt;function-name&gt;, </span><span style=\"color: #008000; text-decoration-color: #008000; background-color: #808000\">\"arguments\"</span><span style=\"color: #000000; text-decoration-color: #000000; background-color: #808000; font-style: italic\">: &lt;args-json-object&gt;</span><span style=\"color: #000000; text-decoration-color: #000000; background-color: #808000; font-weight: bold; font-style: italic\">}</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; background-color: #808000; font-style: italic\">&lt;</span><span style=\"color: #800080; text-decoration-color: #800080; background-color: #808000; font-style: italic\">/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; background-color: #808000; font-style: italic\">tool_call</span><span style=\"color: #000000; text-decoration-color: #000000; background-color: #808000; font-style: italic\">&gt;&lt;|im_end|&gt;</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; background-color: #808000; font-style: italic\">&lt;|im_start|&gt;user</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; background-color: #808000; font-style: italic\">Tell me something about large language models.&lt;|im_end|&gt;</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; background-color: #808000; font-style: italic\">&lt;|im_start|&gt;assistant</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; background-color: #808000; font-style: italic\">Large language models are powerful models that can generate human-like text.&lt;|im_end|&gt;</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; background-color: #808000; font-style: italic\">&lt;|im_start|&gt;user</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; background-color: #808000; font-style: italic\">Can you show me an example of a large language model?&lt;|im_end|&gt;</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; background-color: #808000; font-style: italic\">&lt;|im_start|</span><span style=\"color: #800080; text-decoration-color: #800080; background-color: #808000; font-weight: bold; font-style: italic\">&gt;</span><span style=\"color: #800080; text-decoration-color: #800080; background-color: #808000; font-style: italic\">assistant</span>\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;3;35;43m<\u001b[0m\u001b[1;3;95;43m|im_start|\u001b[0m\u001b[3;39;43m>system\u001b[0m\n",
       "\u001b[3;39;43mYour name is Iida, You are a helpful assistant.\u001b[0m\n",
       "\n",
       "\u001b[3;39;43m# Tools\u001b[0m\n",
       "\n",
       "\u001b[3;39;43mYou may call one or more functions to assist with the user query.\u001b[0m\n",
       "\n",
       "\u001b[3;39;43mYou are provided with function signatures within <tools><\u001b[0m\u001b[3;35;43m/\u001b[0m\u001b[3;95;43mtools\u001b[0m\u001b[3;39;43m> XML tags:\u001b[0m\n",
       "\u001b[3;39;43m<tools>\u001b[0m\n",
       "\u001b[1;3;39;43m{\u001b[0m\u001b[32;43m\"type\"\u001b[0m\u001b[3;39;43m: \u001b[0m\u001b[32;43m\"function\"\u001b[0m\u001b[3;39;43m, \u001b[0m\u001b[32;43m\"function\"\u001b[0m\u001b[3;39;43m: \u001b[0m\u001b[1;3;39;43m{\u001b[0m\u001b[32;43m\"name\"\u001b[0m\u001b[3;39;43m: \u001b[0m\u001b[32;43m\"current_time\"\u001b[0m\u001b[3;39;43m, \u001b[0m\u001b[32;43m\"description\"\u001b[0m\u001b[3;39;43m: \u001b[0m\u001b[32;43m\"Get the current local time as a string.\"\u001b[0m\u001b[3;39;43m,\u001b[0m\n",
       "\u001b[32;43m\"parameters\"\u001b[0m\u001b[3;39;43m: \u001b[0m\u001b[1;3;39;43m{\u001b[0m\u001b[32;43m\"type\"\u001b[0m\u001b[3;39;43m: \u001b[0m\u001b[32;43m\"object\"\u001b[0m\u001b[3;39;43m, \u001b[0m\u001b[32;43m\"properties\"\u001b[0m\u001b[3;39;43m: \u001b[0m\u001b[1;3;39;43m{\u001b[0m\u001b[1;3;39;43m}\u001b[0m\u001b[1;3;39;43m}\u001b[0m\u001b[1;3;39;43m}\u001b[0m\u001b[1;3;39;43m}\u001b[0m\n",
       "\u001b[1;3;39;43m{\u001b[0m\u001b[32;43m\"type\"\u001b[0m\u001b[3;39;43m: \u001b[0m\u001b[32;43m\"function\"\u001b[0m\u001b[3;39;43m, \u001b[0m\u001b[32;43m\"function\"\u001b[0m\u001b[3;39;43m: \u001b[0m\u001b[1;3;39;43m{\u001b[0m\u001b[32;43m\"name\"\u001b[0m\u001b[3;39;43m: \u001b[0m\u001b[32;43m\"multiply\"\u001b[0m\u001b[3;39;43m, \u001b[0m\u001b[32;43m\"description\"\u001b[0m\u001b[3;39;43m: \u001b[0m\u001b[32;43m\"A function that multiplies two numbers\"\u001b[0m\u001b[3;39;43m, \u001b[0m\n",
       "\u001b[32;43m\"parameters\"\u001b[0m\u001b[3;39;43m: \u001b[0m\u001b[1;3;39;43m{\u001b[0m\u001b[32;43m\"type\"\u001b[0m\u001b[3;39;43m: \u001b[0m\u001b[32;43m\"object\"\u001b[0m\u001b[3;39;43m, \u001b[0m\u001b[32;43m\"properties\"\u001b[0m\u001b[3;39;43m: \u001b[0m\u001b[1;3;39;43m{\u001b[0m\u001b[32;43m\"a\"\u001b[0m\u001b[3;39;43m: \u001b[0m\u001b[1;3;39;43m{\u001b[0m\u001b[32;43m\"type\"\u001b[0m\u001b[3;39;43m: \u001b[0m\u001b[32;43m\"number\"\u001b[0m\u001b[3;39;43m, \u001b[0m\u001b[32;43m\"description\"\u001b[0m\u001b[3;39;43m: \u001b[0m\u001b[32;43m\"The first number to \u001b[0m\n",
       "\u001b[32;43mmultiply\"\u001b[0m\u001b[1;3;39;43m}\u001b[0m\u001b[3;39;43m, \u001b[0m\u001b[32;43m\"b\"\u001b[0m\u001b[3;39;43m: \u001b[0m\u001b[1;3;39;43m{\u001b[0m\u001b[32;43m\"type\"\u001b[0m\u001b[3;39;43m: \u001b[0m\u001b[32;43m\"number\"\u001b[0m\u001b[3;39;43m, \u001b[0m\u001b[32;43m\"description\"\u001b[0m\u001b[3;39;43m: \u001b[0m\u001b[32;43m\"The second number to multiply\"\u001b[0m\u001b[1;3;39;43m}\u001b[0m\u001b[1;3;39;43m}\u001b[0m\u001b[3;39;43m, \u001b[0m\u001b[32;43m\"required\"\u001b[0m\u001b[3;39;43m: \u001b[0m\u001b[1;3;39;43m[\u001b[0m\u001b[32;43m\"a\"\u001b[0m\u001b[3;39;43m, \u001b[0m\u001b[32;43m\"b\"\u001b[0m\u001b[1;3;39;43m]\u001b[0m\u001b[1;3;39;43m}\u001b[0m\u001b[1;3;39;43m}\u001b[0m\u001b[1;3;39;43m}\u001b[0m\n",
       "\u001b[3;39;43m<\u001b[0m\u001b[3;35;43m/\u001b[0m\u001b[3;95;43mtools\u001b[0m\u001b[3;39;43m>\u001b[0m\n",
       "\n",
       "\u001b[3;39;43mFor each function call, return a json object with function name and arguments within <tool_call><\u001b[0m\u001b[3;35;43m/\u001b[0m\u001b[3;95;43mtool_call\u001b[0m\u001b[3;39;43m> XML \u001b[0m\n",
       "\u001b[3;39;43mtags:\u001b[0m\n",
       "\u001b[3;39;43m<tool_call>\u001b[0m\n",
       "\u001b[1;3;39;43m{\u001b[0m\u001b[32;43m\"name\"\u001b[0m\u001b[3;39;43m: <function-name>, \u001b[0m\u001b[32;43m\"arguments\"\u001b[0m\u001b[3;39;43m: <args-json-object>\u001b[0m\u001b[1;3;39;43m}\u001b[0m\n",
       "\u001b[3;39;43m<\u001b[0m\u001b[3;35;43m/\u001b[0m\u001b[3;95;43mtool_call\u001b[0m\u001b[3;39;43m><|im_end|>\u001b[0m\n",
       "\u001b[3;39;43m<|im_start|>user\u001b[0m\n",
       "\u001b[3;39;43mTell me something about large language models.<|im_end|>\u001b[0m\n",
       "\u001b[3;39;43m<|im_start|>assistant\u001b[0m\n",
       "\u001b[3;39;43mLarge language models are powerful models that can generate human-like text.<|im_end|>\u001b[0m\n",
       "\u001b[3;39;43m<|im_start|>user\u001b[0m\n",
       "\u001b[3;39;43mCan you show me an example of a large language model?<|im_end|>\u001b[0m\n",
       "\u001b[3;39;43m<|im_start|\u001b[0m\u001b[1;3;35;43m>\u001b[0m\u001b[3;35;43massistant\u001b[0m\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Memory usage: 63.5%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000; background-color: #c0c0c0\">Model: gemma</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31;47mModel: gemma\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: System role not supported\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; background-color: #808000; font-weight: bold; font-style: italic\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; background-color: #808000; font-weight: bold; font-style: italic\">bos</span><span style=\"color: #000000; text-decoration-color: #000000; background-color: #808000; font-style: italic\">&gt;&lt;start_of_turn&gt;user</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; background-color: #808000; font-style: italic\">Hello, how are you?&lt;end_of_turn&gt;</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; background-color: #808000; font-style: italic\">&lt;start_of_turn&gt;model</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; background-color: #808000; font-style: italic\">I'm doing great. How can I help you today?&lt;end_of_turn&gt;</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; background-color: #808000; font-style: italic\">&lt;start_of_turn&gt;user</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; background-color: #808000; font-style: italic\">I'd like to show off how chat templating works!&lt;end_of_turn&gt;</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; background-color: #808000; font-style: italic\">&lt;start_of_turn</span><span style=\"color: #800080; text-decoration-color: #800080; background-color: #808000; font-weight: bold; font-style: italic\">&gt;</span><span style=\"color: #800080; text-decoration-color: #800080; background-color: #808000; font-style: italic\">model</span>\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;3;35;43m<\u001b[0m\u001b[1;3;95;43mbos\u001b[0m\u001b[3;39;43m><start_of_turn>user\u001b[0m\n",
       "\u001b[3;39;43mHello, how are you?<end_of_turn>\u001b[0m\n",
       "\u001b[3;39;43m<start_of_turn>model\u001b[0m\n",
       "\u001b[3;39;43mI'm doing great. How can I help you today?<end_of_turn>\u001b[0m\n",
       "\u001b[3;39;43m<start_of_turn>user\u001b[0m\n",
       "\u001b[3;39;43mI'd like to show off how chat templating works!<end_of_turn>\u001b[0m\n",
       "\u001b[3;39;43m<start_of_turn\u001b[0m\u001b[1;3;35;43m>\u001b[0m\u001b[3;35;43mmodel\u001b[0m\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Memory usage: 63.5%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000; background-color: #c0c0c0\">Model: phi</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31;47mModel: phi\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; background-color: #808000; font-weight: bold; font-style: italic\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; background-color: #808000; font-weight: bold; font-style: italic\">|system|</span><span style=\"color: #000000; text-decoration-color: #000000; background-color: #808000; font-style: italic\">&gt;Your name is Iida, You are a helpful assistant.&lt;|end|&gt;&lt;|user|&gt;Tell me something about large language </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; background-color: #808000; font-style: italic\">models.&lt;|end|&gt;&lt;|assistant|&gt;Large language models are powerful models that can generate human-like </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; background-color: #808000; font-style: italic\">text.&lt;|end|&gt;&lt;|user|&gt;Can you show me an example of a large language model?&lt;|end|&gt;&lt;|assistant|</span><span style=\"color: #800080; text-decoration-color: #800080; background-color: #808000; font-weight: bold; font-style: italic\">&gt;</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;3;35;43m<\u001b[0m\u001b[1;3;95;43m|system|\u001b[0m\u001b[3;39;43m>Your name is Iida, You are a helpful assistant.<|end|><|user|>Tell me something about large language \u001b[0m\n",
       "\u001b[3;39;43mmodels.<|end|><|assistant|>Large language models are powerful models that can generate human-like \u001b[0m\n",
       "\u001b[3;39;43mtext.<|end|><|user|>Can you show me an example of a large language model?<|end|><|assistant|\u001b[0m\u001b[1;3;35;43m>\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Loop through the local models and see the templates\n",
    "for model_name, model_id in local_models.items():\n",
    "    if \"mini\" in model_name:\n",
    "        continue\n",
    "    # Print memory usage\n",
    "    print(f\"Memory usage: {psutil.virtual_memory().percent}%\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    console.print(f\"Model: {model_name}\", style=\"red on white\")\n",
    "    try:\n",
    "        text = tokenizer.apply_chat_template(\n",
    "            messages, tools=tools, tokenize=False, add_generation_prompt=True\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        text = tokenizer.apply_chat_template(\n",
    "            fallback_messages, tools=tools, tokenize=False, add_generation_prompt=True\n",
    "        )\n",
    "    # Delete the tokenizer to free up memory\n",
    "    del tokenizer\n",
    "    console.print(text, style=\"italic magenta on yellow\")\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/venkat/projects/gen-ai-cookbook/.venv/lib/python3.12/site-packages/pydantic/_internal/_config.py:345: UserWarning: Valid config keys have changed in V2:\n",
      "* 'schema_extra' has been renamed to 'json_schema_extra'\n",
      "  warnings.warn(message, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "# Defining pydantic model for chat history\n",
    "\n",
    "class RoleEnum(str, Enum):\n",
    "    user = \"user\"\n",
    "    assistant = \"assistant\"\n",
    "    system = \"system\"\n",
    "\n",
    "class ChatHistory(BaseModel):\n",
    "    \"\"\"\n",
    "    Pydantic model for chat history.\n",
    "    \"\"\"\n",
    "    role: RoleEnum = Field(..., description=\"The role of the speaker (user or assistant).\")\n",
    "    content: str = Field(..., description=\"The content of the message.\")\n",
    "\n",
    "    class Config:\n",
    "        schema_extra = {\n",
    "            \"example\": {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"Hello, how are you?\",\n",
    "            }\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a base class for the models, since we will be experimenting with different models which have different requirements\n",
    "class BaseLLM(abc.ABC):\n",
    "    \"\"\"\n",
    "    Abstract base class for LLM models, defining common functionality.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, model: str, max_history: int = 10, system_prompt: str = \"\", **kwargs\n",
    "    ):\n",
    "        self.logger = logging.getLogger(self.__class__.__name__)\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.system_prompt = system_prompt\n",
    "        self.max_history = max_history\n",
    "        self.history: List[tuple] = []\n",
    "\n",
    "        # Load model and tokenizer\n",
    "        self.tokenizer = None\n",
    "        self.model = None\n",
    "        self.rag_prompt: str = None\n",
    "        self.rag_prompt_template: str = None\n",
    "        self.tools_prompt_template: str = None\n",
    "        self.default_prompt_template: str = None\n",
    "        self.system_template: str = None\n",
    "        self.user_turn_template: str = None\n",
    "        self.assistant_turn_template: str = None\n",
    "        self.assistant_template: str = None\n",
    "        self.non_sys_prompt_template: str = None\n",
    "        self.load_model_and_tokenizer(model, **kwargs)\n",
    "        self.load_rag_prompt()  # This is just defined as a seperate function for keeping the code clean\n",
    "        self.load_prompt_templates()\n",
    "\n",
    "    def load_model_and_tokenizer(self, model: str, **kwargs) -> None:\n",
    "        \"\"\"\n",
    "        Loads the tokenizer and model.\n",
    "        \"\"\"\n",
    "        self.logger.info(\"Initializing tokenizer and model...\")\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "            model, torch_dtype=torch.bfloat16, **kwargs\n",
    "        )\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            model, torch_dtype=torch.bfloat16, **kwargs\n",
    "        )\n",
    "        self.model.to(self.device)\n",
    "\n",
    "        self.logger.info(\"Loaded model: %s\", model)\n",
    "        self.logger.info(\"Model type: %s\", type(self.model).__name__)\n",
    "        self.logger.info(\"Number of parameters: %s\", self.model.num_parameters())\n",
    "        self.logger.info(\"Device: %s\", self.device.type)\n",
    "\n",
    "    def get_token_count(self, text: str) -> int:\n",
    "        \"\"\"\n",
    "        Gets the token count of the given text.\n",
    "        \"\"\"\n",
    "        return len(self.tokenizer(text)[\"input_ids\"])\n",
    "\n",
    "    def trim_conversation(self, conversation_history, token_limit) -> List:\n",
    "        \"\"\"\n",
    "        Trims the conversation history to fit within the given token limit.\n",
    "        \"\"\"\n",
    "        total_tokens = 0\n",
    "        tokenized_history = []\n",
    "\n",
    "        if not conversation_history:\n",
    "            return []\n",
    "\n",
    "        for user, assistant in conversation_history:\n",
    "            user_tokens = self.get_token_count(user)\n",
    "            assistant_tokens = self.get_token_count(assistant)\n",
    "            total_tokens += user_tokens + assistant_tokens\n",
    "            tokenized_history.append((user, assistant, user_tokens + assistant_tokens))\n",
    "\n",
    "        while total_tokens > token_limit and tokenized_history:\n",
    "            removed_entry = tokenized_history.pop(0)\n",
    "            total_tokens -= removed_entry[2]\n",
    "\n",
    "        return [(entry[0], entry[1]) for entry in tokenized_history]\n",
    "\n",
    "    def clear_history(self) -> None:\n",
    "        \"\"\"Clears the stored conversation history.\"\"\"\n",
    "        self.history = []\n",
    "\n",
    "    def add_to_history(self, user_input, model_response) -> None:\n",
    "        \"\"\"Adds an interaction to history and maintains max history size.\"\"\"\n",
    "        _user = {\"role\": \"user\", \"content\": user_input}\n",
    "        _assistant = {\"role\": \"assistant\", \"content\": model_response}\n",
    "        self.history.extend([_user, _assistant])\n",
    "        if len(self.history) > self.max_history:\n",
    "            self.history.pop(0)\n",
    "\n",
    "    # Method for getting the templates\n",
    "    def get_templates(self) -> Dict[str, str]:\n",
    "        \"\"\"\n",
    "        Get the templates from the model.\n",
    "        \"\"\"\n",
    "        self.logger.debug(\"User turn template: \", self.user_turn_template)\n",
    "        self.logger.debug(\"Assistant turn template: \", self.assistant_turn_template)\n",
    "        self.logger.debug(\"Assistant template: \", self.assistant_template)\n",
    "        self.logger.debug(\"RAG prompt: \", self.rag_prompt)\n",
    "        self.logger.debug(\"RAG prompt template: \", self.rag_prompt_template)\n",
    "        self.logger.debug(\"Tools prompt template: \", self.tools_prompt_template)\n",
    "        self.logger.debug(\"Default prompt template: \", self.default_prompt_template)\n",
    "        self.logger.debug(\"Non system prompt template: \", self.non_sys_prompt_template)\n",
    "        self.logger.debug(\"System prompt template: \", self.system_template)\n",
    "        self.logger.debug(\"Tool calling prompt: \", self.tool_calling_prompt)\n",
    "\n",
    "        return {\n",
    "            \"user_turn_template\": self.user_turn_template,\n",
    "            \"assistant_turn_template\": self.assistant_turn_template,\n",
    "            \"assistant_template\": self.assistant_template,\n",
    "            \"rag_prompt\": self.rag_prompt,\n",
    "            \"rag_prompt_template\": self.rag_prompt_template,\n",
    "            \"tools_prompt_template\": self.tools_prompt_template,\n",
    "            \"default_prompt_template\": self.default_prompt_template,\n",
    "            \"non_sys_prompt_template\": self.non_sys_prompt_template,\n",
    "            \"system_prompt_template\": self.system_template,\n",
    "            \"tool_calling_prompt\": self.tool_calling_prompt,\n",
    "        }\n",
    "\n",
    "    def generate_text(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        max_new_tokens: int = 120,\n",
    "        skip_special_tokens: bool = False,\n",
    "        **kwargs,\n",
    "    ) -> str:\n",
    "        \"\"\"\n",
    "        Generates text based on the given prompt.\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        prompt : str\n",
    "            The prompt text to generate text from.\n",
    "        max_new_tokens : int, optional\n",
    "            The maximum length of the generated text (default is 120).\n",
    "        skip_special_tokens : bool, optional\n",
    "            Flag to indicate if special tokens should be skipped (default is False).\n",
    "\n",
    "        Returns:\n",
    "        -------\n",
    "        str\n",
    "            The generated text.\n",
    "        \"\"\"\n",
    "\n",
    "        self.logger.info(\"Generating response for prompt: %s\", prompt)\n",
    "        try:\n",
    "            with torch.inference_mode():\n",
    "                self.logger.debug(\"Tokenizing prompt...\", prompt)\n",
    "                print(\"Tokenizing prompt...\", prompt)\n",
    "                inputs = self.tokenizer(prompt, return_tensors=\"pt\")\n",
    "                inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "\n",
    "                _start_time = time.time()\n",
    "                outputs = self.model.generate(\n",
    "                    **inputs,\n",
    "                    max_new_tokens=max_new_tokens,\n",
    "                    pad_token_id=self.tokenizer.eos_token_id,\n",
    "                    **kwargs,\n",
    "                )\n",
    "                _end_time = time.time()\n",
    "                self.logger.debug(\"Time taken: %.2f seconds\", _end_time - _start_time)\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(\"Error generating response: %s\", e)\n",
    "            return \"Error generating response\"\n",
    "\n",
    "        decoded_output = self.tokenizer.decode(\n",
    "            outputs[0], skip_special_tokens=skip_special_tokens\n",
    "        )\n",
    "        self.logger.debug(\"Generated response: %s\", decoded_output)\n",
    "        print(\"Generated response: \", decoded_output)\n",
    "\n",
    "        return decoded_output\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def chat(\n",
    "        self, prompt: str, clear_session: bool = False, **kwargs\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Abstract method for chatting with the model.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def format_prompt(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        system_prompt: str = None,\n",
    "        tools_schema: str = None,\n",
    "        documents: List[Dict] = None,\n",
    "        create_chat_session: bool = False,\n",
    "        chat_history: List[Dict] = None,\n",
    "    ) -> str:\n",
    "        \"\"\"\n",
    "        Formats the prompt using the prompt template.\n",
    "        \"\"\"\n",
    "\n",
    "        system_prompt = system_prompt or self.system_prompt\n",
    "        final_prompt = prompt\n",
    "\n",
    "        if chat_history:\n",
    "            print(\"Formatting prompt with chat history\")\n",
    "            final_prompt = self.bos_token\n",
    "            self.logger.debug(\"Formatting prompt with chat history\")\n",
    "            # Look for system prompt in chat history\n",
    "            system_prompt = next(\n",
    "                (msg.get('content') for msg in chat_history if msg.get('role') == \"system\"), None\n",
    "            )\n",
    "            if system_prompt:\n",
    "                final_prompt += (\n",
    "                    f\"\\n{self.system_template.format(system_prompt=system_prompt)}\"\n",
    "                )\n",
    "            # Build the formatted prompt by looping over the chat history\n",
    "            for msg in chat_history:\n",
    "                if msg.get('role') == \"user\":\n",
    "                    final_prompt += (\n",
    "                        f\"\\n{self.user_turn_template.format(user_prompt=msg.get('content'))}\"\n",
    "                    )\n",
    "                elif msg.get('role') == \"assistant\":\n",
    "                    final_prompt += f\"\\n{self.assistant_turn_template.format(assistant_response=msg.get('content'))}\"\n",
    "            final_prompt += f\"\\n{self.user_turn_template.format(user_prompt=prompt)}\"  \n",
    "            final_prompt += f\"\\n{self.assistant_template}\"  # Add the assistant template at the end so the model knows it's the assistant's turn\n",
    "            return final_prompt\n",
    "        \n",
    "        if create_chat_session:\n",
    "            print(\"Formatting prompt with chat history - create chat session\")\n",
    "            final_prompt = self.bos_token\n",
    "            self.logger.debug(\"Formatting prompt with chat history\")\n",
    "            if system_prompt:\n",
    "                final_prompt += (\n",
    "                    f\"\\n{self.system_template.format(system_prompt=system_prompt)}\"\n",
    "                )\n",
    "            final_prompt += f\"\\n{self.user_turn_template.format(user_prompt=prompt)}\"\n",
    "            final_prompt += f\"\\n{self.assistant_template}\"\n",
    "            return final_prompt\n",
    "\n",
    "        if tools_schema:\n",
    "            print(\"Formatting prompt with tool schema\", tools_schema)\n",
    "            self.logger.debug(\"Formatting prompt with tool schema\")\n",
    "            formatted_prompt = self.tool_calling_prompt.format(functions_definition=tools_schema)\n",
    "            system_prompt = formatted_prompt\n",
    "            final_prompt = self.tools_prompt_template.format(\n",
    "                system_prompt=system_prompt, user_prompt=prompt\n",
    "            )\n",
    "\n",
    "            return final_prompt\n",
    "\n",
    "        if documents:\n",
    "            print(\"Formatting prompt with documents\")\n",
    "            required_keys = {\"reference\", \"content\"}\n",
    "            assert all(\n",
    "                required_keys.issubset(doc.keys()) for doc in documents\n",
    "            ), \"Documents must contain 'reference' and 'content' keys.\"\n",
    "\n",
    "            self.logger.debug(\"Formatting prompt with documents\")\n",
    "            _documents = \"\\n\".join(\n",
    "                [\n",
    "                    f\"**Document {doc['reference']}**: {doc['content']}\"\n",
    "                    for doc in documents\n",
    "                ]\n",
    "            )\n",
    "            formatted_prompt = self.rag_prompt.format(\n",
    "                documents=_documents, question=prompt\n",
    "            )\n",
    "            system_prompt = formatted_prompt\n",
    "\n",
    "            final_prompt = self.rag_prompt_template.format(\n",
    "                system_prompt=system_prompt, user_prompt=prompt\n",
    "            )\n",
    "\n",
    "            return final_prompt\n",
    "\n",
    "        if system_prompt:\n",
    "            self.logger.debug(\"Formatting prompt with system prompt\")\n",
    "            final_prompt = self.default_prompt_template.format(\n",
    "                system_prompt=system_prompt, user_prompt=prompt\n",
    "            )\n",
    "        else:\n",
    "            final_prompt = self.non_sys_prompt_template.format(user_prompt=prompt)\n",
    "\n",
    "        return final_prompt\n",
    "\n",
    "    def __call__(self, prompt: str, **kwargs) -> str:\n",
    "        \"\"\"\n",
    "        Enables direct inference by calling the model instance.\n",
    "        \"\"\"\n",
    "        return self.generate_response(prompt, **kwargs)\n",
    "\n",
    "    def __repr__(self):\n",
    "        \"\"\"\n",
    "        Official string representation for debugging.\n",
    "        \"\"\"\n",
    "        return f\"{self.__class__.__name__}(model={self.model.name_or_path!r}, device={self.device})\"\n",
    "\n",
    "    def __str__(self):\n",
    "        \"\"\"\n",
    "        User-friendly string representation.\n",
    "        \"\"\"\n",
    "        return f\"{self.__class__.__name__} running on {self.device.type}, max history: {self.max_history}\"\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Returns the number of stored conversation history entries.\n",
    "        \"\"\"\n",
    "        return len(self.history)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Retrieves conversation history entries like an array.\n",
    "        \"\"\"\n",
    "        return self.history[index]\n",
    "\n",
    "    def load_rag_prompt(self):\n",
    "        \"\"\"\n",
    "        Loads the RAG prompt from the model.\n",
    "        \"\"\"\n",
    "        # Check for env variable\n",
    "        if \"RAG_PROMPT\" in os.environ:\n",
    "            self.rag_prompt = os.environ[\"RAG_PROMPT\"]\n",
    "            self.logger.info(\"Loaded RAG prompt from environment variable.\")\n",
    "        else:\n",
    "            self.rag_prompt = (\n",
    "                self.rag_prompt\n",
    "            ) = \"\"\"You are an advanced AI assistant with expertise in retrieving and synthesizing information from provided references. Your role is to analyze the given documents and accurately answer the question based on their content.\n",
    "\n",
    "## Context:\n",
    "You will be provided with multiple documents, each containing relevant information. Each document is referenced with a unique identifier. Your response should be derived strictly from the given documents while maintaining clarity and conciseness. If the documents do not contain sufficient information, indicate that explicitly.\n",
    "\n",
    "## Instructions:\n",
    "1. **Extract information** only from the provided documents.\n",
    "2. **Cite references** where applicable by mentioning the document identifier.\n",
    "3. **Maintain coherence** while summarizing details from multiple sources.\n",
    "4. **Avoid speculation** or adding external knowledge.\n",
    "5. **If unclear**, state that the answer is not available in the provided documents.\n",
    "\n",
    "## Expected Output:\n",
    "- A **concise and accurate** response based on the referenced documents.\n",
    "- **Citations** to the corresponding documents where relevant.\n",
    "- A disclaimer if the answer cannot be found within the given context.\n",
    "\n",
    "## Documents:\n",
    "{documents}\n",
    "\n",
    "\n",
    "## User's Question:\n",
    "{question}\n",
    "\"\"\"\n",
    "            self.logger.info(\"Loaded default RAG prompt.\")\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def generate_response(self, prompt: str, **kwargs) -> str:\n",
    "        \"\"\"\n",
    "        Generates a response to the given prompt.\n",
    "        \"\"\"\n",
    "        return self.generate_text(prompt, **kwargs)\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def load_prompt_templates(self):\n",
    "        \"\"\"\n",
    "        Loads the prompt templates from the model.\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Llama\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LocalLLM(BaseLLM):\n",
    "    \"\"\"\n",
    "    A class to represent a DOTLLM model for text generation.\n",
    "\n",
    "    Attributes:\n",
    "    ----------\n",
    "    model : str\n",
    "        The model name or path.\n",
    "    max_history : int, optional\n",
    "        The maximum number of history entries to keep (default is 5).\n",
    "    local_files_only : bool, optional\n",
    "        Flag to indicate if the model is local or remote (default is False).\n",
    "    tokenizer : AutoTokenizer\n",
    "        The tokenizer for the model.\n",
    "    model : AutoModelForCausalLM\n",
    "        The model for causal language modeling.\n",
    "    history : list\n",
    "        The history of text inputs.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: str = \"\",\n",
    "        max_history: int = 10,\n",
    "        system_prompt: str = None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Constructs all the necessary attributes for the DOTLLM object.\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        model : str\n",
    "            The model name or path.\n",
    "        max_history : int, optional\n",
    "            The maximum number of history entries to keep (default is 100).\n",
    "        system_prompt : str, optional\n",
    "            The system prompt text (default is \"You are a helpful AI assistant\").\n",
    "            Note: This is only used if prompt_template is provided.\n",
    "        kwargs : dict,\n",
    "            Additional keyword arguments for the model and tokenizer.\n",
    "        \"\"\"\n",
    "        if not model:\n",
    "            _model = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "        else:\n",
    "            _model = model\n",
    "        self.bos_token = \"<|begin_of_text|>\"\n",
    "        self.tool_calling_prompt = \"\"\"You are an expert in composing functions. You are given a question and a set of possible functions. \n",
    "Based on the question, you will need to make one or more function/tool calls to achieve the purpose. \n",
    "If none of the function can be used, point it out. If the given question lacks the parameters required by the function,\n",
    "also point it out. You should only return the function call in tools call sections.\n",
    "\n",
    "If you decide to invoke any of the function(s), you MUST put it in the format of [func_name1(params_name1=params_value1, params_name2=params_value2...), func_name2(params)]\\n\n",
    "You SHOULD NOT include any other text in the response.\n",
    "\n",
    "Here is a list of functions in JSON format that you can invoke.\\n\\n{functions_definition}\\n\"\"\"\n",
    "        super().__init__(_model, max_history, system_prompt, **kwargs)\n",
    "        self.logger.debug(\"Default role of the AI assistant: %s\", system_prompt)\n",
    "\n",
    "    def generate_response(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        system_prompt: str = None,\n",
    "        tools_schema: str = None,\n",
    "        documents: List[Dict] = None,\n",
    "        create_chat_session: bool = False,\n",
    "        chat_history: List[Dict] = None,\n",
    "        max_new_tokens: int = 120,\n",
    "        skip_special_tokens: bool = False,\n",
    "        **kwargs,\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Generates text based on the given prompt.\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        prompt : str\n",
    "            The prompt text to generate text from.\n",
    "        system_prompt : str, optional\n",
    "            The system prompt text (default is None).\n",
    "        tools_schema : str, optional\n",
    "            The schema for the tools prompt (default is None).\n",
    "        documents : list, optional\n",
    "            The list of documents for the RAG prompt (default is None).\n",
    "        create_chat_session : bool, optional\n",
    "            Flag to indicate if a chat session should be created (default is False).\n",
    "        chat_history : list, optional\n",
    "            The chat history for the prompt (default is None).\n",
    "        max_new_tokens : int, optional\n",
    "            The maximum length of the generated text (default is 120).\n",
    "        skip_special_tokens : bool, optional\n",
    "            Flag to indicate if special tokens should be skipped (default is False).\n",
    "\n",
    "        Returns:\n",
    "        -------\n",
    "        str\n",
    "            The generated text.\n",
    "        \"\"\"\n",
    "        _chat_history = []\n",
    "        special_tokens = [\n",
    "            \"<|begin_of_text|>\",\n",
    "            \"<|start_header_id|>\",\n",
    "            \"<|end_header_id|>\",\n",
    "            \"<|eot_id|>\",\n",
    "        ]\n",
    "        # Check if the chat history aligns with the pydantic model\n",
    "        if chat_history:\n",
    "            try:\n",
    "                _ = [ChatHistory(**msg) for msg in chat_history]\n",
    "                _chat_history.extend(chat_history)\n",
    "            except Exception as e:\n",
    "                self.logger.error(\"Error validating chat history: %s\", e)\n",
    "        input_prompt = self.format_prompt(\n",
    "            prompt,\n",
    "            system_prompt=system_prompt,\n",
    "            tools_schema=tools_schema,\n",
    "            documents=documents,\n",
    "            create_chat_session=create_chat_session,\n",
    "            chat_history=chat_history,\n",
    "        )\n",
    "\n",
    "        model_response = self.generate_text(\n",
    "            input_prompt,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            skip_special_tokens=skip_special_tokens,\n",
    "            **kwargs,\n",
    "        )\n",
    "        # removing the prompt and special tokens from the model response\n",
    "        model_response = model_response.replace(input_prompt, \"\")\n",
    "        for token in special_tokens:\n",
    "            model_response = model_response.replace(token, \"\")\n",
    "        model_response = model_response.strip()\n",
    "        # Add the user input and model response to the chat history\n",
    "        _chat_history.append({\"role\": \"user\", \"content\": prompt})\n",
    "        _chat_history.append({\"role\": \"assistant\", \"content\": model_response})\n",
    "\n",
    "        return {\"response\": model_response, \"chat_history\": _chat_history}\n",
    "\n",
    "    def chat(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        chat_history: List[Dict] = None,\n",
    "        clear_session: bool = False,\n",
    "        **kwargs,\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Chat with the model.\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        prompt : str\n",
    "            The user prompt.\n",
    "        clear_session : bool, optional\n",
    "            Flag to indicate if the session history should be cleared (default is False).\n",
    "\n",
    "        Returns:\n",
    "        -------\n",
    "        dict\n",
    "            The response and chat history.\n",
    "        \"\"\"\n",
    "        _history_checker: bool = (\n",
    "            True  # flag to see if the chat history is passed, so we can return the chat history in the response without affecting original\n",
    "        )\n",
    "        if clear_session:\n",
    "            self.clear_history()\n",
    "\n",
    "        # Initialize chat history if not provided\n",
    "        if chat_history is None:\n",
    "            chat_history = []\n",
    "            _history_checker = False\n",
    "\n",
    "        # Determine if we need to create a new chat session\n",
    "        create_chat_session = not self.history and not chat_history\n",
    "\n",
    "        # If self.history exists, use it as chat_history\n",
    "        if self.history and not chat_history:\n",
    "            chat_history = self.history\n",
    "        # Adding the chat prompt to chat history\n",
    "        generated_response = self.generate_response(\n",
    "            prompt,\n",
    "            create_chat_session=create_chat_session,\n",
    "            chat_history=chat_history,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "        extracted_response = generated_response.get(\n",
    "            \"response\", \"Error generating response\"\n",
    "        )\n",
    "\n",
    "        # If no chat history is passed, add the user input and model response to the history\n",
    "        if not _history_checker:\n",
    "            self.add_to_history(prompt, extracted_response)\n",
    "            generated_response[\"chat_history\"] = self.history\n",
    "        else:  # if chat history is passed, return the chat history as is\n",
    "            generated_response[\"chat_history\"] = chat_history\n",
    "            generated_response[\"chat_history\"].extend(\n",
    "                [\n",
    "                    {\"role\": \"user\", \"content\": prompt},\n",
    "                    {\"role\": \"assistant\", \"content\": extracted_response},\n",
    "                ]\n",
    "            )\n",
    "\n",
    "        return generated_response\n",
    "\n",
    "    def load_prompt_templates(self):\n",
    "        \"\"\"\n",
    "        Loads the prompt templates for the Llama.\n",
    "        \"\"\"\n",
    "        self.system_template = (\n",
    "            \"<|start_header_id|>system<|end_header_id|> {system_prompt} <|eot_id|>\"\n",
    "        )\n",
    "        self.user_turn_template = (\n",
    "            \"<|start_header_id|>user<|end_header_id|> {user_prompt} <|eot_id|>\"\n",
    "        )\n",
    "        self.assistant_turn_template = \"<|start_header_id|>assistant<|end_header_id|> {assistant_response} <|eot_id|>\"\n",
    "        self.assistant_template = \"<|start_header_id|>assistant<|end_header_id|>\"\n",
    "        self.rag_prompt_template = f\"{self.bos_token }\\n{self.system_template}\\n{self.user_turn_template}\\n{self.assistant_template}\"\n",
    "        self.tools_prompt_template = f\"{self.bos_token }\\n{self.system_template}\\n{self.user_turn_template}\\n{self.assistant_template}\"\n",
    "        self.default_prompt_template = f\"{self.bos_token }\\n{self.system_template}\\n{self.user_turn_template}\\n{self.assistant_template}\"\n",
    "        self.non_sys_prompt_template = (\n",
    "            f\"{self.bos_token }\\n{self.user_turn_template}\\n{self.assistant_template}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  5.45it/s]\n"
     ]
    }
   ],
   "source": [
    "# Testing the llama model\n",
    "llama1 = LocalLLM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'user_turn_template': '<|start_header_id|>user<|end_header_id|> {user_prompt} <|eot_id|>',\n",
       " 'assistant_turn_template': '<|start_header_id|>assistant<|end_header_id|> {assistant_response} <|eot_id|>',\n",
       " 'assistant_template': '<|start_header_id|>assistant<|end_header_id|>',\n",
       " 'rag_prompt': \"You are an advanced AI assistant with expertise in retrieving and synthesizing information from provided references. Your role is to analyze the given documents and accurately answer the question based on their content.\\n\\n## Context:\\nYou will be provided with multiple documents, each containing relevant information. Each document is referenced with a unique identifier. Your response should be derived strictly from the given documents while maintaining clarity and conciseness. If the documents do not contain sufficient information, indicate that explicitly.\\n\\n## Instructions:\\n1. **Extract information** only from the provided documents.\\n2. **Cite references** where applicable by mentioning the document identifier.\\n3. **Maintain coherence** while summarizing details from multiple sources.\\n4. **Avoid speculation** or adding external knowledge.\\n5. **If unclear**, state that the answer is not available in the provided documents.\\n\\n## Expected Output:\\n- A **concise and accurate** response based on the referenced documents.\\n- **Citations** to the corresponding documents where relevant.\\n- A disclaimer if the answer cannot be found within the given context.\\n\\n## Documents:\\n{documents}\\n\\n\\n## User's Question:\\n{question}\\n\",\n",
       " 'rag_prompt_template': '<|begin_of_text|>\\n<|start_header_id|>system<|end_header_id|> {system_prompt} <|eot_id|>\\n<|start_header_id|>user<|end_header_id|> {user_prompt} <|eot_id|>\\n<|start_header_id|>assistant<|end_header_id|>',\n",
       " 'tools_prompt_template': '<|begin_of_text|>\\n<|start_header_id|>system<|end_header_id|> {system_prompt} <|eot_id|>\\n<|start_header_id|>user<|end_header_id|> {user_prompt} <|eot_id|>\\n<|start_header_id|>assistant<|end_header_id|>',\n",
       " 'default_prompt_template': '<|begin_of_text|>\\n<|start_header_id|>system<|end_header_id|> {system_prompt} <|eot_id|>\\n<|start_header_id|>user<|end_header_id|> {user_prompt} <|eot_id|>\\n<|start_header_id|>assistant<|end_header_id|>',\n",
       " 'non_sys_prompt_template': '<|begin_of_text|>\\n<|start_header_id|>user<|end_header_id|> {user_prompt} <|eot_id|>\\n<|start_header_id|>assistant<|end_header_id|>',\n",
       " 'system_prompt_template': '<|start_header_id|>system<|end_header_id|> {system_prompt} <|eot_id|>',\n",
       " 'tool_calling_prompt': 'You are an expert in composing functions. You are given a question and a set of possible functions. \\nBased on the question, you will need to make one or more function/tool calls to achieve the purpose. \\nIf none of the function can be used, point it out. If the given question lacks the parameters required by the function,\\nalso point it out. You should only return the function call in tools call sections.\\n\\nIf you decide to invoke any of the function(s), you MUST put it in the format of [func_name1(params_name1=params_value1, params_name2=params_value2...), func_name2(params)]\\n\\nYou SHOULD NOT include any other text in the response.\\n\\nHere is a list of functions in JSON format that you can invoke.\\n\\n{functions_definition}\\n'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llama1.get_templates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing prompt... <|begin_of_text|>\n",
      "<|start_header_id|>user<|end_header_id|> What is the capital of France? <|eot_id|>\n",
      "<|start_header_id|>assistant<|end_header_id|>\n",
      "Generated response:  <|begin_of_text|><|begin_of_text|>\n",
      "<|start_header_id|>user<|end_header_id|> What is the capital of France? <|eot_id|>\n",
      "<|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "The capital of France is Paris.<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "# Let's test the generate_response method\n",
    "response = llama1.generate_response(\"What is the capital of France?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">{</span>\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'response'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'The capital of France is Paris.'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'chat_history'</span>: <span style=\"font-weight: bold\">[</span>\n",
       "        <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'role'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'user'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'content'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'What is the capital of France?'</span><span style=\"font-weight: bold\">}</span>,\n",
       "        <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'role'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'assistant'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'content'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'The capital of France is Paris.'</span><span style=\"font-weight: bold\">}</span>\n",
       "    <span style=\"font-weight: bold\">]</span>\n",
       "<span style=\"font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m{\u001b[0m\n",
       "    \u001b[32m'response'\u001b[0m: \u001b[32m'The capital of France is Paris.'\u001b[0m,\n",
       "    \u001b[32m'chat_history'\u001b[0m: \u001b[1m[\u001b[0m\n",
       "        \u001b[1m{\u001b[0m\u001b[32m'role'\u001b[0m: \u001b[32m'user'\u001b[0m, \u001b[32m'content'\u001b[0m: \u001b[32m'What is the capital of France?'\u001b[0m\u001b[1m}\u001b[0m,\n",
       "        \u001b[1m{\u001b[0m\u001b[32m'role'\u001b[0m: \u001b[32m'assistant'\u001b[0m, \u001b[32m'content'\u001b[0m: \u001b[32m'The capital of France is Paris.'\u001b[0m\u001b[1m}\u001b[0m\n",
       "    \u001b[1m]\u001b[0m\n",
       "\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rprint(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing prompt... <|begin_of_text|>\n",
      "<|start_header_id|>user<|end_header_id|> What is the capital of Germany? <|eot_id|>\n",
      "<|start_header_id|>assistant<|end_header_id|>\n",
      "Generated response:  <|begin_of_text|><|begin_of_text|>\n",
      "<|start_header_id|>user<|end_header_id|> What is the capital of Germany? <|eot_id|>\n",
      "<|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "The capital of Germany is Berlin.<|eot_id|>\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">{</span>\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'response'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'The capital of Germany is Berlin.'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'chat_history'</span>: <span style=\"font-weight: bold\">[</span>\n",
       "        <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'role'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'user'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'content'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'What is the capital of Germany?'</span><span style=\"font-weight: bold\">}</span>,\n",
       "        <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'role'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'assistant'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'content'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'The capital of Germany is Berlin.'</span><span style=\"font-weight: bold\">}</span>\n",
       "    <span style=\"font-weight: bold\">]</span>\n",
       "<span style=\"font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m{\u001b[0m\n",
       "    \u001b[32m'response'\u001b[0m: \u001b[32m'The capital of Germany is Berlin.'\u001b[0m,\n",
       "    \u001b[32m'chat_history'\u001b[0m: \u001b[1m[\u001b[0m\n",
       "        \u001b[1m{\u001b[0m\u001b[32m'role'\u001b[0m: \u001b[32m'user'\u001b[0m, \u001b[32m'content'\u001b[0m: \u001b[32m'What is the capital of Germany?'\u001b[0m\u001b[1m}\u001b[0m,\n",
       "        \u001b[1m{\u001b[0m\u001b[32m'role'\u001b[0m: \u001b[32m'assistant'\u001b[0m, \u001b[32m'content'\u001b[0m: \u001b[32m'The capital of Germany is Berlin.'\u001b[0m\u001b[1m}\u001b[0m\n",
       "    \u001b[1m]\u001b[0m\n",
       "\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Let's ask different question\n",
    "response = llama1.generate_response(\"What is the capital of Germany?\")\n",
    "rprint(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing prompt... <|begin_of_text|>\n",
      "<|start_header_id|>system<|end_header_id|> You are a helpful AI assistant who always responds with one added zen quote <|eot_id|>\n",
      "<|start_header_id|>user<|end_header_id|> What is the capital of Italy? <|eot_id|>\n",
      "<|start_header_id|>assistant<|end_header_id|>\n",
      "Generated response:  <|begin_of_text|><|begin_of_text|>\n",
      "<|start_header_id|>system<|end_header_id|> You are a helpful AI assistant who always responds with one added zen quote <|eot_id|>\n",
      "<|start_header_id|>user<|end_header_id|> What is the capital of Italy? <|eot_id|>\n",
      "<|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "The capital of Italy is Rome. \n",
      "\n",
      "\"The world is a book, and those who do not travel read only one page.\" - Saint Augustine<|eot_id|>\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">{</span>\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'response'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'The capital of Italy is Rome. \\n\\n\"The world is a book, and those who do not travel read only one </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">page.\" - Saint Augustine'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'chat_history'</span>: <span style=\"font-weight: bold\">[</span>\n",
       "        <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'role'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'user'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'content'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'What is the capital of Italy?'</span><span style=\"font-weight: bold\">}</span>,\n",
       "        <span style=\"font-weight: bold\">{</span>\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'role'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'assistant'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'content'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'The capital of Italy is Rome. \\n\\n\"The world is a book, and those who do not travel read </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">only one page.\" - Saint Augustine'</span>\n",
       "        <span style=\"font-weight: bold\">}</span>\n",
       "    <span style=\"font-weight: bold\">]</span>\n",
       "<span style=\"font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m{\u001b[0m\n",
       "    \u001b[32m'response'\u001b[0m: \u001b[32m'The capital of Italy is Rome. \\n\\n\"The world is a book, and those who do not travel read only one \u001b[0m\n",
       "\u001b[32mpage.\" - Saint Augustine'\u001b[0m,\n",
       "    \u001b[32m'chat_history'\u001b[0m: \u001b[1m[\u001b[0m\n",
       "        \u001b[1m{\u001b[0m\u001b[32m'role'\u001b[0m: \u001b[32m'user'\u001b[0m, \u001b[32m'content'\u001b[0m: \u001b[32m'What is the capital of Italy?'\u001b[0m\u001b[1m}\u001b[0m,\n",
       "        \u001b[1m{\u001b[0m\n",
       "            \u001b[32m'role'\u001b[0m: \u001b[32m'assistant'\u001b[0m,\n",
       "            \u001b[32m'content'\u001b[0m: \u001b[32m'The capital of Italy is Rome. \\n\\n\"The world is a book, and those who do not travel read \u001b[0m\n",
       "\u001b[32monly one page.\" - Saint Augustine'\u001b[0m\n",
       "        \u001b[1m}\u001b[0m\n",
       "    \u001b[1m]\u001b[0m\n",
       "\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# one more question with system prompt\n",
    "response = llama1.generate_response(\"What is the capital of Italy?\", system_prompt=\"You are a helpful AI assistant who always responds with one added zen quote\")\n",
    "rprint(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llama1.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formatting prompt with chat history - create chat session\n",
      "Tokenizing prompt... <|begin_of_text|>\n",
      "<|start_header_id|>user<|end_header_id|> What is the capital of Spain? <|eot_id|>\n",
      "<|start_header_id|>assistant<|end_header_id|>\n",
      "Generated response:  <|begin_of_text|><|begin_of_text|>\n",
      "<|start_header_id|>user<|end_header_id|> What is the capital of Spain? <|eot_id|>\n",
      "<|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "The capital of Spain is Madrid.<|eot_id|>\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">{</span>\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'response'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'The capital of Spain is Madrid.'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'chat_history'</span>: <span style=\"font-weight: bold\">[</span>\n",
       "        <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'role'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'user'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'content'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'What is the capital of Spain?'</span><span style=\"font-weight: bold\">}</span>,\n",
       "        <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'role'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'assistant'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'content'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'The capital of Spain is Madrid.'</span><span style=\"font-weight: bold\">}</span>\n",
       "    <span style=\"font-weight: bold\">]</span>\n",
       "<span style=\"font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m{\u001b[0m\n",
       "    \u001b[32m'response'\u001b[0m: \u001b[32m'The capital of Spain is Madrid.'\u001b[0m,\n",
       "    \u001b[32m'chat_history'\u001b[0m: \u001b[1m[\u001b[0m\n",
       "        \u001b[1m{\u001b[0m\u001b[32m'role'\u001b[0m: \u001b[32m'user'\u001b[0m, \u001b[32m'content'\u001b[0m: \u001b[32m'What is the capital of Spain?'\u001b[0m\u001b[1m}\u001b[0m,\n",
       "        \u001b[1m{\u001b[0m\u001b[32m'role'\u001b[0m: \u001b[32m'assistant'\u001b[0m, \u001b[32m'content'\u001b[0m: \u001b[32m'The capital of Spain is Madrid.'\u001b[0m\u001b[1m}\u001b[0m\n",
       "    \u001b[1m]\u001b[0m\n",
       "\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# let's try chat method\n",
    "response = llama1.chat(\"What is the capital of Spain?\")\n",
    "rprint(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'user', 'content': 'What is the capital of Spain?'},\n",
       " {'role': 'assistant', 'content': 'The capital of Spain is Madrid.'}]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llama1.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formatting prompt with chat history\n",
      "Tokenizing prompt... <|begin_of_text|>\n",
      "<|start_header_id|>user<|end_header_id|> What is the capital of Spain? <|eot_id|>\n",
      "<|start_header_id|>assistant<|end_header_id|> The capital of Spain is Madrid. <|eot_id|>\n",
      "<|start_header_id|>user<|end_header_id|> can you tell me some history about it <|eot_id|>\n",
      "<|start_header_id|>assistant<|end_header_id|>\n",
      "Generated response:  <|begin_of_text|><|begin_of_text|>\n",
      "<|start_header_id|>user<|end_header_id|> What is the capital of Spain? <|eot_id|>\n",
      "<|start_header_id|>assistant<|end_header_id|> The capital of Spain is Madrid. <|eot_id|>\n",
      "<|start_header_id|>user<|end_header_id|> can you tell me some history about it <|eot_id|>\n",
      "<|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Madrid, the vibrant capital of Spain, has a rich and fascinating history spanning over 2,000 years. Here's a brief overview:\n",
      "\n",
      "**Ancient Times (9th century BC - 5th century AD)**\n",
      "\n",
      "* The city of Madrid was founded by the Celtic tribe of the Matiloci in the 9th century BC.\n",
      "* In 218 BC, the Romans conquered the city and named it \"Madrid\" after the Roman god of war, Mars.\n",
      "* During the Roman Empire, Madrid was an important center for trade and commerce.\n",
      "\n",
      "**Middle Ages (5th\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">{</span>\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'response'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'Madrid, the vibrant capital of Spain, has a rich and fascinating history spanning over 2,000 </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">years. Here\\'s a brief overview:\\n\\n**Ancient Times (9th century BC - 5th century AD)**\\n\\n* The city of Madrid was</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">founded by the Celtic tribe of the Matiloci in the 9th century BC.\\n* In 218 BC, the Romans conquered the city and </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">named it \"Madrid\" after the Roman god of war, Mars.\\n* During the Roman Empire, Madrid was an important center for </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">trade and commerce.\\n\\n**Middle Ages (5th'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'chat_history'</span>: <span style=\"font-weight: bold\">[</span>\n",
       "        <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'role'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'user'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'content'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'What is the capital of Spain?'</span><span style=\"font-weight: bold\">}</span>,\n",
       "        <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'role'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'assistant'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'content'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'The capital of Spain is Madrid.'</span><span style=\"font-weight: bold\">}</span>,\n",
       "        <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'role'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'user'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'content'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'can you tell me some history about it'</span><span style=\"font-weight: bold\">}</span>,\n",
       "        <span style=\"font-weight: bold\">{</span>\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'role'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'assistant'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'content'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'Madrid, the vibrant capital of Spain, has a rich and fascinating history spanning over </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">2,000 years. Here\\'s a brief overview:\\n\\n**Ancient Times (9th century BC - 5th century AD)**\\n\\n* The city of </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Madrid was founded by the Celtic tribe of the Matiloci in the 9th century BC.\\n* In 218 BC, the Romans conquered </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">the city and named it \"Madrid\" after the Roman god of war, Mars.\\n* During the Roman Empire, Madrid was an </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">important center for trade and commerce.\\n\\n**Middle Ages (5th'</span>\n",
       "        <span style=\"font-weight: bold\">}</span>\n",
       "    <span style=\"font-weight: bold\">]</span>\n",
       "<span style=\"font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m{\u001b[0m\n",
       "    \u001b[32m'response'\u001b[0m: \u001b[32m'Madrid, the vibrant capital of Spain, has a rich and fascinating history spanning over 2,000 \u001b[0m\n",
       "\u001b[32myears. Here\\'s a brief overview:\\n\\n**Ancient Times \u001b[0m\u001b[32m(\u001b[0m\u001b[32m9th century BC - 5th century AD\u001b[0m\u001b[32m)\u001b[0m\u001b[32m**\\n\\n* The city of Madrid was\u001b[0m\n",
       "\u001b[32mfounded by the Celtic tribe of the Matiloci in the 9th century BC.\\n* In 218 BC, the Romans conquered the city and \u001b[0m\n",
       "\u001b[32mnamed it \"Madrid\" after the Roman god of war, Mars.\\n* During the Roman Empire, Madrid was an important center for \u001b[0m\n",
       "\u001b[32mtrade and commerce.\\n\\n**Middle Ages \u001b[0m\u001b[32m(\u001b[0m\u001b[32m5th'\u001b[0m,\n",
       "    \u001b[32m'chat_history'\u001b[0m: \u001b[1m[\u001b[0m\n",
       "        \u001b[1m{\u001b[0m\u001b[32m'role'\u001b[0m: \u001b[32m'user'\u001b[0m, \u001b[32m'content'\u001b[0m: \u001b[32m'What is the capital of Spain?'\u001b[0m\u001b[1m}\u001b[0m,\n",
       "        \u001b[1m{\u001b[0m\u001b[32m'role'\u001b[0m: \u001b[32m'assistant'\u001b[0m, \u001b[32m'content'\u001b[0m: \u001b[32m'The capital of Spain is Madrid.'\u001b[0m\u001b[1m}\u001b[0m,\n",
       "        \u001b[1m{\u001b[0m\u001b[32m'role'\u001b[0m: \u001b[32m'user'\u001b[0m, \u001b[32m'content'\u001b[0m: \u001b[32m'can you tell me some history about it'\u001b[0m\u001b[1m}\u001b[0m,\n",
       "        \u001b[1m{\u001b[0m\n",
       "            \u001b[32m'role'\u001b[0m: \u001b[32m'assistant'\u001b[0m,\n",
       "            \u001b[32m'content'\u001b[0m: \u001b[32m'Madrid, the vibrant capital of Spain, has a rich and fascinating history spanning over \u001b[0m\n",
       "\u001b[32m2,000 years. Here\\'s a brief overview:\\n\\n**Ancient Times \u001b[0m\u001b[32m(\u001b[0m\u001b[32m9th century BC - 5th century AD\u001b[0m\u001b[32m)\u001b[0m\u001b[32m**\\n\\n* The city of \u001b[0m\n",
       "\u001b[32mMadrid was founded by the Celtic tribe of the Matiloci in the 9th century BC.\\n* In 218 BC, the Romans conquered \u001b[0m\n",
       "\u001b[32mthe city and named it \"Madrid\" after the Roman god of war, Mars.\\n* During the Roman Empire, Madrid was an \u001b[0m\n",
       "\u001b[32mimportant center for trade and commerce.\\n\\n**Middle Ages \u001b[0m\u001b[32m(\u001b[0m\u001b[32m5th'\u001b[0m\n",
       "        \u001b[1m}\u001b[0m\n",
       "    \u001b[1m]\u001b[0m\n",
       "\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# now let's continue the chat as we already have the session\n",
    "response = llama1.chat(\"can you tell me some history about it\")\n",
    "rprint(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'user', 'content': 'What is the capital of Spain?'},\n",
       " {'role': 'assistant', 'content': 'The capital of Spain is Madrid.'},\n",
       " {'role': 'user', 'content': 'can you tell me some history about it'},\n",
       " {'role': 'assistant',\n",
       "  'content': 'Madrid, the vibrant capital of Spain, has a rich and fascinating history spanning over 2,000 years. Here\\'s a brief overview:\\n\\n**Ancient Times (9th century BC - 5th century AD)**\\n\\n* The city of Madrid was founded by the Celtic tribe of the Matiloci in the 9th century BC.\\n* In 218 BC, the Romans conquered the city and named it \"Madrid\" after the Roman god of war, Mars.\\n* During the Roman Empire, Madrid was an important center for trade and commerce.\\n\\n**Middle Ages (5th'}]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llama1.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formatting prompt with tool schema [\n",
      "    {\n",
      "        \"name\": \"get_user_info\",\n",
      "        \"description\": \"Retrieve details for a specific user by their unique identifier. Note that the provided function is in Python 3 syntax.\",\n",
      "        \"parameters\": {\n",
      "            \"type\": \"dict\",\n",
      "            \"required\": [\n",
      "                \"user_id\"\n",
      "            ],\n",
      "            \"properties\": {\n",
      "                \"user_id\": {\n",
      "                \"type\": \"integer\",\n",
      "                \"description\": \"The unique identifier of the user. It is used to fetch the specific user details from the database.\"\n",
      "            },\n",
      "            \"special\": {\n",
      "                \"type\": \"string\",\n",
      "                \"description\": \"Any special information or parameters that need to be considered while fetching user details.\",\n",
      "                \"default\": \"none\"\n",
      "                }\n",
      "            }\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "Tokenizing prompt... <|begin_of_text|>\n",
      "<|start_header_id|>system<|end_header_id|> You are an expert in composing functions. You are given a question and a set of possible functions. \n",
      "Based on the question, you will need to make one or more function/tool calls to achieve the purpose. \n",
      "If none of the function can be used, point it out. If the given question lacks the parameters required by the function,\n",
      "also point it out. You should only return the function call in tools call sections.\n",
      "\n",
      "If you decide to invoke any of the function(s), you MUST put it in the format of [func_name1(params_name1=params_value1, params_name2=params_value2...), func_name2(params)]\n",
      "\n",
      "You SHOULD NOT include any other text in the response.\n",
      "\n",
      "Here is a list of functions in JSON format that you can invoke.\n",
      "\n",
      "[\n",
      "    {\n",
      "        \"name\": \"get_user_info\",\n",
      "        \"description\": \"Retrieve details for a specific user by their unique identifier. Note that the provided function is in Python 3 syntax.\",\n",
      "        \"parameters\": {\n",
      "            \"type\": \"dict\",\n",
      "            \"required\": [\n",
      "                \"user_id\"\n",
      "            ],\n",
      "            \"properties\": {\n",
      "                \"user_id\": {\n",
      "                \"type\": \"integer\",\n",
      "                \"description\": \"The unique identifier of the user. It is used to fetch the specific user details from the database.\"\n",
      "            },\n",
      "            \"special\": {\n",
      "                \"type\": \"string\",\n",
      "                \"description\": \"Any special information or parameters that need to be considered while fetching user details.\",\n",
      "                \"default\": \"none\"\n",
      "                }\n",
      "            }\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      " <|eot_id|>\n",
      "<|start_header_id|>user<|end_header_id|> Can you retrieve the details for the user with the ID 7890, who has black as their special request? <|eot_id|>\n",
      "<|start_header_id|>assistant<|end_header_id|>\n",
      "Generated response:  <|begin_of_text|><|begin_of_text|>\n",
      "<|start_header_id|>system<|end_header_id|> You are an expert in composing functions. You are given a question and a set of possible functions. \n",
      "Based on the question, you will need to make one or more function/tool calls to achieve the purpose. \n",
      "If none of the function can be used, point it out. If the given question lacks the parameters required by the function,\n",
      "also point it out. You should only return the function call in tools call sections.\n",
      "\n",
      "If you decide to invoke any of the function(s), you MUST put it in the format of [func_name1(params_name1=params_value1, params_name2=params_value2...), func_name2(params)]\n",
      "\n",
      "You SHOULD NOT include any other text in the response.\n",
      "\n",
      "Here is a list of functions in JSON format that you can invoke.\n",
      "\n",
      "[\n",
      "    {\n",
      "        \"name\": \"get_user_info\",\n",
      "        \"description\": \"Retrieve details for a specific user by their unique identifier. Note that the provided function is in Python 3 syntax.\",\n",
      "        \"parameters\": {\n",
      "            \"type\": \"dict\",\n",
      "            \"required\": [\n",
      "                \"user_id\"\n",
      "            ],\n",
      "            \"properties\": {\n",
      "                \"user_id\": {\n",
      "                \"type\": \"integer\",\n",
      "                \"description\": \"The unique identifier of the user. It is used to fetch the specific user details from the database.\"\n",
      "            },\n",
      "            \"special\": {\n",
      "                \"type\": \"string\",\n",
      "                \"description\": \"Any special information or parameters that need to be considered while fetching user details.\",\n",
      "                \"default\": \"none\"\n",
      "                }\n",
      "            }\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      " <|eot_id|>\n",
      "<|start_header_id|>user<|end_header_id|> Can you retrieve the details for the user with the ID 7890, who has black as their special request? <|eot_id|>\n",
      "<|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "[get_user_info(user_id=7890, special='black')]<|eot_id|>\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">{</span>\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'response'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"[get_user_info(user_id=7890, special='black')]\"</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'chat_history'</span>: <span style=\"font-weight: bold\">[</span>\n",
       "        <span style=\"font-weight: bold\">{</span>\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'role'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'user'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'content'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'Can you retrieve the details for the user with the ID 7890, who has black as their special </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">request?'</span>\n",
       "        <span style=\"font-weight: bold\">}</span>,\n",
       "        <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'role'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'assistant'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'content'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"[get_user_info(user_id=7890, special='black')]\"</span><span style=\"font-weight: bold\">}</span>\n",
       "    <span style=\"font-weight: bold\">]</span>\n",
       "<span style=\"font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m{\u001b[0m\n",
       "    \u001b[32m'response'\u001b[0m: \u001b[32m\"\u001b[0m\u001b[32m[\u001b[0m\u001b[32mget_user_info\u001b[0m\u001b[32m(\u001b[0m\u001b[32muser_id\u001b[0m\u001b[32m=\u001b[0m\u001b[32m7890\u001b[0m\u001b[32m, \u001b[0m\u001b[32mspecial\u001b[0m\u001b[32m='black'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m]\u001b[0m\u001b[32m\"\u001b[0m,\n",
       "    \u001b[32m'chat_history'\u001b[0m: \u001b[1m[\u001b[0m\n",
       "        \u001b[1m{\u001b[0m\n",
       "            \u001b[32m'role'\u001b[0m: \u001b[32m'user'\u001b[0m,\n",
       "            \u001b[32m'content'\u001b[0m: \u001b[32m'Can you retrieve the details for the user with the ID 7890, who has black as their special \u001b[0m\n",
       "\u001b[32mrequest?'\u001b[0m\n",
       "        \u001b[1m}\u001b[0m,\n",
       "        \u001b[1m{\u001b[0m\u001b[32m'role'\u001b[0m: \u001b[32m'assistant'\u001b[0m, \u001b[32m'content'\u001b[0m: \u001b[32m\"\u001b[0m\u001b[32m[\u001b[0m\u001b[32mget_user_info\u001b[0m\u001b[32m(\u001b[0m\u001b[32muser_id\u001b[0m\u001b[32m=\u001b[0m\u001b[32m7890\u001b[0m\u001b[32m, \u001b[0m\u001b[32mspecial\u001b[0m\u001b[32m='black'\u001b[0m\u001b[32m)\u001b[0m\u001b[32m]\u001b[0m\u001b[32m\"\u001b[0m\u001b[1m}\u001b[0m\n",
       "    \u001b[1m]\u001b[0m\n",
       "\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# let's check the tool calling prompt\n",
    "function_definitions = \"\"\"[\n",
    "    {\n",
    "        \"name\": \"get_user_info\",\n",
    "        \"description\": \"Retrieve details for a specific user by their unique identifier. Note that the provided function is in Python 3 syntax.\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"dict\",\n",
    "            \"required\": [\n",
    "                \"user_id\"\n",
    "            ],\n",
    "            \"properties\": {\n",
    "                \"user_id\": {\n",
    "                \"type\": \"integer\",\n",
    "                \"description\": \"The unique identifier of the user. It is used to fetch the specific user details from the database.\"\n",
    "            },\n",
    "            \"special\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"Any special information or parameters that need to be considered while fetching user details.\",\n",
    "                \"default\": \"none\"\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\"\"\"\n",
    "\n",
    "response = llama1.generate_response(\"Can you retrieve the details for the user with the ID 7890, who has black as their special request?\", tools_schema=function_definitions)\n",
    "rprint(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formatting prompt with documents\n",
      "Tokenizing prompt... <|begin_of_text|>\n",
      "<|start_header_id|>system<|end_header_id|> You are an advanced AI assistant with expertise in retrieving and synthesizing information from provided references. Your role is to analyze the given documents and accurately answer the question based on their content.\n",
      "\n",
      "## Context:\n",
      "You will be provided with multiple documents, each containing relevant information. Each document is referenced with a unique identifier. Your response should be derived strictly from the given documents while maintaining clarity and conciseness. If the documents do not contain sufficient information, indicate that explicitly.\n",
      "\n",
      "## Instructions:\n",
      "1. **Extract information** only from the provided documents.\n",
      "2. **Cite references** where applicable by mentioning the document identifier.\n",
      "3. **Maintain coherence** while summarizing details from multiple sources.\n",
      "4. **Avoid speculation** or adding external knowledge.\n",
      "5. **If unclear**, state that the answer is not available in the provided documents.\n",
      "\n",
      "## Expected Output:\n",
      "- A **concise and accurate** response based on the referenced documents.\n",
      "- **Citations** to the corresponding documents where relevant.\n",
      "- A disclaimer if the answer cannot be found within the given context.\n",
      "\n",
      "## Documents:\n",
      "**Document Doc1**: Quantum computing leverages quantum mechanics to perform computations at speeds unattainable by classical computers. It relies on principles like superposition, where quantum bits (qubits) exist in multiple states simultaneously, and entanglement, which enables qubits to be linked regardless of distance. These properties allow quantum computers to solve complex problems efficiently. Current research is focused on improving qubit stability and error correction.\n",
      "**Document Doc2**: The theory of relativity, proposed by Albert Einstein, revolutionized our understanding of space and time. It consists of special relativity, which deals with objects moving at high velocities, and general relativity, which explains gravity as the curvature of spacetime. This theory has been experimentally confirmed through observations like gravitational lensing and time dilation. Modern GPS systems rely on relativity corrections for accurate positioning.\n",
      "**Document Doc3**: Machine learning is a subset of artificial intelligence that enables computers to learn from data without explicit programming. It includes supervised, unsupervised, and reinforcement learning techniques. These models are used in applications like image recognition, fraud detection, and recommendation systems. The effectiveness of a machine learning model depends on the quality and quantity of training data.\n",
      "**Document Doc4**: Blockchain technology provides a decentralized and secure way to record transactions. It uses cryptographic hashing and distributed consensus to ensure data integrity. Originally developed for Bitcoin, blockchain is now used in supply chain management, digital identity, and smart contracts. The technology faces challenges like scalability and energy consumption.\n",
      "**Document Doc5**: The human brain consists of billions of neurons that communicate through electrical and chemical signals. Neural networks in artificial intelligence are inspired by this biological structure. The brain's plasticity allows it to adapt and learn new information throughout life. Research in neuroscience is uncovering new treatments for cognitive disorders.\n",
      "\n",
      "\n",
      "## User's Question:\n",
      "What are the key principles of quantum computing?\n",
      " <|eot_id|>\n",
      "<|start_header_id|>user<|end_header_id|> What are the key principles of quantum computing? <|eot_id|>\n",
      "<|start_header_id|>assistant<|end_header_id|>\n",
      "Generated response:  <|begin_of_text|><|begin_of_text|>\n",
      "<|start_header_id|>system<|end_header_id|> You are an advanced AI assistant with expertise in retrieving and synthesizing information from provided references. Your role is to analyze the given documents and accurately answer the question based on their content.\n",
      "\n",
      "## Context:\n",
      "You will be provided with multiple documents, each containing relevant information. Each document is referenced with a unique identifier. Your response should be derived strictly from the given documents while maintaining clarity and conciseness. If the documents do not contain sufficient information, indicate that explicitly.\n",
      "\n",
      "## Instructions:\n",
      "1. **Extract information** only from the provided documents.\n",
      "2. **Cite references** where applicable by mentioning the document identifier.\n",
      "3. **Maintain coherence** while summarizing details from multiple sources.\n",
      "4. **Avoid speculation** or adding external knowledge.\n",
      "5. **If unclear**, state that the answer is not available in the provided documents.\n",
      "\n",
      "## Expected Output:\n",
      "- A **concise and accurate** response based on the referenced documents.\n",
      "- **Citations** to the corresponding documents where relevant.\n",
      "- A disclaimer if the answer cannot be found within the given context.\n",
      "\n",
      "## Documents:\n",
      "**Document Doc1**: Quantum computing leverages quantum mechanics to perform computations at speeds unattainable by classical computers. It relies on principles like superposition, where quantum bits (qubits) exist in multiple states simultaneously, and entanglement, which enables qubits to be linked regardless of distance. These properties allow quantum computers to solve complex problems efficiently. Current research is focused on improving qubit stability and error correction.\n",
      "**Document Doc2**: The theory of relativity, proposed by Albert Einstein, revolutionized our understanding of space and time. It consists of special relativity, which deals with objects moving at high velocities, and general relativity, which explains gravity as the curvature of spacetime. This theory has been experimentally confirmed through observations like gravitational lensing and time dilation. Modern GPS systems rely on relativity corrections for accurate positioning.\n",
      "**Document Doc3**: Machine learning is a subset of artificial intelligence that enables computers to learn from data without explicit programming. It includes supervised, unsupervised, and reinforcement learning techniques. These models are used in applications like image recognition, fraud detection, and recommendation systems. The effectiveness of a machine learning model depends on the quality and quantity of training data.\n",
      "**Document Doc4**: Blockchain technology provides a decentralized and secure way to record transactions. It uses cryptographic hashing and distributed consensus to ensure data integrity. Originally developed for Bitcoin, blockchain is now used in supply chain management, digital identity, and smart contracts. The technology faces challenges like scalability and energy consumption.\n",
      "**Document Doc5**: The human brain consists of billions of neurons that communicate through electrical and chemical signals. Neural networks in artificial intelligence are inspired by this biological structure. The brain's plasticity allows it to adapt and learn new information throughout life. Research in neuroscience is uncovering new treatments for cognitive disorders.\n",
      "\n",
      "\n",
      "## User's Question:\n",
      "What are the key principles of quantum computing?\n",
      " <|eot_id|>\n",
      "<|start_header_id|>user<|end_header_id|> What are the key principles of quantum computing? <|eot_id|>\n",
      "<|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "According to Document Doc1, the key principles of quantum computing are:\n",
      "\n",
      "1. **Superposition**: Quantum bits (qubits) can exist in multiple states simultaneously.\n",
      "2. **Entanglement**: Qubits can be linked regardless of distance, enabling correlations between them.\n",
      "\n",
      "These properties allow quantum computers to solve complex problems efficiently.\n",
      "\n",
      "(Note: Document Doc1 is the primary source for this information.)<|eot_id|>\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">{</span>\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'response'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'According to Document Doc1, the key principles of quantum computing are:\\n\\n1. **Superposition**: </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Quantum bits (qubits) can exist in multiple states simultaneously.\\n2. **Entanglement**: Qubits can be linked </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">regardless of distance, enabling correlations between them.\\n\\nThese properties allow quantum computers to solve </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">complex problems efficiently.\\n\\n(Note: Document Doc1 is the primary source for this information.)'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'chat_history'</span>: <span style=\"font-weight: bold\">[</span>\n",
       "        <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'role'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'user'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'content'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'What are the key principles of quantum computing?'</span><span style=\"font-weight: bold\">}</span>,\n",
       "        <span style=\"font-weight: bold\">{</span>\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'role'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'assistant'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'content'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'According to Document Doc1, the key principles of quantum computing are:\\n\\n1. </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">**Superposition**: Quantum bits (qubits) can exist in multiple states simultaneously.\\n2. **Entanglement**: Qubits </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">can be linked regardless of distance, enabling correlations between them.\\n\\nThese properties allow quantum </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">computers to solve complex problems efficiently.\\n\\n(Note: Document Doc1 is the primary source for this </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">information.)'</span>\n",
       "        <span style=\"font-weight: bold\">}</span>\n",
       "    <span style=\"font-weight: bold\">]</span>\n",
       "<span style=\"font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m{\u001b[0m\n",
       "    \u001b[32m'response'\u001b[0m: \u001b[32m'According to Document Doc1, the key principles of quantum computing are:\\n\\n1. **Superposition**: \u001b[0m\n",
       "\u001b[32mQuantum bits \u001b[0m\u001b[32m(\u001b[0m\u001b[32mqubits\u001b[0m\u001b[32m)\u001b[0m\u001b[32m can exist in multiple states simultaneously.\\n2. **Entanglement**: Qubits can be linked \u001b[0m\n",
       "\u001b[32mregardless of distance, enabling correlations between them.\\n\\nThese properties allow quantum computers to solve \u001b[0m\n",
       "\u001b[32mcomplex problems efficiently.\\n\\n\u001b[0m\u001b[32m(\u001b[0m\u001b[32mNote: Document Doc1 is the primary source for this information.\u001b[0m\u001b[32m)\u001b[0m\u001b[32m'\u001b[0m,\n",
       "    \u001b[32m'chat_history'\u001b[0m: \u001b[1m[\u001b[0m\n",
       "        \u001b[1m{\u001b[0m\u001b[32m'role'\u001b[0m: \u001b[32m'user'\u001b[0m, \u001b[32m'content'\u001b[0m: \u001b[32m'What are the key principles of quantum computing?'\u001b[0m\u001b[1m}\u001b[0m,\n",
       "        \u001b[1m{\u001b[0m\n",
       "            \u001b[32m'role'\u001b[0m: \u001b[32m'assistant'\u001b[0m,\n",
       "            \u001b[32m'content'\u001b[0m: \u001b[32m'According to Document Doc1, the key principles of quantum computing are:\\n\\n1. \u001b[0m\n",
       "\u001b[32m**Superposition**: Quantum bits \u001b[0m\u001b[32m(\u001b[0m\u001b[32mqubits\u001b[0m\u001b[32m)\u001b[0m\u001b[32m can exist in multiple states simultaneously.\\n2. **Entanglement**: Qubits \u001b[0m\n",
       "\u001b[32mcan be linked regardless of distance, enabling correlations between them.\\n\\nThese properties allow quantum \u001b[0m\n",
       "\u001b[32mcomputers to solve complex problems efficiently.\\n\\n\u001b[0m\u001b[32m(\u001b[0m\u001b[32mNote: Document Doc1 is the primary source for this \u001b[0m\n",
       "\u001b[32minformation.\u001b[0m\u001b[32m)\u001b[0m\u001b[32m'\u001b[0m\n",
       "        \u001b[1m}\u001b[0m\n",
       "    \u001b[1m]\u001b[0m\n",
       "\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Let's now test the RAG prompt\n",
    "documents = [\n",
    "    {\n",
    "        \"reference\": \"Doc1\",\n",
    "        \"content\": \"Quantum computing leverages quantum mechanics to perform computations at speeds unattainable by classical computers. It relies on principles like superposition, where quantum bits (qubits) exist in multiple states simultaneously, and entanglement, which enables qubits to be linked regardless of distance. These properties allow quantum computers to solve complex problems efficiently. Current research is focused on improving qubit stability and error correction.\"\n",
    "    },\n",
    "    {\n",
    "        \"reference\": \"Doc2\",\n",
    "        \"content\": \"The theory of relativity, proposed by Albert Einstein, revolutionized our understanding of space and time. It consists of special relativity, which deals with objects moving at high velocities, and general relativity, which explains gravity as the curvature of spacetime. This theory has been experimentally confirmed through observations like gravitational lensing and time dilation. Modern GPS systems rely on relativity corrections for accurate positioning.\"\n",
    "    },\n",
    "    {\n",
    "        \"reference\": \"Doc3\",\n",
    "        \"content\": \"Machine learning is a subset of artificial intelligence that enables computers to learn from data without explicit programming. It includes supervised, unsupervised, and reinforcement learning techniques. These models are used in applications like image recognition, fraud detection, and recommendation systems. The effectiveness of a machine learning model depends on the quality and quantity of training data.\"\n",
    "    },\n",
    "    {\n",
    "        \"reference\": \"Doc4\",\n",
    "        \"content\": \"Blockchain technology provides a decentralized and secure way to record transactions. It uses cryptographic hashing and distributed consensus to ensure data integrity. Originally developed for Bitcoin, blockchain is now used in supply chain management, digital identity, and smart contracts. The technology faces challenges like scalability and energy consumption.\"\n",
    "    },\n",
    "    {\n",
    "        \"reference\": \"Doc5\",\n",
    "        \"content\": \"The human brain consists of billions of neurons that communicate through electrical and chemical signals. Neural networks in artificial intelligence are inspired by this biological structure. The brain's plasticity allows it to adapt and learn new information throughout life. Research in neuroscience is uncovering new treatments for cognitive disorders.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "response = llama1.generate_response(\"What are the key principles of quantum computing?\", documents=documents)\n",
    "rprint(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">According to Document Doc1, the key principles of quantum computing are:\n",
       "\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>. **Superposition**: Quantum bits <span style=\"font-weight: bold\">(</span>qubits<span style=\"font-weight: bold\">)</span> can exist in multiple states simultaneously.\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>. **Entanglement**: Qubits can be linked regardless of distance, enabling correlations between them.\n",
       "\n",
       "These properties allow quantum computers to solve complex problems efficiently.\n",
       "\n",
       "<span style=\"font-weight: bold\">(</span>Note: Document Doc1 is the primary source for this information.<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "According to Document Doc1, the key principles of quantum computing are:\n",
       "\n",
       "\u001b[1;36m1\u001b[0m. **Superposition**: Quantum bits \u001b[1m(\u001b[0mqubits\u001b[1m)\u001b[0m can exist in multiple states simultaneously.\n",
       "\u001b[1;36m2\u001b[0m. **Entanglement**: Qubits can be linked regardless of distance, enabling correlations between them.\n",
       "\n",
       "These properties allow quantum computers to solve complex problems efficiently.\n",
       "\n",
       "\u001b[1m(\u001b[0mNote: Document Doc1 is the primary source for this information.\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rprint(response.get(\"response\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formatting prompt with chat history\n",
      "Tokenizing prompt... <|begin_of_text|>\n",
      "<|start_header_id|>user<|end_header_id|> What are the key principles of quantum computing? <|eot_id|>\n",
      "<|start_header_id|>assistant<|end_header_id|> According to Document Doc1, the key principles of quantum computing are:\n",
      "\n",
      "1. **Superposition**: Quantum bits (qubits) can exist in multiple states simultaneously.\n",
      "2. **Entanglement**: Qubits can be linked regardless of distance, enabling correlations between them.\n",
      "\n",
      "These properties allow quantum computers to solve complex problems efficiently.\n",
      "\n",
      "(Note: Document Doc1 is the primary source for this information.) <|eot_id|>\n",
      "<|start_header_id|>user<|end_header_id|> What do you think about it on whole <|eot_id|>\n",
      "<|start_header_id|>assistant<|end_header_id|>\n",
      "Generated response:  <|begin_of_text|><|begin_of_text|>\n",
      "<|start_header_id|>user<|end_header_id|> What are the key principles of quantum computing? <|eot_id|>\n",
      "<|start_header_id|>assistant<|end_header_id|> According to Document Doc1, the key principles of quantum computing are:\n",
      "\n",
      "1. **Superposition**: Quantum bits (qubits) can exist in multiple states simultaneously.\n",
      "2. **Entanglement**: Qubits can be linked regardless of distance, enabling correlations between them.\n",
      "\n",
      "These properties allow quantum computers to solve complex problems efficiently.\n",
      "\n",
      "(Note: Document Doc1 is the primary source for this information.) <|eot_id|>\n",
      "<|start_header_id|>user<|end_header_id|> What do you think about it on whole <|eot_id|>\n",
      "<|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Quantum computing is a revolutionary concept that has the potential to transform the way we approach complex problems in various fields, such as cryptography, optimization, simulation, and machine learning. Here are some potential benefits and implications of quantum computing:\n",
      "\n",
      "**Benefits:**\n",
      "\n",
      "1. **Exponential scaling**: Quantum computers can solve certain problems much faster than classical computers, potentially leading to breakthroughs in fields like medicine, finance, and climate modeling.\n",
      "2. **Simulation and optimization**: Quantum computers can simulate complex systems and optimize processes, which could lead to significant advances in fields like chemistry, materials science, and logistics.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">{</span>\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'response'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'Quantum computing is a revolutionary concept that has the potential to transform the way we </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">approach complex problems in various fields, such as cryptography, optimization, simulation, and machine learning. </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Here are some potential benefits and implications of quantum computing:\\n\\n**Benefits:**\\n\\n1. **Exponential </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">scaling**: Quantum computers can solve certain problems much faster than classical computers, potentially leading </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">to breakthroughs in fields like medicine, finance, and climate modeling.\\n2. **Simulation and optimization**: </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Quantum computers can simulate complex systems and optimize processes, which could lead to significant advances in </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">fields like chemistry, materials science, and logistics.'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'chat_history'</span>: <span style=\"font-weight: bold\">[</span>\n",
       "        <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'role'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'user'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'content'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'What are the key principles of quantum computing?'</span><span style=\"font-weight: bold\">}</span>,\n",
       "        <span style=\"font-weight: bold\">{</span>\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'role'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'assistant'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'content'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'According to Document Doc1, the key principles of quantum computing are:\\n\\n1. </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">**Superposition**: Quantum bits (qubits) can exist in multiple states simultaneously.\\n2. **Entanglement**: Qubits </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">can be linked regardless of distance, enabling correlations between them.\\n\\nThese properties allow quantum </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">computers to solve complex problems efficiently.\\n\\n(Note: Document Doc1 is the primary source for this </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">information.)'</span>\n",
       "        <span style=\"font-weight: bold\">}</span>,\n",
       "        <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'role'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'user'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'content'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'What do you think about it on whole'</span><span style=\"font-weight: bold\">}</span>,\n",
       "        <span style=\"font-weight: bold\">{</span>\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'role'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'assistant'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'content'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'Quantum computing is a revolutionary concept that has the potential to transform the way we</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">approach complex problems in various fields, such as cryptography, optimization, simulation, and machine learning. </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Here are some potential benefits and implications of quantum computing:\\n\\n**Benefits:**\\n\\n1. **Exponential </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">scaling**: Quantum computers can solve certain problems much faster than classical computers, potentially leading </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">to breakthroughs in fields like medicine, finance, and climate modeling.\\n2. **Simulation and optimization**: </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Quantum computers can simulate complex systems and optimize processes, which could lead to significant advances in </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">fields like chemistry, materials science, and logistics.'</span>\n",
       "        <span style=\"font-weight: bold\">}</span>\n",
       "    <span style=\"font-weight: bold\">]</span>\n",
       "<span style=\"font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m{\u001b[0m\n",
       "    \u001b[32m'response'\u001b[0m: \u001b[32m'Quantum computing is a revolutionary concept that has the potential to transform the way we \u001b[0m\n",
       "\u001b[32mapproach complex problems in various fields, such as cryptography, optimization, simulation, and machine learning. \u001b[0m\n",
       "\u001b[32mHere are some potential benefits and implications of quantum computing:\\n\\n**Benefits:**\\n\\n1. **Exponential \u001b[0m\n",
       "\u001b[32mscaling**: Quantum computers can solve certain problems much faster than classical computers, potentially leading \u001b[0m\n",
       "\u001b[32mto breakthroughs in fields like medicine, finance, and climate modeling.\\n2. **Simulation and optimization**: \u001b[0m\n",
       "\u001b[32mQuantum computers can simulate complex systems and optimize processes, which could lead to significant advances in \u001b[0m\n",
       "\u001b[32mfields like chemistry, materials science, and logistics.'\u001b[0m,\n",
       "    \u001b[32m'chat_history'\u001b[0m: \u001b[1m[\u001b[0m\n",
       "        \u001b[1m{\u001b[0m\u001b[32m'role'\u001b[0m: \u001b[32m'user'\u001b[0m, \u001b[32m'content'\u001b[0m: \u001b[32m'What are the key principles of quantum computing?'\u001b[0m\u001b[1m}\u001b[0m,\n",
       "        \u001b[1m{\u001b[0m\n",
       "            \u001b[32m'role'\u001b[0m: \u001b[32m'assistant'\u001b[0m,\n",
       "            \u001b[32m'content'\u001b[0m: \u001b[32m'According to Document Doc1, the key principles of quantum computing are:\\n\\n1. \u001b[0m\n",
       "\u001b[32m**Superposition**: Quantum bits \u001b[0m\u001b[32m(\u001b[0m\u001b[32mqubits\u001b[0m\u001b[32m)\u001b[0m\u001b[32m can exist in multiple states simultaneously.\\n2. **Entanglement**: Qubits \u001b[0m\n",
       "\u001b[32mcan be linked regardless of distance, enabling correlations between them.\\n\\nThese properties allow quantum \u001b[0m\n",
       "\u001b[32mcomputers to solve complex problems efficiently.\\n\\n\u001b[0m\u001b[32m(\u001b[0m\u001b[32mNote: Document Doc1 is the primary source for this \u001b[0m\n",
       "\u001b[32minformation.\u001b[0m\u001b[32m)\u001b[0m\u001b[32m'\u001b[0m\n",
       "        \u001b[1m}\u001b[0m,\n",
       "        \u001b[1m{\u001b[0m\u001b[32m'role'\u001b[0m: \u001b[32m'user'\u001b[0m, \u001b[32m'content'\u001b[0m: \u001b[32m'What do you think about it on whole'\u001b[0m\u001b[1m}\u001b[0m,\n",
       "        \u001b[1m{\u001b[0m\n",
       "            \u001b[32m'role'\u001b[0m: \u001b[32m'assistant'\u001b[0m,\n",
       "            \u001b[32m'content'\u001b[0m: \u001b[32m'Quantum computing is a revolutionary concept that has the potential to transform the way we\u001b[0m\n",
       "\u001b[32mapproach complex problems in various fields, such as cryptography, optimization, simulation, and machine learning. \u001b[0m\n",
       "\u001b[32mHere are some potential benefits and implications of quantum computing:\\n\\n**Benefits:**\\n\\n1. **Exponential \u001b[0m\n",
       "\u001b[32mscaling**: Quantum computers can solve certain problems much faster than classical computers, potentially leading \u001b[0m\n",
       "\u001b[32mto breakthroughs in fields like medicine, finance, and climate modeling.\\n2. **Simulation and optimization**: \u001b[0m\n",
       "\u001b[32mQuantum computers can simulate complex systems and optimize processes, which could lead to significant advances in \u001b[0m\n",
       "\u001b[32mfields like chemistry, materials science, and logistics.'\u001b[0m\n",
       "        \u001b[1m}\u001b[0m\n",
       "    \u001b[1m]\u001b[0m\n",
       "\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Let's try to make a chat session continuing this rag prompt\n",
    "response = llama1.chat(\"What do you think about it on whole\", chat_history=response.get(\"chat_history\"))\n",
    "rprint(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'user', 'content': 'What is the capital of Spain?'},\n",
       " {'role': 'assistant', 'content': 'The capital of Spain is Madrid.'},\n",
       " {'role': 'user', 'content': 'can you tell me some history about it'},\n",
       " {'role': 'assistant',\n",
       "  'content': 'Madrid, the vibrant capital of Spain, has a rich and fascinating history spanning over 2,000 years. Here\\'s a brief overview:\\n\\n**Ancient Times (9th century BC - 5th century AD)**\\n\\n* The city of Madrid was founded by the Celtic tribe of the Matiloci in the 9th century BC.\\n* In 218 BC, the Romans conquered the city and named it \"Madrid\" after the Roman god of war, Mars.\\n* During the Roman Empire, Madrid was an important center for trade and commerce.\\n\\n**Middle Ages (5th'}]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now let's check the history\n",
    "llama1.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# as we could see the history remains intact while we could pass custome history to the chat method to generate the response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
